#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ ‡ä¹¦ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·
"""

import asyncio
import time
import json
import statistics
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import psutil
import sys
import threading
from dataclasses import dataclass
from typing import List, Dict, Tuple

# æµ‹è¯•é…ç½®
BASE_URL = "http://localhost:8082"
TEST_QUERIES = [
    "äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨",
    "5Gç½‘ç»œå»ºè®¾æ–¹æ¡ˆ",
    "ä¾›åº”å•†ç®¡ç†ç³»ç»Ÿ",
    "ä¼ä¸šæ•°å­—åŒ–è½¬å‹",
    "äº‘è®¡ç®—å¹³å°æ¶æ„",
    "å¤§æ•°æ®åˆ†æå·¥å…·",
    "ç½‘ç»œå®‰å…¨é˜²æŠ¤",
    "é¡¹ç›®ç®¡ç†ç³»ç»Ÿ",
    "æ‹›æ ‡é‡‡è´­æµç¨‹",
    "è´¨é‡ç®¡ç†ä½“ç³»"
]

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    name: str
    response_times: List[float]
    success_rate: float
    throughput: float  # requests per second
    memory_usage: List[float]
    cpu_usage: List[float]
    error_count: int

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, base_url=BASE_URL):
        self.base_url = base_url
        self.session = requests.Session()
        self.metrics = {}
        self.system_monitor = SystemMonitor()

    def run_single_request_test(self):
        """å•è¯·æ±‚æ€§èƒ½æµ‹è¯•"""
        print("ğŸ”§ è¿è¡Œå•è¯·æ±‚æ€§èƒ½æµ‹è¯•...")

        response_times = []
        errors = 0

        for query in TEST_QUERIES:
            start_time = time.time()
            try:
                response = self.session.post(
                    f"{self.base_url}/api/vector_search/search",
                    json={"query": query, "top_k": 5, "threshold": 0.3},
                    timeout=10
                )
                response_time = time.time() - start_time

                if response.status_code == 200:
                    response_times.append(response_time)
                else:
                    errors += 1

            except Exception:
                errors += 1

        if response_times:
            self.metrics['single_request'] = PerformanceMetric(
                name="å•è¯·æ±‚æµ‹è¯•",
                response_times=response_times,
                success_rate=(len(response_times) / len(TEST_QUERIES)) * 100,
                throughput=len(response_times) / sum(response_times) if response_times else 0,
                memory_usage=[],
                cpu_usage=[],
                error_count=errors
            )

            print(f"  âœ… å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.3f}s")
            print(f"  âœ… æˆåŠŸç‡: {(len(response_times) / len(TEST_QUERIES)) * 100:.1f}%")

    def run_concurrent_test(self, concurrency_levels=[1, 5, 10, 20]):
        """å¹¶å‘è¯·æ±‚æ€§èƒ½æµ‹è¯•"""
        print("âš¡ è¿è¡Œå¹¶å‘è¯·æ±‚æ€§èƒ½æµ‹è¯•...")

        for concurrency in concurrency_levels:
            print(f"  æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")

            # å¯åŠ¨ç³»ç»Ÿç›‘æ§
            self.system_monitor.start_monitoring()

            response_times = []
            errors = 0
            start_time = time.time()

            def make_concurrent_request():
                query = TEST_QUERIES[0]  # ä½¿ç”¨å›ºå®šæŸ¥è¯¢é¿å…ç»“æœå·®å¼‚
                try:
                    req_start = time.time()
                    response = self.session.post(
                        f"{self.base_url}/api/vector_search/search",
                        json={"query": query, "top_k": 5, "threshold": 0.3},
                        timeout=10
                    )
                    req_time = time.time() - req_start
                    return req_time, response.status_code == 200
                except Exception:
                    return time.time() - req_start, False

            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [executor.submit(make_concurrent_request) for _ in range(concurrency * 2)]

                for future in as_completed(futures):
                    response_time, success = future.result()
                    if success:
                        response_times.append(response_time)
                    else:
                        errors += 1

            total_time = time.time() - start_time

            # åœæ­¢ç³»ç»Ÿç›‘æ§
            memory_usage, cpu_usage = self.system_monitor.stop_monitoring()

            if response_times:
                self.metrics[f'concurrent_{concurrency}'] = PerformanceMetric(
                    name=f"å¹¶å‘æµ‹è¯•(x{concurrency})",
                    response_times=response_times,
                    success_rate=(len(response_times) / (concurrency * 2)) * 100,
                    throughput=len(response_times) / total_time,
                    memory_usage=memory_usage,
                    cpu_usage=cpu_usage,
                    error_count=errors
                )

                print(f"    âœ… å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.3f}s")
                print(f"    âœ… ååé‡: {len(response_times) / total_time:.2f} req/s")
                print(f"    âœ… æˆåŠŸç‡: {(len(response_times) / (concurrency * 2)) * 100:.1f}%")

    def run_stress_test(self, duration_seconds=30):
        """å‹åŠ›æµ‹è¯•"""
        print(f"ğŸš€ è¿è¡Œå‹åŠ›æµ‹è¯• ({duration_seconds}ç§’)...")

        self.system_monitor.start_monitoring()

        response_times = []
        errors = 0
        request_count = 0
        start_time = time.time()

        def stress_worker():
            nonlocal response_times, errors, request_count
            while time.time() - start_time < duration_seconds:
                try:
                    query = TEST_QUERIES[request_count % len(TEST_QUERIES)]
                    req_start = time.time()

                    response = self.session.post(
                        f"{self.base_url}/api/vector_search/search",
                        json={"query": query, "top_k": 5, "threshold": 0.3},
                        timeout=5
                    )

                    req_time = time.time() - req_start
                    request_count += 1

                    if response.status_code == 200:
                        response_times.append(req_time)
                    else:
                        errors += 1

                except Exception:
                    errors += 1

                time.sleep(0.1)  # é¿å…è¿‡åº¦å‹æµ‹

        # å¯åŠ¨å¤šä¸ªå·¥ä½œçº¿ç¨‹
        threads = []
        for _ in range(5):  # 5ä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹
            thread = threading.Thread(target=stress_worker)
            thread.start()
            threads.append(thread)

        # ç­‰å¾…å®Œæˆ
        for thread in threads:
            thread.join()

        total_time = time.time() - start_time
        memory_usage, cpu_usage = self.system_monitor.stop_monitoring()

        if response_times:
            self.metrics['stress_test'] = PerformanceMetric(
                name="å‹åŠ›æµ‹è¯•",
                response_times=response_times,
                success_rate=(len(response_times) / request_count) * 100 if request_count > 0 else 0,
                throughput=len(response_times) / total_time,
                memory_usage=memory_usage,
                cpu_usage=cpu_usage,
                error_count=errors
            )

            print(f"  âœ… æ€»è¯·æ±‚æ•°: {request_count}")
            print(f"  âœ… æˆåŠŸè¯·æ±‚: {len(response_times)}")
            print(f"  âœ… å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.3f}s")
            print(f"  âœ… å¹³å‡ååé‡: {len(response_times) / total_time:.2f} req/s")

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*60)

        report_data = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_results': {},
            'summary': {}
        }

        total_tests = len(self.metrics)
        performance_score = 0

        for test_name, metric in self.metrics.items():
            if not metric.response_times:
                continue

            avg_response_time = statistics.mean(metric.response_times)
            min_response_time = min(metric.response_times)
            max_response_time = max(metric.response_times)
            p95_response_time = sorted(metric.response_times)[int(len(metric.response_times) * 0.95)]

            # è®¡ç®—æ€§èƒ½è¯„åˆ† (å“åº”æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜)
            response_score = max(0, 100 - (avg_response_time * 100))
            throughput_score = min(100, metric.throughput * 10)
            success_score = metric.success_rate

            test_score = (response_score + throughput_score + success_score) / 3
            performance_score += test_score

            print(f"\n{metric.name}:")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
            print(f"  å“åº”æ—¶é—´èŒƒå›´: {min_response_time:.3f}s - {max_response_time:.3f}s")
            print(f"  95%å“åº”æ—¶é—´: {p95_response_time:.3f}s")
            print(f"  ååé‡: {metric.throughput:.2f} req/s")
            print(f"  æˆåŠŸç‡: {metric.success_rate:.1f}%")
            print(f"  æ€§èƒ½è¯„åˆ†: {test_score:.1f}/100")

            if metric.memory_usage:
                avg_memory = statistics.mean(metric.memory_usage)
                max_memory = max(metric.memory_usage)
                print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f} MB")
                print(f"  å³°å€¼å†…å­˜ä½¿ç”¨: {max_memory:.1f} MB")

            if metric.cpu_usage:
                avg_cpu = statistics.mean(metric.cpu_usage)
                max_cpu = max(metric.cpu_usage)
                print(f"  å¹³å‡CPUä½¿ç”¨: {avg_cpu:.1f}%")
                print(f"  å³°å€¼CPUä½¿ç”¨: {max_cpu:.1f}%")

            # ä¿å­˜åˆ°æŠ¥å‘Šæ•°æ®
            report_data['test_results'][test_name] = {
                'avg_response_time': avg_response_time,
                'min_response_time': min_response_time,
                'max_response_time': max_response_time,
                'p95_response_time': p95_response_time,
                'throughput': metric.throughput,
                'success_rate': metric.success_rate,
                'performance_score': test_score,
                'memory_usage': {
                    'avg': statistics.mean(metric.memory_usage) if metric.memory_usage else 0,
                    'max': max(metric.memory_usage) if metric.memory_usage else 0
                },
                'cpu_usage': {
                    'avg': statistics.mean(metric.cpu_usage) if metric.cpu_usage else 0,
                    'max': max(metric.cpu_usage) if metric.cpu_usage else 0
                }
            }

        # æ€»ä½“è¯„åˆ†
        overall_score = performance_score / total_tests if total_tests > 0 else 0

        print(f"\næ€»ä½“æ€§èƒ½è¯„åˆ†: {overall_score:.1f}/100")

        # æ€§èƒ½ç­‰çº§
        if overall_score >= 80:
            grade = "ä¼˜ç§€"
        elif overall_score >= 60:
            grade = "è‰¯å¥½"
        elif overall_score >= 40:
            grade = "åŠæ ¼"
        else:
            grade = "éœ€è¦ä¼˜åŒ–"

        print(f"æ€§èƒ½ç­‰çº§: {grade}")

        report_data['summary'] = {
            'overall_score': overall_score,
            'grade': grade,
            'total_tests': total_tests
        }

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = Path("performance_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ è¯¦ç»†æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        return overall_score >= 60  # 60åˆ†åŠæ ¼

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹AIæ ‡ä¹¦ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print(f"ç›®æ ‡æœåŠ¡å™¨: {self.base_url}")
        print("-" * 60)

        start_time = time.time()

        try:
            # åŸºç¡€æ€§èƒ½æµ‹è¯•
            self.run_single_request_test()

            # å¹¶å‘æ€§èƒ½æµ‹è¯•
            self.run_concurrent_test()

            # å‹åŠ›æµ‹è¯•
            self.run_stress_test(duration_seconds=30)

        except KeyboardInterrupt:
            print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}")

        total_time = time.time() - start_time
        print(f"\næ€§èƒ½æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}s")

        # ç”ŸæˆæŠ¥å‘Š
        performance_acceptable = self.generate_performance_report()

        return performance_acceptable


class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.memory_usage = []
        self.cpu_usage = []
        self.monitor_thread = None

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.memory_usage = []
        self.cpu_usage = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§å¹¶è¿”å›æ•°æ®"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        return self.memory_usage.copy(), self.cpu_usage.copy()

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                # è·å–å½“å‰è¿›ç¨‹çš„èµ„æºä½¿ç”¨æƒ…å†µ
                process = psutil.Process()

                # å†…å­˜ä½¿ç”¨ (MB)
                memory_mb = process.memory_info().rss / 1024 / 1024
                self.memory_usage.append(memory_mb)

                # CPUä½¿ç”¨ç‡
                cpu_percent = process.cpu_percent()
                self.cpu_usage.append(cpu_percent)

                time.sleep(0.5)  # æ¯0.5ç§’é‡‡æ ·ä¸€æ¬¡
            except:
                break


def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    performance_acceptable = benchmark.run_all_tests()

    if performance_acceptable:
        print("\nğŸ‰ ç³»ç»Ÿæ€§èƒ½è¾¾æ ‡ï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸  ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ–ã€‚")
        sys.exit(1)


if __name__ == "__main__":
    main()