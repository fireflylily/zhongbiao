# ç« èŠ‚è§£ææ”¹è¿›å®æ–½è·¯çº¿å›¾

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

**ç›®æ ‡**: å°†ç« èŠ‚è§£æå‡†ç¡®ç‡ä»å½“å‰çš„ 74.2% (æ–¹æ³•3) / 238% (æ–¹æ³•2) æå‡è‡³ **95%+ è‡ªåŠ¨å‡†ç¡® + 100% æœ€ç»ˆå‡†ç¡®**

**ç­–ç•¥**: ä¸‰å±‚éªŒè¯ä½“ç³»
- **Layer 1**: æ™ºèƒ½æ··åˆè§£æï¼ˆè‡ªåŠ¨ï¼‰
- **Layer 2**: è¾¹ç•ŒéªŒè¯å’Œä¿®æ­£ï¼ˆè‡ªåŠ¨ï¼‰
- **Layer 3**: äººå·¥å¯è§†åŒ–æ ¡éªŒï¼ˆè¾…åŠ©ï¼‰

**é¢„æœŸæ”¶ç›Š**:
- âœ… ç« èŠ‚è¯†åˆ«å‡†ç¡®ç‡: 95%+
- âœ… å­—æ•°ç»Ÿè®¡å‡†ç¡®ç‡: 95-100%
- âœ… äººå·¥æ ¡éªŒæ—¶é—´: å‡å°‘ 80% (ä»20åˆ†é’Ÿé™è‡³4åˆ†é’Ÿ)
- âœ… åç»­å·¥ä½œåŸºç¡€ç¨³å›º: 100% (ç»äººå·¥ç¡®è®¤)

---

## ğŸ¯ Phase 1: æ ¸å¿ƒç®—æ³•æ”¹è¿› (ä¼˜å…ˆçº§: é«˜)

### 1.1 å¢å¼ºç‰ˆç²¾ç¡®åŒ¹é… (åŸºäºç›®å½•)

**æ–‡ä»¶**: `ai_tender_system/modules/tender_processing/structure_parser.py`

**æ”¹è¿›ç‚¹**:

#### âœ… å®ç°æ¨¡ç³ŠåŒ¹é…å‡½æ•°

```python
def _fuzzy_match_title(self, doc: Document, title: str, start_idx: int,
                       similarity_threshold: float = 0.85) -> Optional[int]:
    """
    æ¨¡ç³ŠåŒ¹é…ç« èŠ‚æ ‡é¢˜

    Args:
        doc: Wordæ–‡æ¡£å¯¹è±¡
        title: ç›®æ ‡æ ‡é¢˜ï¼ˆæ¥è‡ªç›®å½•ï¼‰
        start_idx: å¼€å§‹æœç´¢çš„æ®µè½ç´¢å¼•
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)

    Returns:
        åŒ¹é…åˆ°çš„æ®µè½ç´¢å¼•ï¼Œæœªæ‰¾åˆ°è¿”å›None

    åŒ¹é…ç­–ç•¥:
        1. å®Œå…¨åŒ¹é…: title == para.text
        2. è§„èŒƒåŒ–åŒ¹é…: normalize(title) == normalize(para.text)
        3. ç›¸ä¼¼åº¦åŒ¹é…: similarity(title, para.text) >= threshold
    """
    normalized_target = self._normalize_title(title)

    for idx in range(start_idx, len(doc.paragraphs)):
        para = doc.paragraphs[idx]
        para_text = para.text.strip()

        # å®Œå…¨åŒ¹é…
        if title == para_text:
            logger.debug(f"å®Œå…¨åŒ¹é…: '{title}' at para {idx}")
            return idx

        # è§„èŒƒåŒ–åŒ¹é…
        normalized_para = self._normalize_title(para_text)
        if normalized_target == normalized_para:
            logger.info(f"è§„èŒƒåŒ–åŒ¹é…: '{title}' â†’ '{para_text}' at para {idx}")
            return idx

        # ç›¸ä¼¼åº¦åŒ¹é…
        similarity = SequenceMatcher(None, normalized_target, normalized_para).ratio()
        if similarity >= similarity_threshold:
            logger.info(
                f"æ¨¡ç³ŠåŒ¹é…: '{title}' â†’ '{para_text}' "
                f"(ç›¸ä¼¼åº¦: {similarity:.2%}) at para {idx}"
            )
            return idx

    logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…: '{title}' (æœç´¢èŒƒå›´: {start_idx}-{len(doc.paragraphs)})")
    return None

def _normalize_title(self, text: str) -> str:
    """
    æ ‡é¢˜è§„èŒƒåŒ–

    è§„åˆ™:
    1. ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€å…¨è§’ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ï¼‰
    2. ç§»é™¤å¸¸è§æ ‡ç‚¹ç¬¦å·ï¼ˆå†’å·ã€é€—å·ã€å¥å·ï¼‰
    3. ç»Ÿä¸€å¤§å°å†™ï¼ˆè½¬å°å†™ï¼‰
    4. ç§»é™¤ç¼–å·åçš„å†’å·ï¼ˆå¦‚ "ç¬¬ä¸€éƒ¨åˆ†ï¼š" â†’ "ç¬¬ä¸€éƒ¨åˆ†"ï¼‰
    """
    import re

    # ç§»é™¤ç©ºç™½å’Œæ ‡ç‚¹
    text = re.sub(r'[\s\u3000\tï¼š:ã€ï¼Œã€‚]', '', text)

    # è½¬å°å†™
    text = text.lower()

    return text
```

**æµ‹è¯•ç”¨ä¾‹**:
```python
def test_fuzzy_matching():
    """æµ‹è¯•æ¨¡ç³ŠåŒ¹é…çš„å„ç§åœºæ™¯"""
    test_cases = [
        # (ç›®å½•æ ‡é¢˜, æ­£æ–‡æ ‡é¢˜, é¢„æœŸåŒ¹é…)
        ("ç¬¬ä¸‰éƒ¨åˆ† è¯„æ ‡åŠæ³•", "ç¬¬ä¸‰éƒ¨åˆ†è¯„æ ‡åŠæ³•", True),      # ç©ºæ ¼å·®å¼‚
        ("ç¬¬ä¸‰éƒ¨åˆ† è¯„æ ‡åŠæ³•", "ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯„æ ‡åŠæ³•", True),   # æ ‡ç‚¹å·®å¼‚
        ("ç¬¬ä¸‰éƒ¨åˆ† è¯„æ ‡åŠæ³•", "ç¬¬ä¸‰éƒ¨ä»½ è¯„æ ‡åŠæ³•", True),    # é”™åˆ«å­—
        ("ç¬¬ä¸€ç«  é¡¹ç›®æ¦‚è¿°", "1. é¡¹ç›®æ¦‚è¿°", False),           # ç¼–å·æ ¼å¼ä¸åŒ
    ]

    for toc_title, body_title, expected in test_cases:
        result = parser._fuzzy_match_title(doc, toc_title, 0)
        assert (result is not None) == expected
```

**å·¥ä½œé‡**: 2-3å°æ—¶

---

#### âœ… å¢å¼º `parse_by_toc_exact` æ–¹æ³•

**ä¿®æ”¹ä½ç½®**: `structure_parser.py:1200` (å¤§çº¦ä½ç½®ï¼Œéœ€è¦æŸ¥æ‰¾ `parse_by_toc_exact` æ–¹æ³•)

**æ”¹è¿›æ–¹æ¡ˆ**:

```python
def parse_by_toc_exact(self, doc: Document, force_parse: bool = False) -> Tuple[List[ChapterNode], Dict]:
    """
    åŸºäºç›®å½•çš„ç²¾ç¡®åŒ¹é…è§£æ (å¢å¼ºç‰ˆ)

    æ”¹è¿›ç‚¹:
    1. ä½¿ç”¨å¤šç­–ç•¥åŒ¹é…ï¼ˆå®Œå…¨åŒ¹é… â†’ æ¨¡ç³ŠåŒ¹é…ï¼‰
    2. è®°å½•åŒ¹é…çŠ¶æ€ï¼ˆmatched, fuzzy_matched, not_foundï¼‰
    3. è¿”å›æœªåŒ¹é…ç« èŠ‚åˆ—è¡¨ï¼ˆä¾›åç»­å¤„ç†ï¼‰
    """
    # 1. æ£€æµ‹ç›®å½•åŒºåŸŸ
    toc_info = self._find_toc_section(doc)
    if not toc_info:
        logger.warning("æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œæ— æ³•ä½¿ç”¨ç²¾ç¡®åŒ¹é…")
        return [], {"status": "no_toc"}

    toc_start_idx, toc_end_idx = toc_info['start_idx'], toc_info['end_idx']

    # 2. æå–ç›®å½•é¡¹
    toc_items = self._parse_toc_items(doc, toc_start_idx, toc_end_idx)
    logger.info(f"ä»ç›®å½•ä¸­æå–äº† {len(toc_items)} ä¸ªç« èŠ‚")

    # 3. åŒ¹é…æ­£æ–‡ä¸­çš„ç« èŠ‚ä½ç½®ï¼ˆğŸ†• å¢å¼ºåŒ¹é…ï¼‰
    chapters = []
    match_stats = {
        'total': len(toc_items),
        'exact_matched': 0,
        'fuzzy_matched': 0,
        'not_found': 0,
        'not_found_list': []
    }

    for item in toc_items:
        title = item['title']

        # ğŸ†• å¤šç­–ç•¥åŒ¹é…
        para_idx = self._find_chapter_in_body(
            doc,
            title,
            start_idx=toc_end_idx + 1,
            use_fuzzy=True  # å¯ç”¨æ¨¡ç³ŠåŒ¹é…
        )

        if para_idx is not None:
            chapter = ChapterNode(
                id=f"ch_{len(chapters) + 1}",
                level=item.get('level', 1),
                title=title,
                para_start_idx=para_idx,
                para_end_idx=None,  # ç¨åè®¡ç®—
                word_count=0,
                preview_text="",
                auto_selected=False,
                skip_recommended=False,
                content_tags=[]
            )

            # è®°å½•åŒ¹é…æ–¹å¼
            if para_idx == self._exact_match(doc, title, toc_end_idx + 1):
                match_stats['exact_matched'] += 1
                chapter.content_tags.append('exact_match')
            else:
                match_stats['fuzzy_matched'] += 1
                chapter.content_tags.append('fuzzy_match')

            chapters.append(chapter)
        else:
            # æœªåŒ¹é…åˆ°
            match_stats['not_found'] += 1
            match_stats['not_found_list'].append(title)
            logger.warning(f"æœªæ‰¾åˆ°ç« èŠ‚: {title}")

            # ğŸ†• ä»ç„¶åˆ›å»ºèŠ‚ç‚¹ï¼Œä½†æ ‡è®°ä¸ºæœªæ‰¾åˆ°
            chapter = ChapterNode(
                id=f"ch_{len(chapters) + 1}_not_found",
                level=item.get('level', 1),
                title=title,
                para_start_idx=None,
                para_end_idx=None,
                word_count=0,
                preview_text="[æœªæ‰¾åˆ°åŒ¹é…å†…å®¹]",
                auto_selected=False,
                skip_recommended=True,
                content_tags=['not_found', 'needs_manual_review']
            )
            chapters.append(chapter)

    # 4. è®¡ç®—ç« èŠ‚è¾¹ç•Œå’Œå­—æ•°
    chapters = self._calculate_chapter_boundaries(chapters, doc)

    # 5. è¿”å›ç»“æœå’Œç»Ÿè®¡
    logger.info(
        f"åŒ¹é…å®Œæˆ: {match_stats['exact_matched']} ç²¾ç¡®, "
        f"{match_stats['fuzzy_matched']} æ¨¡ç³Š, "
        f"{match_stats['not_found']} æœªæ‰¾åˆ°"
    )

    return chapters, {
        'status': 'success',
        'method': 'toc_exact_enhanced',
        'match_stats': match_stats
    }

def _find_chapter_in_body(self, doc: Document, title: str, start_idx: int,
                           use_fuzzy: bool = True) -> Optional[int]:
    """
    åœ¨æ­£æ–‡ä¸­æŸ¥æ‰¾ç« èŠ‚ä½ç½®ï¼ˆç»„åˆå¤šç§åŒ¹é…ç­–ç•¥ï¼‰
    """
    # ç­–ç•¥1: å®Œå…¨åŒ¹é…
    idx = self._exact_match(doc, title, start_idx)
    if idx is not None:
        return idx

    # ç­–ç•¥2: æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_fuzzy:
        idx = self._fuzzy_match_title(doc, title, start_idx, similarity_threshold=0.85)
        if idx is not None:
            return idx

    # ç­–ç•¥3: æ¨¡å¼åŒ¹é…ï¼ˆåŸºäºç¼–å·ï¼‰
    # TODO: å®ç°åŸºäºç¼–å·æ¨¡å¼çš„æ™ºèƒ½åŒ¹é…
    # ä¾‹å¦‚: "ç¬¬ä¸‰éƒ¨åˆ† è¯„æ ‡åŠæ³•" â†’ æœç´¢ä»¥ "ç¬¬ä¸‰éƒ¨åˆ†" æˆ– "ä¸‰ã€" å¼€å¤´çš„æ®µè½

    return None
```

**å·¥ä½œé‡**: 4-5å°æ—¶

---

### 1.2 è¿‡æ»¤å¼å¤§çº²è¯†åˆ« (æ— ç›®å½•æ—¶)

**æ”¹è¿›ç‚¹**:

#### âœ… å®ç°ä¼ªç« èŠ‚è¿‡æ»¤å‡½æ•°

```python
def _is_real_chapter_title(self, text: str, para_idx: int, doc: Document) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ï¼ˆè¿‡æ»¤ä¼ªæ ‡é¢˜ï¼‰

    Args:
        text: æ®µè½æ–‡æœ¬
        para_idx: æ®µè½ç´¢å¼•
        doc: æ–‡æ¡£å¯¹è±¡

    Returns:
        True: çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜
        False: ä¼ªæ ‡é¢˜ï¼ˆåº”è¿‡æ»¤ï¼‰

    è¿‡æ»¤è§„åˆ™:
        1. é•¿åº¦è¿‡çŸ­æˆ–è¿‡é•¿
        2. ä»¥å†’å·ç»“å°¾çš„å­—æ®µæ ‡é¢˜ï¼ˆå¦‚"é¡¹ç›®åç§°ï¼š"ï¼‰
        3. ä¸åŒ…å«ç« èŠ‚ç¼–å·æ ‡è®°
        4. æ ·å¼ä¸ç¬¦åˆç« èŠ‚æ ·å¼
    """
    text = text.strip()

    # è§„åˆ™1: é•¿åº¦æ£€æŸ¥
    if len(text) < 2 or len(text) > 100:
        logger.debug(f"è¿‡æ»¤: é•¿åº¦ä¸åˆé€‚ ({len(text)}å­—) - '{text[:20]}...'")
        return False

    # è§„åˆ™2: æ’é™¤å­—æ®µæ ‡é¢˜ï¼ˆå†’å·ç»“å°¾ï¼‰
    if re.match(r'^[^ï¼š:]{2,15}[ï¼š:]$', text):
        logger.debug(f"è¿‡æ»¤: å­—æ®µæ ‡é¢˜ - '{text}'")
        return False

    # è§„åˆ™3: å¿…é¡»åŒ…å«ç« èŠ‚ç¼–å·æ ‡è®°
    has_numbering = False
    for pattern in self.NUMBERING_PATTERNS:
        if re.search(pattern, text):
            has_numbering = True
            break

    if not has_numbering:
        logger.debug(f"è¿‡æ»¤: æ— ç« èŠ‚ç¼–å· - '{text}'")
        return False

    # è§„åˆ™4: æ ·å¼æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
    # æ£€æŸ¥æ˜¯å¦æœ‰åŠ ç²—ã€å­—å·ç­‰æ ¼å¼ç‰¹å¾
    para = doc.paragraphs[para_idx]
    if para.runs:
        first_run = para.runs[0]
        is_bold = first_run.bold
        font_size = first_run.font.size

        # ç« èŠ‚æ ‡é¢˜é€šå¸¸æ˜¯åŠ ç²—ä¸”å­—å·è¾ƒå¤§
        # è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©åˆ¤æ–­ï¼Œä¸ä½œä¸ºå†³å®šæ€§æ¡ä»¶
        if is_bold or (font_size and font_size.pt >= 14):
            logger.debug(f"æ ·å¼æç¤º: å¯èƒ½æ˜¯ç« èŠ‚ (åŠ ç²—={is_bold}, å­—å·={font_size})")

    logger.debug(f"âœ“ è¯†åˆ«ä¸ºçœŸç« èŠ‚: '{text}'")
    return True
```

**æµ‹è¯•ç”¨ä¾‹**:
```python
def test_chapter_title_filtering():
    """æµ‹è¯•ç« èŠ‚æ ‡é¢˜è¿‡æ»¤"""
    test_cases = [
        # (æ–‡æœ¬, é¢„æœŸç»“æœ)
        ("ç¬¬ä¸€éƒ¨åˆ† æ‹›æ ‡å…¬å‘Š", True),           # çœŸç« èŠ‚
        ("1.1 é¡¹ç›®æ¦‚è¿°", True),               # çœŸç« èŠ‚
        ("é¡¹ç›®åç§°ï¼š", False),                # å­—æ®µæ ‡é¢˜
        ("æ‹›æ ‡ç¼–å·ï¼š", False),                # å­—æ®µæ ‡é¢˜
        ("è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æè¿°æ€§æ–‡å­—...", False),  # æ™®é€šæ®µè½
        ("é™„ä»¶1ï¼šæŠ€æœ¯è§„æ ¼ä¹¦", True),          # çœŸç« èŠ‚ï¼ˆé™„ä»¶ï¼‰
    ]

    for text, expected in test_cases:
        result = parser._is_real_chapter_title(text, 0, doc)
        assert result == expected, f"Failed for: {text}"
```

**å·¥ä½œé‡**: 3-4å°æ—¶

---

#### âœ… ä¿®æ­£é‡å¤è®¡ç®—é—®é¢˜

**é—®é¢˜æ ¹æº**: `_calculate_statistics` é€’å½’ç´¯åŠ çˆ¶å­ç« èŠ‚å­—æ•°

**è§£å†³æ–¹æ¡ˆ**:

```python
def _calculate_statistics(self, chapter_tree: List[ChapterNode]) -> Dict:
    """
    è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿®æ­£ç‰ˆï¼šé¿å…é‡å¤è®¡æ•°ï¼‰

    ç­–ç•¥:
    - åªç»Ÿè®¡å¶å­èŠ‚ç‚¹çš„å­—æ•°ï¼ˆæ— å­ç« èŠ‚çš„ç« èŠ‚ï¼‰
    - æˆ–è€…ç»Ÿè®¡æ ¹èŠ‚ç‚¹æ—¶æ’é™¤å·²è¢«å­ç« èŠ‚è¦†ç›–çš„å†…å®¹
    """
    stats = {
        "total_chapters": 0,
        "total_words": 0,
        "avg_words_per_chapter": 0,
        "max_level": 0,
        "chapters_by_level": {}
    }

    def traverse(chapters, current_level=1):
        """é€’å½’éå†ç« èŠ‚æ ‘"""
        for ch in chapters:
            stats["total_chapters"] += 1
            stats["max_level"] = max(stats["max_level"], current_level)

            # æ›´æ–°å±‚çº§ç»Ÿè®¡
            level_key = f"level_{current_level}"
            stats["chapters_by_level"][level_key] = \
                stats["chapters_by_level"].get(level_key, 0) + 1

            # ğŸ†• ä¿®æ­£: åªç»Ÿè®¡å¶å­èŠ‚ç‚¹å­—æ•°
            if not ch.children:
                # å¶å­èŠ‚ç‚¹: ç»Ÿè®¡å…¨éƒ¨å†…å®¹
                stats["total_words"] += ch.word_count
            else:
                # éå¶å­èŠ‚ç‚¹: åªç»Ÿè®¡æ ‡é¢˜åˆ°ç¬¬ä¸€ä¸ªå­ç« èŠ‚ä¹‹é—´çš„å†…å®¹
                # è¿™éƒ¨åˆ†å·²åœ¨ calculate_leaf_content_only ä¸­å¤„ç†
                intro_word_count = self._calculate_intro_content(ch)
                stats["total_words"] += intro_word_count

            # é€’å½’å¤„ç†å­ç« èŠ‚
            if ch.children:
                traverse(ch.children, current_level + 1)

    traverse(chapter_tree)

    if stats["total_chapters"] > 0:
        stats["avg_words_per_chapter"] = stats["total_words"] // stats["total_chapters"]

    return stats

def _calculate_intro_content(self, chapter: ChapterNode) -> int:
    """
    è®¡ç®—éå¶å­èŠ‚ç‚¹çš„å¼•å¯¼å†…å®¹å­—æ•°

    å¼•å¯¼å†…å®¹ = ç« èŠ‚æ ‡é¢˜å åˆ° ç¬¬ä¸€ä¸ªå­ç« èŠ‚å‰ çš„å†…å®¹
    """
    if not chapter.children:
        return chapter.word_count

    # è®¡ç®—èŒƒå›´: para_start_idx åˆ° children[0].para_start_idx - 1
    start = chapter.para_start_idx
    end = chapter.children[0].para_start_idx - 1

    if end <= start:
        return 0

    # è¿™é‡Œéœ€è¦è®¿é—®åŸå§‹æ–‡æ¡£æ¥è®¡ç®—å­—æ•°
    # ä¸ºäº†é¿å…é‡æ–°è¯»å–ï¼Œå»ºè®®åœ¨è§£ææ—¶å°±è®¡ç®—å¥½
    return 0  # æš‚æ—¶è¿”å›0ï¼Œå®é™…åº”è¯¥è®¡ç®—
```

**å·¥ä½œé‡**: 2-3å°æ—¶

---

### 1.3 æ··åˆè§£æç­–ç•¥

**å®ç°å‡½æ•°**:

```python
def parse_document_structure(self, file_path: str, methods: List[str] = None,
                              fallback: bool = True,
                              enable_hybrid: bool = True) -> Dict:
    """
    è§£ææ–‡æ¡£ç»“æ„ï¼ˆæ”¯æŒæ··åˆç­–ç•¥ï¼‰

    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        methods: ä½¿ç”¨çš„è§£ææ–¹æ³•åˆ—è¡¨ ['toc_exact', 'outline_level']
        fallback: æ˜¯å¦å¯ç”¨é™çº§ç­–ç•¥
        enable_hybrid: æ˜¯å¦å¯ç”¨æ··åˆè§£æï¼ˆğŸ†•ï¼‰

    Returns:
        è§£æç»“æœå­—å…¸ï¼ŒåŒ…å«ç« èŠ‚æ ‘å’Œç»Ÿè®¡ä¿¡æ¯
    """
    doc = Document(file_path)

    # ğŸ†• æ··åˆè§£ææ¨¡å¼
    if enable_hybrid:
        logger.info("ğŸ”„ å¯ç”¨æ··åˆè§£æç­–ç•¥")

        # æ­¥éª¤1: å°è¯•åŸºäºç›®å½•çš„ç²¾ç¡®åŒ¹é…
        toc_chapters, toc_meta = self.parse_by_toc_exact(doc, force_parse=False)

        if toc_meta.get('status') == 'success':
            not_found_count = toc_meta['match_stats']['not_found']

            if not_found_count == 0:
                # å®Œç¾åŒ¹é…ï¼Œç›´æ¥è¿”å›
                logger.info("âœ… ç›®å½•ç²¾ç¡®åŒ¹é…æˆåŠŸï¼Œæ— éœ€æ··åˆ")
                return self._build_result(toc_chapters, toc_meta, doc)

            else:
                # éƒ¨åˆ†åŒ¹é…å¤±è´¥ï¼Œå¯ç”¨æ··åˆç­–ç•¥
                logger.warning(f"âš ï¸  {not_found_count} ä¸ªç« èŠ‚æœªåŒ¹é…ï¼Œå¯ç”¨æ··åˆç­–ç•¥")

                # æ­¥éª¤2: ä½¿ç”¨å¤§çº²çº§åˆ«è¡¥å……
                outline_chapters, outline_meta = self.parse_by_outline_level(
                    doc,
                    filter_pseudo_titles=True  # ğŸ†• å¯ç”¨è¿‡æ»¤
                )

                # æ­¥éª¤3: æ™ºèƒ½åˆå¹¶
                merged_chapters = self._smart_merge_chapters(
                    toc_chapters,
                    outline_chapters,
                    toc_meta['match_stats']['not_found_list']
                )

                # æ­¥éª¤4: è¾¹ç•ŒéªŒè¯å’Œä¿®æ­£
                validated_chapters, issues = self._validate_and_fix_boundaries(
                    merged_chapters,
                    doc
                )

                return self._build_result(validated_chapters, {
                    'status': 'hybrid_success',
                    'method': 'toc_exact + outline_level',
                    'toc_matched': toc_meta['match_stats']['exact_matched'] +
                                   toc_meta['match_stats']['fuzzy_matched'],
                    'outline_è£œå……': not_found_count,
                    'validation_issues': issues
                }, doc)

        else:
            # æ— ç›®å½•ï¼Œé™çº§åˆ°å¤§çº²çº§åˆ«
            logger.info("ğŸ“‹ æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨è¿‡æ»¤å¼å¤§çº²è¯†åˆ«")
            outline_chapters, outline_meta = self.parse_by_outline_level(
                doc,
                filter_pseudo_titles=True
            )
            return self._build_result(outline_chapters, outline_meta, doc)

    # åŸæœ‰é€»è¾‘ä¿æŒä¸å˜...
    else:
        # æ—§çš„å•ä¸€æ–¹æ³•è§£æé€»è¾‘
        pass

def _smart_merge_chapters(self, toc_chapters: List[ChapterNode],
                          outline_chapters: List[ChapterNode],
                          not_found_titles: List[str]) -> List[ChapterNode]:
    """
    æ™ºèƒ½åˆå¹¶ä¸¤ç§æ–¹æ³•çš„è§£æç»“æœ

    ç­–ç•¥:
    1. TOCåŒ¹é…çš„ç« èŠ‚ä¼˜å…ˆ
    2. å¯¹äºæœªåŒ¹é…çš„ç« èŠ‚ï¼Œä»outline_chaptersä¸­æŸ¥æ‰¾æœ€ä½³åŒ¹é…
    3. ä¿æŒåŸæœ‰é¡ºåº
    """
    merged = []

    for toc_ch in toc_chapters:
        if 'not_found' in toc_ch.content_tags:
            # ä»å¤§çº²çº§åˆ«ç»“æœä¸­æŸ¥æ‰¾åŒ¹é…
            best_match = self._find_best_outline_match(
                toc_ch.title,
                outline_chapters
            )

            if best_match:
                logger.info(f"ğŸ”— æ··åˆåŒ¹é…æˆåŠŸ: '{toc_ch.title}' â† {best_match.title}")
                toc_ch.para_start_idx = best_match.para_start_idx
                toc_ch.para_end_idx = best_match.para_end_idx
                toc_ch.word_count = best_match.word_count
                toc_ch.preview_text = best_match.preview_text
                toc_ch.content_tags.remove('not_found')
                toc_ch.content_tags.append('hybrid_matched')
                toc_ch.skip_recommended = False
            else:
                logger.warning(f"âŒ ä»æœªæ‰¾åˆ°åŒ¹é…: '{toc_ch.title}'")

        merged.append(toc_ch)

    return merged

def _find_best_outline_match(self, toc_title: str,
                              outline_chapters: List[ChapterNode]) -> Optional[ChapterNode]:
    """
    ä»å¤§çº²çº§åˆ«ç»“æœä¸­æŸ¥æ‰¾æœ€ä½³åŒ¹é…

    ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ç®—æ³•
    """
    normalized_target = self._normalize_title(toc_title)
    best_match = None
    best_similarity = 0.0

    for outline_ch in outline_chapters:
        normalized_outline = self._normalize_title(outline_ch.title)
        similarity = SequenceMatcher(None, normalized_target, normalized_outline).ratio()

        if similarity > best_similarity:
            best_similarity = similarity
            best_match = outline_ch

    # åªè¿”å›ç›¸ä¼¼åº¦ >= 0.75 çš„åŒ¹é…
    if best_similarity >= 0.75:
        logger.debug(f"æ‰¾åˆ°åŒ¹é…: '{toc_title}' â†’ '{best_match.title}' (ç›¸ä¼¼åº¦: {best_similarity:.2%})")
        return best_match

    return None
```

**å·¥ä½œé‡**: 5-6å°æ—¶

---

## ğŸ”§ Phase 2: è¾¹ç•ŒéªŒè¯å’Œä¿®æ­£ (ä¼˜å…ˆçº§: é«˜)

### 2.1 è‡ªåŠ¨è¾¹ç•ŒéªŒè¯

**æ–‡ä»¶**: `ai_tender_system/modules/tender_processing/structure_parser.py`

**å®ç°å‡½æ•°**:

```python
def _validate_and_fix_boundaries(self, chapters: List[ChapterNode],
                                  doc: Document,
                                  expected_total_words: Optional[int] = None) -> Tuple[List[ChapterNode], List[Dict]]:
    """
    éªŒè¯å’Œä¿®æ­£ç« èŠ‚è¾¹ç•Œ

    æ£€æŸ¥é¡¹:
    1. ç« èŠ‚é—´æ— é‡å 
    2. ç« èŠ‚é—´æ— å¤§é—´éš™
    3. å­—æ•°åˆç†æ€§ï¼ˆé0ï¼‰
    4. æ€»å­—æ•°åŒ¹é…ï¼ˆå¦‚æœæä¾›äº†é¢„æœŸå€¼ï¼‰

    Returns:
        (ä¿®æ­£åçš„ç« èŠ‚åˆ—è¡¨, é—®é¢˜åˆ—è¡¨)
    """
    issues = []

    # æ£€æŸ¥1: é‡å å’Œé—´éš™
    for i in range(len(chapters) - 1):
        current = chapters[i]
        next_ch = chapters[i + 1]

        if current.para_start_idx is None or next_ch.para_start_idx is None:
            continue

        # æ£€æŸ¥é‡å 
        if current.para_end_idx and current.para_end_idx >= next_ch.para_start_idx:
            issues.append({
                'type': 'OVERLAP',
                'severity': 'error',
                'chapter_index': i,
                'chapter_title': current.title,
                'description': f"ç« èŠ‚ '{current.title}' çš„ç»“æŸä½ç½® ({current.para_end_idx}) "
                               f"è¶…è¿‡äº†ä¸‹ä¸€ç« èŠ‚ '{next_ch.title}' çš„èµ·å§‹ä½ç½® ({next_ch.para_start_idx})",
                'auto_fix': 'adjust_end_idx'
            })

            # è‡ªåŠ¨ä¿®æ­£
            logger.warning(f"ğŸ”§ è‡ªåŠ¨ä¿®æ­£é‡å : '{current.title}' end_idx {current.para_end_idx} â†’ {next_ch.para_start_idx - 1}")
            current.para_end_idx = next_ch.para_start_idx - 1

        # æ£€æŸ¥é—´éš™
        elif current.para_end_idx:
            gap = next_ch.para_start_idx - current.para_end_idx - 1
            if gap > 5:  # é—´éš™è¶…è¿‡5ä¸ªæ®µè½
                gap_content = '\n'.join(
                    p.text for p in doc.paragraphs[current.para_end_idx + 1:next_ch.para_start_idx]
                    if p.text.strip()
                )

                if gap_content:
                    issues.append({
                        'type': 'GAP',
                        'severity': 'warning',
                        'chapter_index': i,
                        'location': f"ç« èŠ‚ '{current.title}' å’Œ '{next_ch.title}' ä¹‹é—´",
                        'gap_size': gap,
                        'gap_content_preview': gap_content[:200] + ('...' if len(gap_content) > 200 else ''),
                        'description': f"å­˜åœ¨ {gap} ä¸ªæ®µè½çš„é—´éš™ï¼Œå¯èƒ½é—æ¼äº†å†…å®¹"
                    })

    # æ£€æŸ¥2: 0å­—ç« èŠ‚
    for i, chapter in enumerate(chapters):
        if chapter.word_count == 0 and chapter.title not in ['å°é¢', 'ç›®å½•', 'æ‰‰é¡µ']:
            issues.append({
                'type': 'ZERO_WORDS',
                'severity': 'error',
                'chapter_index': i,
                'chapter_title': chapter.title,
                'description': f"ç« èŠ‚ '{chapter.title}' å­—æ•°ä¸º0ï¼Œå¯èƒ½è¾¹ç•Œé”™è¯¯æˆ–åŒ¹é…å¤±è´¥",
                'needs_manual_review': True
            })

    # æ£€æŸ¥3: æ€»å­—æ•°éªŒè¯
    if expected_total_words:
        total_calculated = sum(ch.word_count for ch in chapters if ch.word_count)
        diff = total_calculated - expected_total_words
        diff_percent = abs(diff) / expected_total_words * 100

        if diff_percent > 10:  # å·®å¼‚è¶…è¿‡10%
            issues.append({
                'type': 'TOTAL_MISMATCH',
                'severity': 'error',
                'calculated_words': total_calculated,
                'expected_words': expected_total_words,
                'difference': diff,
                'difference_percent': diff_percent,
                'description': f"æ€»å­—æ•°å·®å¼‚è¿‡å¤§: è®¡ç®— {total_calculated} vs é¢„æœŸ {expected_total_words} "
                               f"(å·®å¼‚ {diff_percent:.1f}%)"
            })

    logger.info(f"è¾¹ç•ŒéªŒè¯å®Œæˆ: å‘ç° {len(issues)} ä¸ªé—®é¢˜")
    return chapters, issues
```

**å·¥ä½œé‡**: 4-5å°æ—¶

---

### 2.2 åç«¯APIå¢å¼º

**æ–‡ä»¶**: `ai_tender_system/web/blueprints/api_parser_debug_bp.py`

**æ–°å¢APIç«¯ç‚¹**:

```python
@api_parser_debug_bp.route('/api/parser-debug/validate-boundaries', methods=['POST'])
def validate_chapter_boundaries():
    """
    éªŒè¯ç« èŠ‚è¾¹ç•Œï¼ˆæä¾›é¢„æœŸå­—æ•°ï¼‰

    Request:
    {
        "test_id": 123,
        "expected_total_words": 28600  # ä»Wordæ–‡æ¡£ç»Ÿè®¡è·å–
    }

    Response:
    {
        "status": "success",
        "validation_result": {
            "total_chapters": 6,
            "calculated_words": 21212,
            "expected_words": 28600,
            "match_percentage": 74.2,
            "issues": [
                {
                    "type": "ZERO_WORDS",
                    "severity": "error",
                    "chapter_title": "ç¬¬ä¸‰éƒ¨åˆ† è¯„æ ‡åŠæ³•",
                    "description": "..."
                }
            ]
        }
    }
    """
    data = request.json
    test_id = data.get('test_id')
    expected_words = data.get('expected_total_words')

    # è·å–è§£æç»“æœ
    test = ParserDebugTest.get_by_id(test_id)
    result = json.loads(test.result_data)

    # æ‰§è¡ŒéªŒè¯
    parser = DocumentStructureParser()
    doc = Document(test.document_path)

    chapters = [ChapterNode(**ch) for ch in result['chapters']]
    validated_chapters, issues = parser._validate_and_fix_boundaries(
        chapters,
        doc,
        expected_total_words=expected_words
    )

    # è®¡ç®—åŒ¹é…åº¦
    total_calculated = sum(ch.word_count for ch in validated_chapters)
    match_percentage = (total_calculated / expected_words * 100) if expected_words else 100

    return jsonify({
        'status': 'success',
        'validation_result': {
            'total_chapters': len(validated_chapters),
            'calculated_words': total_calculated,
            'expected_words': expected_words,
            'match_percentage': round(match_percentage, 1),
            'issues': issues
        }
    })
```

**å·¥ä½œé‡**: 3-4å°æ—¶

---

## ğŸ¨ Phase 3: äººå·¥æ ¡éªŒç•Œé¢ (ä¼˜å…ˆçº§: ä¸­)

### 3.1 è¾¹ç•Œè°ƒæ•´ç»„ä»¶

**æ–‡ä»¶**: `frontend/src/components/ChapterBoundaryEditor.vue` (æ–°å»º)

**åŠŸèƒ½**:
1. å¯è§†åŒ–æ˜¾ç¤ºç« èŠ‚è¾¹ç•Œ
2. æ‰‹åŠ¨è°ƒæ•´èµ·æ­¢æ®µè½ç´¢å¼•
3. å®æ—¶é¢„è§ˆå†…å®¹
4. å®æ—¶è®¡ç®—å­—æ•°
5. æ˜¾ç¤ºéªŒè¯é—®é¢˜

**å®ç°éª¨æ¶** (è§ CHAPTER_PARSING_IMPROVEMENT.md ç¬¬ 448-634 è¡Œ)

**å·¥ä½œé‡**: 8-10å°æ—¶

---

### 3.2 é›†æˆåˆ°è§£æå¯¹æ¯”é¡µé¢

**æ–‡ä»¶**: `frontend/src/views/Debug/ParserComparison.vue`

**ä¿®æ”¹ç‚¹**:

```vue
<template>
  <div class="parser-comparison">
    <!-- åŸæœ‰å¯¹æ¯”å¡ç‰‡ -->

    <!-- ğŸ†• è¾¹ç•Œæ ¡éªŒé¢æ¿ -->
    <el-collapse v-model="activePanel" v-if="showBoundaryEditor">
      <el-collapse-item title="ç« èŠ‚è¾¹ç•Œæ ¡éªŒå’Œè°ƒæ•´" name="boundary">
        <ChapterBoundaryEditor
          :chapters="selectedMethodResult.chapters"
          :expected-words="expectedTotalWords"
          :total-paragraphs="documentTotalParagraphs"
          @boundaries-confirmed="onBoundariesConfirmed"
        />
      </el-collapse-item>
    </el-collapse>
  </div>
</template>

<script setup>
import ChapterBoundaryEditor from '@/components/ChapterBoundaryEditor.vue'

const showBoundaryEditor = ref(false)
const expectedTotalWords = ref(null)

// ä»Wordæ–‡æ¡£è·å–é¢„æœŸå­—æ•°
async function loadExpectedWordCount() {
  const response = await api.getDocumentStats(documentId)
  expectedTotalWords.value = response.total_words
}

function onBoundariesConfirmed(adjustedChapters) {
  // ä¿å­˜è°ƒæ•´åçš„ç« èŠ‚è¾¹ç•Œ
  api.saveChapterBoundaries(testId, adjustedChapters)
  ElMessage.success('ç« èŠ‚è¾¹ç•Œå·²ç¡®è®¤')
}
</script>
```

**å·¥ä½œé‡**: 4-5å°æ—¶

---

## ğŸ“Š Phase 4: æµ‹è¯•å’Œä¼˜åŒ– (ä¼˜å…ˆçº§: é«˜)

### 4.1 å‡†å¤‡æµ‹è¯•æ•°æ®é›†

**ç›®æ ‡**: æ”¶é›† 10-15 ä¸ªçœŸå®æ ‡ä¹¦æ–‡æ¡£

**åˆ†ç±»**:
- æœ‰ç›®å½•æ–‡æ¡£: 5ä¸ª
- æ— ç›®å½•æ–‡æ¡£: 5ä¸ª
- ç‰¹æ®Šæ ¼å¼æ–‡æ¡£: 3-5ä¸ªï¼ˆå¤æ‚ç›®å½•ã€å¤šå±‚çº§ã€é™„ä»¶å¤šç­‰ï¼‰

**æµ‹è¯•æŒ‡æ ‡**:
- ç« èŠ‚è¯†åˆ«å‡†ç¡®ç‡ (%)
- å­—æ•°ç»Ÿè®¡å‡†ç¡®ç‡ (%)
- è¾¹ç•Œå‡†ç¡®æ€§ (äººå·¥éªŒè¯)
- æ€§èƒ½ (è§£ææ—¶é—´)

**å·¥ä½œé‡**: 4-6å°æ—¶ï¼ˆåŒ…æ‹¬æ•°æ®æ”¶é›†å’Œäººå·¥æ ‡æ³¨ï¼‰

---

### 4.2 è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

**æ–‡ä»¶**: `tests/test_structure_parser_accuracy.py` (æ–°å»º)

```python
#!/usr/bin/env python3
"""
ç»“æ„è§£æå™¨å‡†ç¡®æ€§æµ‹è¯•
"""
import pytest
from ai_tender_system.modules.tender_processing.structure_parser import DocumentStructureParser

# æµ‹è¯•æ•°æ®é›†ï¼ˆäººå·¥æ ‡æ³¨çš„çœŸå®æ ‡ä¹¦ï¼‰
TEST_DOCUMENTS = [
    {
        'file_path': '/path/to/doc1.docx',
        'expected_chapters': 6,
        'expected_total_words': 28600,
        'has_toc': True,
        'chapter_titles': ['ç¬¬ä¸€éƒ¨åˆ† æ‹›æ ‡å…¬å‘Š', 'ç¬¬äºŒéƒ¨åˆ† æŠ•æ ‡äººé¡»çŸ¥', ...]
    },
    # ... æ›´å¤šæµ‹è¯•æ–‡æ¡£
]

@pytest.mark.parametrize("doc_info", TEST_DOCUMENTS)
def test_parsing_accuracy(doc_info):
    """æµ‹è¯•è§£æå‡†ç¡®ç‡"""
    parser = DocumentStructureParser()
    result = parser.parse_document_structure(
        doc_info['file_path'],
        enable_hybrid=True
    )

    # éªŒè¯ç« èŠ‚æ•°é‡
    assert len(result['chapters']) == doc_info['expected_chapters'], \
        f"ç« èŠ‚æ•°é‡ä¸åŒ¹é…: {len(result['chapters'])} vs {doc_info['expected_chapters']}"

    # éªŒè¯æ€»å­—æ•°ï¼ˆå…è®¸5%è¯¯å·®ï¼‰
    total_words = result['statistics']['total_words']
    expected_words = doc_info['expected_total_words']
    diff_percent = abs(total_words - expected_words) / expected_words * 100

    assert diff_percent <= 5, \
        f"å­—æ•°å·®å¼‚è¿‡å¤§: {total_words} vs {expected_words} (å·®å¼‚ {diff_percent:.1f}%)"

    # éªŒè¯ç« èŠ‚æ ‡é¢˜
    parsed_titles = [ch['title'] for ch in result['chapters']]
    for expected_title in doc_info['chapter_titles']:
        assert any(
            SequenceMatcher(None, expected_title, parsed_title).ratio() >= 0.9
            for parsed_title in parsed_titles
        ), f"æœªæ‰¾åˆ°åŒ¹é…æ ‡é¢˜: {expected_title}"

def test_performance():
    """æµ‹è¯•è§£ææ€§èƒ½"""
    import time

    parser = DocumentStructureParser()

    for doc_info in TEST_DOCUMENTS:
        start_time = time.time()
        result = parser.parse_document_structure(doc_info['file_path'])
        elapsed = time.time() - start_time

        # è§£ææ—¶é—´åº” < 5ç§’
        assert elapsed < 5.0, f"è§£ææ—¶é—´è¿‡é•¿: {elapsed:.2f}s"
```

**å·¥ä½œé‡**: 6-8å°æ—¶

---

## ğŸ“… å®æ–½æ—¶é—´è¡¨

### Sprint 1 (ç¬¬1-2å‘¨): æ ¸å¿ƒç®—æ³•æ”¹è¿›

| ä»»åŠ¡ | å·¥ä½œé‡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|--------|--------|------|
| 1.1.1 æ¨¡ç³ŠåŒ¹é…å‡½æ•° | 2-3h | - | â³ å¾…å¼€å§‹ |
| 1.1.2 å¢å¼º parse_by_toc_exact | 4-5h | - | â³ å¾…å¼€å§‹ |
| 1.2.1 ä¼ªç« èŠ‚è¿‡æ»¤ | 3-4h | - | â³ å¾…å¼€å§‹ |
| 1.2.2 ä¿®æ­£é‡å¤è®¡ç®— | 2-3h | - | â³ å¾…å¼€å§‹ |
| 1.3 æ··åˆè§£æç­–ç•¥ | 5-6h | - | â³ å¾…å¼€å§‹ |
| **å°è®¡** | **16-21h** | | |

### Sprint 2 (ç¬¬3å‘¨): è¾¹ç•ŒéªŒè¯

| ä»»åŠ¡ | å·¥ä½œé‡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|--------|--------|------|
| 2.1 è‡ªåŠ¨è¾¹ç•ŒéªŒè¯ | 4-5h | - | â³ å¾…å¼€å§‹ |
| 2.2 åç«¯APIå¢å¼º | 3-4h | - | â³ å¾…å¼€å§‹ |
| **å°è®¡** | **7-9h** | | |

### Sprint 3 (ç¬¬4-5å‘¨): äººå·¥æ ¡éªŒç•Œé¢

| ä»»åŠ¡ | å·¥ä½œé‡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|--------|--------|------|
| 3.1 è¾¹ç•Œè°ƒæ•´ç»„ä»¶ | 8-10h | - | â³ å¾…å¼€å§‹ |
| 3.2 é›†æˆåˆ°è§£æå¯¹æ¯”é¡µé¢ | 4-5h | - | â³ å¾…å¼€å§‹ |
| **å°è®¡** | **12-15h** | | |

### Sprint 4 (ç¬¬6å‘¨): æµ‹è¯•å’Œä¼˜åŒ–

| ä»»åŠ¡ | å·¥ä½œé‡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|--------|--------|------|
| 4.1 å‡†å¤‡æµ‹è¯•æ•°æ®é›† | 4-6h | - | â³ å¾…å¼€å§‹ |
| 4.2 è‡ªåŠ¨åŒ–æµ‹è¯• | 6-8h | - | â³ å¾…å¼€å§‹ |
| 4.3 é—®é¢˜ä¿®å¤å’Œä¼˜åŒ– | 8-10h | - | â³ å¾…å¼€å§‹ |
| **å°è®¡** | **18-24h** | | |

**æ€»å·¥ä½œé‡**: 53-69å°æ—¶ (çº¦ 1.5-2 ä¸ªæœˆï¼ŒæŒ‰æ¯å‘¨ 20 å°æ—¶è®¡ç®—)

---

## ğŸ¯ æˆåŠŸæŒ‡æ ‡

### é‡åŒ–ç›®æ ‡

| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | éªŒè¯æ–¹å¼ |
|------|--------|--------|----------|
| **å­—æ•°å‡†ç¡®ç‡** | 74.2% (æ–¹æ³•3) | **95%+** | è‡ªåŠ¨æµ‹è¯• (ä¸Wordç»Ÿè®¡å¯¹æ¯”) |
| **ç« èŠ‚è¯†åˆ«ç‡** | 50% (3/6ç« èŠ‚0å­—) | **100%** | äººå·¥éªŒè¯ |
| **0å­—ç« èŠ‚æ¯”ä¾‹** | 50% | **< 5%** | è‡ªåŠ¨æ£€æµ‹ |
| **äººå·¥æ ¡éªŒæ—¶é—´** | ~20åˆ†é’Ÿ/æ–‡æ¡£ | **< 5åˆ†é’Ÿ** | æ—¶é—´æµ‹é‡ |
| **é—®é¢˜è‡ªåŠ¨æ£€æµ‹ç‡** | 0% | **100%** | éªŒè¯è¦†ç›–æ‰€æœ‰å·²çŸ¥é—®é¢˜ç±»å‹ |

### éªŒæ”¶æ ‡å‡†

#### Phase 1 éªŒæ”¶:
- [ ] æ¨¡ç³ŠåŒ¹é…é€šè¿‡ç‡ >= 90% (æµ‹è¯•ç”¨ä¾‹)
- [ ] ä¼ªç« èŠ‚è¿‡æ»¤å‡†ç¡®ç‡ >= 95%
- [ ] æ··åˆè§£æå­—æ•°å‡†ç¡®ç‡ >= 85%

#### Phase 2 éªŒæ”¶:
- [ ] è¾¹ç•ŒéªŒè¯è¦†ç›–æ‰€æœ‰é—®é¢˜ç±»å‹ï¼ˆé‡å ã€é—´éš™ã€0å­—ã€æ€»æ•°ï¼‰
- [ ] è‡ªåŠ¨ä¿®æ­£æˆåŠŸç‡ >= 80%

#### Phase 3 éªŒæ”¶:
- [ ] ç•Œé¢å¯ç”¨æ€§æµ‹è¯•é€šè¿‡ï¼ˆ5åç”¨æˆ·æµ‹è¯•ï¼‰
- [ ] æ‰‹åŠ¨è°ƒæ•´åå­—æ•°åŒ¹é…åº¦ >= 98%

#### Phase 4 éªŒæ”¶:
- [ ] 10ä¸ªæµ‹è¯•æ–‡æ¡£çš„å¹³å‡å‡†ç¡®ç‡ >= 95%
- [ ] è§£ææ—¶é—´ < 5ç§’/æ–‡æ¡£
- [ ] 0 critical bugs

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# 1. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/chapter-parsing-improvement

# 2. å®‰è£…ä¾èµ–ï¼ˆå¦‚æœ‰æ–°å¢ï¼‰
pip install -r requirements.txt

# 3. è¿è¡Œç°æœ‰æµ‹è¯•ï¼ˆç¡®ä¿åŸºç¡€åŠŸèƒ½æ­£å¸¸ï¼‰
pytest tests/test_structure_parser.py

# 4. å¼€å§‹å¼€å‘...
```

### å¼€å‘é¡ºåºå»ºè®®

```
ç¬¬1æ­¥: å®ç°æ¨¡ç³ŠåŒ¹é…å‡½æ•° (_fuzzy_match_title, _normalize_title)
  â†“
ç¬¬2æ­¥: å®ç°ä¼ªç« èŠ‚è¿‡æ»¤ (_is_real_chapter_title)
  â†“
ç¬¬3æ­¥: å¢å¼º parse_by_toc_exact
  â†“
ç¬¬4æ­¥: ä¿®æ­£ç»Ÿè®¡è®¡ç®— (_calculate_statistics)
  â†“
ç¬¬5æ­¥: å®ç°æ··åˆç­–ç•¥ (_smart_merge_chapters)
  â†“
ç¬¬6æ­¥: å®ç°è¾¹ç•ŒéªŒè¯ (_validate_and_fix_boundaries)
  â†“
ç¬¬7æ­¥: åç«¯API
  â†“
ç¬¬8æ­¥: å‰ç«¯ç•Œé¢
  â†“
ç¬¬9æ­¥: é›†æˆæµ‹è¯•
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

### é£é™©å’Œç¼“è§£æªæ–½

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| ç°æœ‰åŠŸèƒ½å›å½’ | é«˜ | ä¸­ | å®Œå–„å•å…ƒæµ‹è¯•ï¼Œä¿æŒå‘åå…¼å®¹ |
| æ€§èƒ½ä¸‹é™ | ä¸­ | ä½ | æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œä¼˜åŒ–ç®—æ³• |
| æ–°å¼•å…¥bug | é«˜ | ä¸­ | ä»£ç å®¡æŸ¥ï¼Œè‡ªåŠ¨åŒ–æµ‹è¯• |
| æ–‡æ¡£æ ¼å¼å¤šæ ·æ€§ | é«˜ | é«˜ | æ”¶é›†æ›´å¤šæ ·æœ¬ï¼Œè¿­ä»£ä¼˜åŒ– |

### å¼€å‘è§„èŒƒ

1. **ä»£ç å®¡æŸ¥**: æ‰€æœ‰æ”¹åŠ¨éœ€è¦PRå’Œå®¡æŸ¥
2. **æµ‹è¯•è¦†ç›–**: æ–°å¢ä»£ç æµ‹è¯•è¦†ç›–ç‡ >= 80%
3. **æ–‡æ¡£æ›´æ–°**: åŒæ­¥æ›´æ–°APIæ–‡æ¡£å’Œç”¨æˆ·æ‰‹å†Œ
4. **æ—¥å¿—è®°å½•**: å…³é”®è·¯å¾„æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼ˆDEBUG/INFOçº§åˆ«ï¼‰
5. **é”™è¯¯å¤„ç†**: æ‰€æœ‰å¼‚å¸¸æƒ…å†µéƒ½æœ‰å‹å¥½æç¤º

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [åŸå§‹é—®é¢˜åˆ†æ](/Users/lvhe/Downloads/zhongbiao/zhongbiao/CHAPTER_PARSING_IMPROVEMENT.md)
- [é¡¹ç›®åˆ›å»ºæµç¨‹è®¾è®¡](/Users/lvhe/Downloads/zhongbiao/zhongbiao/PROJECT_CREATION_DESIGN.md)
- [ç°æœ‰è§£æå™¨ä»£ç ](ai_tender_system/modules/tender_processing/structure_parser.py)
- [å±‚çº§åˆ†æå™¨](ai_tender_system/modules/tender_processing/level_analyzer.py)

---

**æœ€åæ›´æ–°**: 2025-12-22
**è´Ÿè´£äºº**: [å¾…åˆ†é…]
**çŠ¶æ€**: ğŸ“‹ è§„åˆ’ä¸­
