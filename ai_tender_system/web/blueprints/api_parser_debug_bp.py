#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›®å½•è§£ææ–¹æ³•å¯¹æ¯”è°ƒè¯• API
åŠŸèƒ½ï¼š
- ä¸Šä¼ æ ‡ä¹¦æ–‡æ¡£å¹¶è¿è¡Œæ‰€æœ‰è§£ææ–¹æ³•
- å¯¹æ¯”ä¸åŒè§£ææ–¹æ³•çš„å‡†ç¡®ç‡
- æ”¯æŒäººå·¥æ ‡æ³¨æ­£ç¡®ç­”æ¡ˆ
- è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡ï¼ˆP/R/F1ï¼‰
"""

import os
import json
import uuid
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from flask import Blueprint, request, jsonify, send_file
from werkzeug.utils import secure_filename
from docx import Document

from common import get_module_logger, get_config
from common.database import get_knowledge_base_db
from modules.tender_processing.structure_parser import DocumentStructureParser, ChapterNode

# å°è¯•å¯¼å…¥ Azure è§£æå™¨
try:
    from modules.tender_processing.azure_parser import AzureDocumentParser, is_azure_available
    AZURE_PARSER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Azure è§£æå™¨ä¸å¯ç”¨: {e}")
    AZURE_PARSER_AVAILABLE = False

    def is_azure_available():
        return False

logger = get_module_logger("api_parser_debug")

api_parser_debug_bp = Blueprint('api_parser_debug', __name__, url_prefix='/api/parser-debug')


class ParserDebugger:
    """è§£ææ–¹æ³•å¯¹æ¯”è°ƒè¯•å™¨"""

    def __init__(self, doc_path: str):
        """
        åˆå§‹åŒ–è°ƒè¯•å™¨

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„
        """
        self.doc_path = doc_path
        self.parser = DocumentStructureParser()
        self.doc = Document(doc_path)

        # æ–‡æ¡£ä¿¡æ¯
        self.total_paragraphs = len(self.doc.paragraphs)
        self.has_toc = False
        self.toc_items_count = 0
        self.toc_start_idx = None
        self.toc_end_idx = None

        # è®¡ç®—æ–‡æ¡£æ€»å­—æ•° (å»é™¤ç©ºæ ¼)
        self.total_chars = sum(len(p.text.replace(' ', '').replace('\t', '')) for p in self.doc.paragraphs)

        # é¢„å…ˆæ£€æµ‹ç›®å½•
        self._detect_toc_info()

    def _detect_toc_info(self):
        """æ£€æµ‹ç›®å½•ä¿¡æ¯"""
        try:
            toc_idx = self.parser._find_toc_section(self.doc)
            if toc_idx is not None:
                self.has_toc = True
                self.toc_start_idx = toc_idx
                toc_items, toc_end_idx = self.parser._parse_toc_items(self.doc, toc_idx)
                self.toc_items_count = len(toc_items)
                self.toc_end_idx = toc_end_idx
                logger.info(f"æ£€æµ‹åˆ°ç›®å½•: {self.toc_items_count} é¡¹ï¼Œä½äºæ®µè½ {toc_idx}-{toc_end_idx}")
        except Exception as e:
            logger.warning(f"ç›®å½•æ£€æµ‹å¤±è´¥: {e}")

    def get_document_info(self) -> Dict:
        """è·å–æ–‡æ¡£åŸºæœ¬ä¿¡æ¯"""
        return {
            'filename': Path(self.doc_path).name,
            'total_paragraphs': self.total_paragraphs,
            'total_chars': self.total_chars,  # æ–‡æ¡£æ€»å­—æ•°
            'has_toc': self.has_toc,
            'toc_items_count': self.toc_items_count,
            'toc_start_idx': self.toc_start_idx,
            'toc_end_idx': self.toc_end_idx
        }

    def run_all_methods(self) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰è§£ææ–¹æ³•

        Returns:
            {
                'semantic': {...},
                'style': {...},
                'hybrid': {...},
                'azure': {...},  # å¯é€‰
                'docx_native': {...}
            }
        """
        results = {}

        # æ–¹æ³•1: è¯­ä¹‰é”šç‚¹è§£æ
        results['semantic'] = self._run_with_timing(
            self._run_semantic_anchors,
            "è¯­ä¹‰é”šç‚¹è§£æ"
        )

        # æ–¹æ³•2: æ ·å¼è¯†åˆ«(å¢å¼º)
        results['style'] = self._run_with_timing(
            self._run_style_detection,
            "æ ·å¼è¯†åˆ«"
        )

        # æ–¹æ³•3: æ··åˆå¯å‘å¼è¯†åˆ«
        results['hybrid'] = self._run_with_timing(
            self._run_hybrid_detection,
            "æ··åˆå¯å‘å¼è¯†åˆ«"
        )

        # æ–¹æ³•4: Azure Form Recognizerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if is_azure_available() and AZURE_PARSER_AVAILABLE:
            results['azure'] = self._run_with_timing(
                self._run_azure_parser,
                "Azure Form Recognizer"
            )
        else:
            results['azure'] = {
                'success': False,
                'error': 'Azure Form Recognizer æœªé…ç½®æˆ–SDKæœªå®‰è£…',
                'chapters': [],
                'method_name': 'Azure Form Recognizer',
                'performance': {'elapsed': 0}
            }

        # æ–¹æ³•5: Wordå¤§çº²çº§åˆ«è¯†åˆ«
        results['docx_native'] = self._run_with_timing(
            self._run_docx_native,
            "Wordå¤§çº²çº§åˆ«è¯†åˆ«"
        )

        return results

    def _run_with_timing(self, method_func, method_name: str) -> Dict:
        """
        è¿è¡Œæ–¹æ³•å¹¶è®¡æ—¶

        Args:
            method_func: è¦è¿è¡Œçš„æ–¹æ³•
            method_name: æ–¹æ³•åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            åŒ…å«ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        logger.info(f"å¼€å§‹è¿è¡Œ: {method_name}")
        start_time = time.time()

        try:
            result = method_func()
            elapsed = time.time() - start_time

            result['performance'] = {
                'elapsed': round(elapsed, 3),
                'elapsed_formatted': f"{elapsed:.3f}s"
            }

            logger.info(f"{method_name} å®Œæˆï¼Œè€—æ—¶ {elapsed:.3f}s")
            return result

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"{method_name} å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'chapters': [],
                'performance': {'elapsed': round(elapsed, 3)}
            }

    def _run_semantic_anchors(self) -> Dict:
        """æ–¹æ³•1: å¼ºåˆ¶ä½¿ç”¨è¯­ä¹‰é”šç‚¹è§£æï¼ˆåŒ…å«å­ç« èŠ‚è¯†åˆ«ï¼‰"""
        if not self.has_toc:
            return {
                'success': False,
                'error': 'æ–‡æ¡£æ— ç›®å½•ï¼Œæ— æ³•ä½¿ç”¨è¯­ä¹‰é”šç‚¹è§£æ',
                'chapters': [],
                'method_name': 'è¯­ä¹‰é”šç‚¹è§£æ'
            }

        try:
            toc_items, toc_end_idx = self.parser._parse_toc_items(self.doc, self.toc_start_idx)
            toc_targets = [item['title'] for item in toc_items]

            chapters = self.parser._parse_chapters_by_semantic_anchors(
                self.doc, toc_targets, toc_end_idx
            )

            # â­ å…³é”®ä¿®å¤ï¼šä¸ºæ¯ä¸ªç« èŠ‚è¯†åˆ«å­ç« èŠ‚ï¼ˆä¸æ—§ç›®å½•å®šä½æ–¹æ³•ä¿æŒä¸€è‡´ï¼‰
            for i, chapter in enumerate(chapters):
                logger.info(f"æ­£åœ¨è¯†åˆ«ç« èŠ‚ '{chapter.title}' çš„å­ç« èŠ‚...")
                subsections = self.parser._parse_subsections_in_range(
                    self.doc,
                    chapter.para_start_idx,
                    chapter.para_end_idx,
                    chapter.level,
                    f"sem_{i}"
                )

                if subsections:
                    chapter.children = subsections
                    # æ³¨æ„ï¼šä¸éœ€è¦ç´¯åŠ å­ç« èŠ‚å­—æ•°ï¼Œå› ä¸ºçˆ¶ç« èŠ‚çš„word_countå·²ç»åŒ…å«äº†
                    # å…¶æ®µè½èŒƒå›´å†…çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬å­ç« èŠ‚æ‰€åœ¨çš„æ®µè½ï¼‰
                    logger.info(f"  â””â”€ è¯†åˆ«åˆ° {len(subsections)} ä¸ªå­ç« èŠ‚ï¼ˆçˆ¶ç« èŠ‚å­—æ•°: {chapter.word_count}ï¼‰")

            # æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self.parser._build_chapter_tree(chapters)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_detected_words = sum(ch.word_count for ch in chapters)
            coverage_rate = total_detected_words / self.total_chars if self.total_chars > 0 else 0

            # è¦†ç›–ç‡è­¦å‘Šï¼šå¦‚æœè¯†åˆ«å­—æ•°å°‘äºæ–‡æ¡£æ€»å­—æ•°çš„60%,å¯èƒ½æœ‰é—®é¢˜
            coverage_warning = None
            if coverage_rate < 0.60:
                coverage_warning = f"âš ï¸ è¦†ç›–ç‡ä»…{coverage_rate:.1%},å¯èƒ½æ¼è¯†åˆ«äº†ç« èŠ‚"
                logger.warning(f"è¯­ä¹‰é”šç‚¹è§£æ - {coverage_warning}")

            return {
                'success': True,
                'method_name': 'è¯­ä¹‰é”šç‚¹è§£æ',
                'chapters': [ch.to_dict() for ch in chapter_tree],
                'statistics': {
                    'total_chapters': len(chapters),
                    'total_words': total_detected_words,
                    'document_total_chars': self.total_chars,
                    'coverage_rate': round(coverage_rate, 4),
                    'coverage_warning': coverage_warning,
                    'toc_items_count': len(toc_items),
                    'match_rate': len(chapters) / len(toc_items) if toc_items else 0
                }
            }
        except Exception as e:
            logger.error(f"è¯­ä¹‰é”šç‚¹è§£æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _run_style_detection(self) -> Dict:
        """æ–¹æ³•3: å¼ºåˆ¶ä½¿ç”¨æ ·å¼è¯†åˆ«æ–¹æ¡ˆ"""
        try:
            # ç›´æ¥ä½¿ç”¨æ ·å¼è§£æ
            chapters = self.parser._parse_chapters_from_doc(self.doc)
            chapters = self.parser._locate_chapter_content(self.doc, chapters)

            # æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self.parser._build_chapter_tree(chapters)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_detected_words = sum(ch.word_count for ch in chapters)
            coverage_rate = total_detected_words / self.total_chars if self.total_chars > 0 else 0

            # è¦†ç›–ç‡è­¦å‘Š
            coverage_warning = None
            if coverage_rate < 0.60:
                coverage_warning = f"âš ï¸ è¦†ç›–ç‡ä»…{coverage_rate:.1%},å¯èƒ½æ¼è¯†åˆ«äº†ç« èŠ‚"
                logger.warning(f"æ ·å¼è¯†åˆ« - {coverage_warning}")

            return {
                'success': True,
                'method_name': 'æ ·å¼è¯†åˆ«',
                'chapters': [ch.to_dict() for ch in chapter_tree],
                'statistics': {
                    'total_chapters': len(chapters),
                    'total_words': total_detected_words,
                    'document_total_chars': self.total_chars,
                    'coverage_rate': round(coverage_rate, 4),
                    'coverage_warning': coverage_warning
                }
            }
        except Exception as e:
            logger.error(f"æ ·å¼è¯†åˆ«å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _run_hybrid_detection(self) -> Dict:
        """æ–¹æ³•3: æ··åˆå¯å‘å¼è¯†åˆ« - ç»¼åˆå¤šç§ç‰¹å¾åˆ¤æ–­æ ‡é¢˜"""
        import re

        try:
            chapters = []

            for i, para in enumerate(self.doc.paragraphs):
                text = para.text.strip()

                # åŸºç¡€è¿‡æ»¤: è·³è¿‡ç©ºè¡Œå’Œè¿‡é•¿æ–‡æœ¬
                if not text or len(text) > 150 or len(text) < 2:
                    continue

                # è®¡ç®—å¤šç»´åº¦å¾—åˆ†
                score = 0

                # ç‰¹å¾1: ç¼–å·æ¨¡å¼è¯†åˆ« (30åˆ†)
                numbering_patterns = [
                    (r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', 30),  # ç¬¬Xç« /éƒ¨åˆ†
                    (r'^\d+\.\s+\S', 25),  # 1. xxx
                    (r'^\d+\.\d+\s+\S', 20),  # 1.1 xxx
                    (r'^\d+\.\d+\.\d+\s+\S', 15),  # 1.1.1 xxx
                    (r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', 20),  # ä¸€ã€xxx
                    (r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)', 15),  # (ä¸€)
                ]

                for pattern, points in numbering_patterns:
                    if re.match(pattern, text):
                        score += points
                        break

                # ç‰¹å¾2: å­—ä½“å¤§å°å’ŒåŠ ç²— (25åˆ†)
                if para.runs:
                    sizes = []
                    bold_count = 0
                    total_runs = len(para.runs)

                    for run in para.runs:
                        if run.font.size:
                            sizes.append(run.font.size.pt)
                        if run.bold:
                            bold_count += 1

                    # åŠ ç²—æ¯”ä¾‹
                    if bold_count >= total_runs * 0.5:
                        score += 10

                    # å­—ä½“å¤§å°
                    if sizes:
                        avg_size = sum(sizes) / len(sizes)
                        if avg_size >= 16:
                            score += 15
                        elif avg_size >= 13:
                            score += 10
                        elif avg_size >= 10:
                            score += 5

                # ç‰¹å¾3: æ®µè½ç¼©è¿› (20åˆ†)
                try:
                    if para.paragraph_format.left_indent:
                        indent_pt = para.paragraph_format.left_indent.pt
                        # ç¼©è¿›è¶Šå°è¶Šå¯èƒ½æ˜¯æ ‡é¢˜
                        if indent_pt == 0:
                            score += 20
                        elif indent_pt <= 10:
                            score += 10
                        elif indent_pt <= 20:
                            score += 5
                except (AttributeError, TypeError):
                    # æ— ç¼©è¿›ä¿¡æ¯æ—¶é»˜è®¤ç»™ä¸€äº›åˆ†æ•°
                    score += 10

                # ç‰¹å¾4: å†…å®¹é•¿åº¦ (15åˆ†)
                text_len = len(text)
                if text_len <= 30:
                    score += 15
                elif text_len <= 50:
                    score += 10
                elif text_len <= 80:
                    score += 5

                # ç‰¹å¾5: ä½ç½®ç‰¹å¾ (10åˆ†)
                # æ–‡æ¡£å‰éƒ¨çš„çŸ­æ–‡æœ¬æ›´å¯èƒ½æ˜¯æ ‡é¢˜
                if i < len(self.doc.paragraphs) * 0.1:  # å‰10%
                    score += 10
                elif i < len(self.doc.paragraphs) * 0.3:  # å‰30%
                    score += 5

                # åˆ¤æ–­é˜ˆå€¼: 60åˆ†ä»¥ä¸Šè®¤ä¸ºæ˜¯æ ‡é¢˜
                if score >= 60:
                    # åˆ¤æ–­å±‚çº§
                    level = self._determine_level_by_text(text)

                    chapter = ChapterNode(
                        id=f"hybrid_{i}",
                        level=level,
                        title=text,
                        para_start_idx=i,
                        para_end_idx=i,
                        word_count=0,
                        preview_text="",
                        auto_selected=False,
                        skip_recommended=False,
                        content_tags=[f'score_{score}']
                    )
                    chapters.append(chapter)
                    logger.debug(f"æ··åˆè¯†åˆ«æ ‡é¢˜ (å¾—åˆ†{score}): {text[:50]}")

            # å®šä½å†…å®¹
            chapters = self.parser._locate_chapter_content(self.doc, chapters)

            # æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self.parser._build_chapter_tree(chapters)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_detected_words = sum(ch.word_count for ch in chapters)
            coverage_rate = total_detected_words / self.total_chars if self.total_chars > 0 else 0

            # è¦†ç›–ç‡è­¦å‘Š
            coverage_warning = None
            if coverage_rate < 0.60:
                coverage_warning = f"âš ï¸ è¦†ç›–ç‡ä»…{coverage_rate:.1%},å¯èƒ½æ¼è¯†åˆ«äº†ç« èŠ‚"
                logger.warning(f"æ··åˆå¯å‘å¼è¯†åˆ« - {coverage_warning}")

            return {
                'success': True,
                'method_name': 'æ··åˆå¯å‘å¼è¯†åˆ«',
                'chapters': [ch.to_dict() for ch in chapter_tree],
                'statistics': {
                    'total_chapters': len(chapters),
                    'total_words': total_detected_words,
                    'document_total_chars': self.total_chars,
                    'coverage_rate': round(coverage_rate, 4),
                    'coverage_warning': coverage_warning
                }
            }
        except Exception as e:
            logger.error(f"æ··åˆå¯å‘å¼è¯†åˆ«å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _determine_level_by_text(self, text: str) -> int:
        """æ ¹æ®æ–‡æœ¬å†…å®¹åˆ¤æ–­æ ‡é¢˜å±‚çº§"""
        import re

        # ä¸€çº§æ ‡é¢˜: ç¬¬Xç« /éƒ¨åˆ†, å•ä¸ªæ•°å­—
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', text):
            return 1
        if re.match(r'^\d+\.\s+\S', text) and not re.match(r'^\d+\.\d+', text):
            return 1

        # äºŒçº§æ ‡é¢˜: X.Yæ ¼å¼
        if re.match(r'^\d+\.\d+\s+\S', text) and not re.match(r'^\d+\.\d+\.\d+', text):
            return 2

        # ä¸‰çº§æ ‡é¢˜: X.Y.Zæ ¼å¼
        if re.match(r'^\d+\.\d+\.\d+\s+\S', text):
            return 3

        # é»˜è®¤äºŒçº§
        return 2

    def _run_azure_parser(self) -> Dict:
        """æ–¹æ³•4: Azure Form Recognizer è§£æ"""
        try:
            azure_parser = AzureDocumentParser()
            result = azure_parser.parse_document_structure(self.doc_path)
            return result
        except Exception as e:
            logger.error(f"Azure è§£æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _run_docx_native(self) -> Dict:
        """æ–¹æ³•5: Wordå¤§çº²çº§åˆ«è¯†åˆ«ï¼ˆå¾®è½¯å®˜æ–¹APIï¼‰"""
        import re

        try:
            headings = []

            # ç›´æ¥ä»Wordæ–‡æ¡£æå–æ ‡é¢˜ - ä½¿ç”¨å¾®è½¯å®˜æ–¹çš„å¤§çº²çº§åˆ«API
            for idx, para in enumerate(self.doc.paragraphs):
                is_heading = False
                level = 0
                detection_method = ""

                # â­ ä¼˜å…ˆçº§1: æ£€æŸ¥å¤§çº²çº§åˆ« (Outline Level) - å¾®è½¯å®˜æ–¹è¯­ä¹‰æ ‡è®°
                # è¿™æ˜¯Wordå¯¼èˆªçª—æ ¼å’Œå¤§çº²è§†å›¾ä½¿ç”¨çš„ç»“æ„ï¼Œå‡†ç¡®åº¦æœ€é«˜
                try:
                    pPr = para._element.pPr
                    if pPr is not None:
                        outlineLvl = pPr.outlineLvl
                        if outlineLvl is not None:
                            outline_level_val = int(outlineLvl.val)
                            # Wordå¤§çº²çº§åˆ«: 0-8è¡¨ç¤ºæ ‡é¢˜(0=ä¸€çº§), 9è¡¨ç¤ºæ­£æ–‡
                            if outline_level_val <= 8:
                                # ğŸ”§ æ·»åŠ è¿‡æ»¤è§„åˆ™ï¼Œæ’é™¤å™ªéŸ³å†…å®¹
                                text = para.text.strip()
                                should_skip = False

                                # è¿‡æ»¤1: è·³è¿‡æ–‡æ¡£å‰30æ®µçš„å°é¢/å…ƒæ•°æ®ï¼ˆLevel 0ï¼‰
                                if idx < 30 and outline_level_val == 0:
                                    metadata_keywords = ['é¡¹ç›®ç¼–å·', 'æ‹›æ ‡äºº', 'ä»£ç†æœºæ„', 'è”ç³»äºº', 'è”ç³»æ–¹å¼',
                                                        'åœ°å€', 'ç”µè¯', 'ä¼ çœŸ', 'é‚®ç¼–', 'ç½‘å€', 'http']
                                    if any(kw in text for kw in metadata_keywords):
                                        should_skip = True
                                        logger.debug(f"è¿‡æ»¤å°é¢: æ®µè½{idx} '{text[:30]}'")

                                # è¿‡æ»¤2: è·³è¿‡Level 3-4çš„é•¿æ¡æ¬¾å†…å®¹
                                if not should_skip and outline_level_val >= 3:
                                    # å½¢å¦‚ "1.1 è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„è¯´æ˜æ–‡å­—..." çš„æ˜¯æ¡æ¬¾ï¼Œä¸æ˜¯æ ‡é¢˜
                                    if re.match(r'^\d+\.\d+\s+.{15,}', text):
                                        should_skip = True
                                        logger.debug(f"è¿‡æ»¤æ¡æ¬¾: æ®µè½{idx} '{text[:30]}'")

                                # è¿‡æ»¤3: æ ‡é¢˜é•¿åº¦é™åˆ¶ï¼ˆè¶…è¿‡50å­—çš„é€šå¸¸ä¸æ˜¯æ ‡é¢˜ï¼‰
                                if not should_skip and len(text) > 50:
                                    # é™¤éæœ‰æ˜ç¡®çš„ç« èŠ‚ç¼–å·
                                    if not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', text):
                                        should_skip = True
                                        logger.debug(f"è¿‡æ»¤é•¿æ–‡æœ¬: æ®µè½{idx} '{text[:30]}'")

                                if not should_skip:
                                    is_heading = True
                                    level = outline_level_val + 1  # è½¬æ¢: 0â†’1çº§, 1â†’2çº§, ...
                                    detection_method = f"å¤§çº²çº§åˆ«{outline_level_val}"
                except (AttributeError, TypeError, ValueError):
                    pass  # æ²¡æœ‰å¤§çº²çº§åˆ«ï¼Œç»§ç»­å…¶ä»–æ–¹æ³•

                # ä¼˜å…ˆçº§2: æ£€æŸ¥æ ‡å‡†Headingæ ·å¼ (å¤‡ç”¨æ–¹æ¡ˆ)
                if not is_heading:
                    style_name = para.style.name if para.style else ""

                    # åªæ¥å—æ ‡å‡†çš„Headingæ ·å¼ï¼ˆç²¾ç¡®åŒ¹é…ï¼Œé¿å…è¯¯è¯†åˆ«ï¼‰
                    if style_name.startswith('Heading '):  # 'Heading 1', 'Heading 2'
                        match = re.search(r'Heading (\d+)', style_name)
                        if match:
                            is_heading = True
                            level = int(match.group(1))
                            detection_method = f"æ ·å¼{style_name}"
                    elif style_name.startswith('æ ‡é¢˜ '):  # 'æ ‡é¢˜ 1', 'æ ‡é¢˜ 2'
                        match = re.search(r'æ ‡é¢˜ (\d+)', style_name)
                        if match:
                            is_heading = True
                            level = int(match.group(1))
                            detection_method = f"æ ·å¼{style_name}"

                if is_heading and para.text.strip():
                    headings.append({
                        'index': idx,
                        'text': para.text.strip(),
                        'level': level if level > 0 else 1,
                        'detection_method': detection_method
                    })
                    logger.debug(f"è¯†åˆ«æ ‡é¢˜: æ®µè½{idx} [{detection_method}] '{para.text.strip()[:50]}'")

            if not headings:
                return {
                    'success': False,
                    'error': 'Wordæ–‡æ¡£ä¸­æœªæ‰¾åˆ°æ ‡é¢˜ï¼ˆæœªè®¾ç½®å¤§çº²çº§åˆ«ï¼Œä¹Ÿæœªä½¿ç”¨Headingæ ·å¼ï¼‰',
                    'chapters': [],
                    'method_name': 'Wordå¤§çº²çº§åˆ«è¯†åˆ«',
                    'statistics': {
                        'total_chapters': 0,
                        'detection_note': 'æ–‡æ¡£æœªä½¿ç”¨Wordæ ‡å‡†æ ‡é¢˜ç»“æ„'
                    }
                }

            logger.info(f"âœ… åŸºäºå¤§çº²çº§åˆ«è¯†åˆ«åˆ° {len(headings)} ä¸ªæ ‡é¢˜")

            # æ„å»ºç« èŠ‚ç»“æ„
            chapters = []
            for i, heading in enumerate(headings):
                # ç¡®å®šç« èŠ‚èŒƒå›´
                start_idx = heading['index']
                end_idx = headings[i + 1]['index'] - 1 if i + 1 < len(headings) else self.total_paragraphs - 1

                # æå–ç« èŠ‚å†…å®¹
                content_paras = self.doc.paragraphs[start_idx + 1:end_idx + 1]
                content_text = '\n'.join(p.text for p in content_paras if p.text.strip())
                word_count = len(content_text.replace(' ', '').replace('\n', ''))

                # ç”Ÿæˆé¢„è§ˆæ–‡æœ¬
                preview_lines = []
                for p in content_paras[:5]:
                    text = p.text.strip()
                    if text:
                        preview_lines.append(text[:100] + ('...' if len(text) > 100 else ''))
                    if len(preview_lines) >= 5:
                        break
                preview_text = '\n'.join(preview_lines) if preview_lines else "(æ— å†…å®¹)"

                # åˆ›å»ºç« èŠ‚èŠ‚ç‚¹
                chapter = ChapterNode(
                    id=f"docx_{i}",
                    level=heading['level'],
                    title=heading['text'],
                    para_start_idx=start_idx,
                    para_end_idx=end_idx,
                    word_count=word_count,
                    preview_text=preview_text,
                    auto_selected=False,
                    skip_recommended=False,
                    content_tags=['docx_native', heading.get('detection_method', 'unknown')]
                )

                chapters.append(chapter)

            # æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self.parser._build_chapter_tree(chapters)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_detected_words = sum(ch.word_count for ch in chapters)
            coverage_rate = total_detected_words / self.total_chars if self.total_chars > 0 else 0

            # è¦†ç›–ç‡è­¦å‘Š
            coverage_warning = None
            if coverage_rate < 0.60:
                coverage_warning = f"âš ï¸ è¦†ç›–ç‡ä»…{coverage_rate:.1%},å¯èƒ½æ¼è¯†åˆ«äº†ç« èŠ‚"
                logger.warning(f"Wordå¤§çº²çº§åˆ«è¯†åˆ« - {coverage_warning}")

            # ç»Ÿè®¡æ£€æµ‹æ–¹æ³•åˆ†å¸ƒ
            detection_stats = {}
            for h in headings:
                method = h.get('detection_method', 'unknown')
                detection_stats[method] = detection_stats.get(method, 0) + 1

            return {
                'success': True,
                'method_name': 'Wordå¤§çº²çº§åˆ«è¯†åˆ«',
                'chapters': [ch.to_dict() for ch in chapter_tree],
                'statistics': {
                    'total_chapters': len(chapters),
                    'total_words': total_detected_words,
                    'document_total_chars': self.total_chars,
                    'coverage_rate': round(coverage_rate, 4),
                    'coverage_warning': coverage_warning,
                    'detection_methods': detection_stats  # è®°å½•ä½¿ç”¨äº†å“ªäº›æ£€æµ‹æ–¹æ³•
                }
            }
        except Exception as e:
            logger.error(f"Wordå¤§çº²çº§åˆ«è¯†åˆ«å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    @staticmethod
    def calculate_accuracy(detected_chapters: List[Dict], ground_truth_chapters: List[Dict]) -> Dict:
        """
        è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡

        Args:
            detected_chapters: æ£€æµ‹åˆ°çš„ç« èŠ‚åˆ—è¡¨
            ground_truth_chapters: æ­£ç¡®ç­”æ¡ˆç« èŠ‚åˆ—è¡¨

        Returns:
            {
                'precision': 0.0-1.0,
                'recall': 0.0-1.0,
                'f1_score': 0.0-1.0,
                'matched_count': int,
                'detected_count': int,
                'ground_truth_count': int,
                'details': [...]
            }
        """
        if not ground_truth_chapters:
            return {
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'matched_count': 0,
                'detected_count': len(detected_chapters),
                'ground_truth_count': 0
            }

        # æ‰å¹³åŒ–ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å«å­ç« èŠ‚ï¼‰
        def flatten_chapters(chapters_list):
            flat = []
            for ch in chapters_list:
                flat.append(ch)
                if 'children' in ch and ch['children']:
                    flat.extend(flatten_chapters(ch['children']))
            return flat

        detected_flat = flatten_chapters(detected_chapters)
        truth_flat = flatten_chapters(ground_truth_chapters)

        # è§„èŒƒåŒ–æ ‡é¢˜ï¼ˆç”¨äºåŒ¹é…ï¼‰
        def normalize_title(title: str) -> str:
            import re
            # ç§»é™¤æ‰€æœ‰ç©ºæ ¼ã€ç¼–å·
            cleaned = re.sub(r'^\d+\.\s*', '', title)
            cleaned = re.sub(r'^\d+\.\d+\s*', '', cleaned)
            cleaned = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*', '', cleaned)
            cleaned = re.sub(r'\s+', '', cleaned)
            return cleaned.lower()

        # æ„å»ºçœŸå®ç­”æ¡ˆçš„æ ‡é¢˜é›†åˆ
        truth_titles = {normalize_title(ch['title']): ch for ch in truth_flat}
        detected_titles = {normalize_title(ch['title']): ch for ch in detected_flat}

        # è®¡ç®—åŒ¹é…
        matched_titles = set(truth_titles.keys()) & set(detected_titles.keys())
        matched_count = len(matched_titles)

        # è®¡ç®—æŒ‡æ ‡
        precision = matched_count / len(detected_flat) if detected_flat else 0.0
        recall = matched_count / len(truth_flat) if truth_flat else 0.0
        f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

        # è¯¦ç»†åŒ¹é…ä¿¡æ¯
        details = []
        for title in truth_titles.keys():
            if title in matched_titles:
                details.append({
                    'title': truth_titles[title]['title'],
                    'status': 'matched',
                    'detected': True
                })
            else:
                details.append({
                    'title': truth_titles[title]['title'],
                    'status': 'missed',
                    'detected': False
                })

        # æ£€æµ‹å¤šä½™çš„ï¼ˆè¯¯æ£€ï¼‰
        for title in detected_titles.keys():
            if title not in matched_titles:
                details.append({
                    'title': detected_titles[title]['title'],
                    'status': 'false_positive',
                    'detected': True
                })

        return {
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'f1_score': round(f1_score, 4),
            'matched_count': matched_count,
            'detected_count': len(detected_flat),
            'ground_truth_count': len(truth_flat),
            'details': details
        }


@api_parser_debug_bp.route('/upload', methods=['POST'])
def upload_document():
    """
    ä¸Šä¼ æ–‡æ¡£å¹¶è¿è¡Œæ‰€æœ‰è§£ææ–¹æ³•

    è¯·æ±‚:
        - file: .docxæ–‡ä»¶
        - methods: è¦è¿è¡Œçš„æ–¹æ³•åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…¨éƒ¨ï¼‰

    å“åº”:
        {
            "success": true,
            "document_id": "uuid",
            "document_info": {...},
            "results": {...}
        }
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400

        file = request.files['file']
        if not file.filename:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶åä¸ºç©º'}), 400

        if not file.filename.endswith('.docx'):
            return jsonify({'success': False, 'error': 'ä»…æ”¯æŒ .docx æ ¼å¼æ–‡ä»¶'}), 400

        # ä¿å­˜æ–‡ä»¶
        document_id = str(uuid.uuid4())
        filename = secure_filename(file.filename)

        config = get_config()
        upload_dir = config.get_path('data') / 'parser_debug'
        upload_dir.mkdir(parents=True, exist_ok=True)

        file_path = upload_dir / f"{document_id}_{filename}"
        file.save(str(file_path))

        logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")

        # åˆ›å»ºè°ƒè¯•å™¨å¹¶è¿è¡Œæ‰€æœ‰æ–¹æ³•
        debugger = ParserDebugger(str(file_path))
        document_info = debugger.get_document_info()
        results = debugger.run_all_methods()

        # ä¿å­˜åˆ°æ•°æ®åº“
        db = get_knowledge_base_db()
        db.execute_query("""
            INSERT INTO parser_debug_tests (
                document_id, filename, file_path,
                total_paragraphs, has_toc, toc_items_count, toc_start_idx, toc_end_idx,
                semantic_result, style_result, hybrid_result, azure_result, docx_native_result,
                semantic_elapsed, style_elapsed, hybrid_elapsed, azure_elapsed, docx_native_elapsed,
                semantic_chapters_count, style_chapters_count, hybrid_chapters_count, azure_chapters_count, docx_native_chapters_count
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            document_id,
            filename,
            str(file_path),
            document_info['total_paragraphs'],
            document_info['has_toc'],
            document_info['toc_items_count'],
            document_info['toc_start_idx'],
            document_info['toc_end_idx'],
            json.dumps(results['semantic'], ensure_ascii=False),
            json.dumps(results['style'], ensure_ascii=False),
            json.dumps(results['hybrid'], ensure_ascii=False),
            json.dumps(results['azure'], ensure_ascii=False),
            json.dumps(results['docx_native'], ensure_ascii=False),
            results['semantic']['performance']['elapsed'],
            results['style']['performance']['elapsed'],
            results['hybrid']['performance']['elapsed'],
            results['azure']['performance']['elapsed'],
            results['docx_native']['performance']['elapsed'],
            len(results['semantic'].get('chapters', [])),
            len(results['style'].get('chapters', [])),
            len(results['hybrid'].get('chapters', [])),
            len(results['azure'].get('chapters', [])),
            len(results['docx_native'].get('chapters', []))
        ))

        return jsonify({
            'success': True,
            'document_id': document_id,
            'document_info': document_info,
            'results': results
        })

    except Exception as e:
        logger.error(f"ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/<document_id>', methods=['GET'])
def get_test_result(document_id):
    """
    è·å–æµ‹è¯•ç»“æœ

    å“åº”:
        {
            "success": true,
            "document_info": {...},
            "results": {...},
            "ground_truth": {...},
            "accuracy": {...}
        }
    """
    try:
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT * FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æµ‹è¯•è®°å½•ä¸å­˜åœ¨'}), 404

        # è§£æç»“æœ
        results = {
            'semantic': json.loads(row['semantic_result']) if row['semantic_result'] else None,
            'style': json.loads(row['style_result']) if row['style_result'] else None,
            'hybrid': json.loads(row['hybrid_result']) if row.get('hybrid_result') else None,
            'azure': json.loads(row['azure_result']) if row.get('azure_result') else None,
            'docx_native': json.loads(row['docx_native_result']) if row.get('docx_native_result') else None,
        }

        document_info = {
            'filename': row['filename'],
            'total_paragraphs': row['total_paragraphs'],
            'has_toc': bool(row['has_toc']),
            'toc_items_count': row['toc_items_count'],
            'upload_time': row['upload_time']
        }

        ground_truth = json.loads(row['ground_truth']) if row['ground_truth'] else None

        # å¦‚æœæœ‰ground_truthï¼Œè¿”å›å‡†ç¡®ç‡æ•°æ®
        accuracy = None
        if ground_truth:
            accuracy = {
                'semantic': {
                    'precision': row['semantic_precision'],
                    'recall': row['semantic_recall'],
                    'f1_score': row['semantic_f1']
                },
                'style': {
                    'precision': row['style_precision'],
                    'recall': row['style_recall'],
                    'f1_score': row['style_f1']
                },
                'hybrid': {
                    'precision': row.get('hybrid_precision'),
                    'recall': row.get('hybrid_recall'),
                    'f1_score': row.get('hybrid_f1')
                } if row.get('hybrid_precision') else None,
                'azure': {
                    'precision': row.get('azure_precision'),
                    'recall': row.get('azure_recall'),
                    'f1_score': row.get('azure_f1')
                } if row.get('azure_precision') else None,
                'docx_native': {
                    'precision': row.get('docx_native_precision'),
                    'recall': row.get('docx_native_recall'),
                    'f1_score': row.get('docx_native_f1')
                } if row.get('docx_native_precision') else None,
                'best_method': row['best_method'],
                'best_f1_score': row['best_f1_score']
            }

        return jsonify({
            'success': True,
            'document_id': document_id,
            'document_info': document_info,
            'results': results,
            'ground_truth': ground_truth,
            'accuracy': accuracy
        })

    except Exception as e:
        logger.error(f"è·å–æµ‹è¯•ç»“æœå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/<document_id>/ground-truth', methods=['POST'])
def save_ground_truth(document_id):
    """
    ä¿å­˜äººå·¥æ ‡æ³¨çš„æ­£ç¡®ç­”æ¡ˆ

    è¯·æ±‚:
        {
            "chapters": [...],  # æ­£ç¡®çš„ç« èŠ‚åˆ—è¡¨
            "annotator": "ç”¨æˆ·å"
        }

    å“åº”:
        {
            "success": true,
            "accuracy": {...}  # è‡ªåŠ¨è®¡ç®—çš„å‡†ç¡®ç‡
        }
    """
    try:
        data = request.get_json()
        if not data or 'chapters' not in data:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ç« èŠ‚æ•°æ®'}), 400

        chapters = data['chapters']
        annotator = data.get('annotator', 'unknown')

        # è·å–ç°æœ‰æµ‹è¯•ç»“æœ
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT semantic_result, style_result, hybrid_result, azure_result, docx_native_result FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æµ‹è¯•è®°å½•ä¸å­˜åœ¨'}), 404

        # è§£æå„æ–¹æ³•çš„ç»“æœ
        semantic_chapters = json.loads(row['semantic_result'])['chapters'] if row['semantic_result'] else []
        style_chapters = json.loads(row['style_result'])['chapters'] if row['style_result'] else []
        hybrid_chapters = json.loads(row['hybrid_result'])['chapters'] if row.get('hybrid_result') else []
        azure_chapters = json.loads(row['azure_result'])['chapters'] if row.get('azure_result') else []
        docx_native_chapters = json.loads(row['docx_native_result'])['chapters'] if row.get('docx_native_result') else []

        # è®¡ç®—å„æ–¹æ³•çš„å‡†ç¡®ç‡
        semantic_acc = ParserDebugger.calculate_accuracy(semantic_chapters, chapters)
        style_acc = ParserDebugger.calculate_accuracy(style_chapters, chapters)
        hybrid_acc = ParserDebugger.calculate_accuracy(hybrid_chapters, chapters) if hybrid_chapters else None
        azure_acc = ParserDebugger.calculate_accuracy(azure_chapters, chapters) if azure_chapters else None
        docx_native_acc = ParserDebugger.calculate_accuracy(docx_native_chapters, chapters) if docx_native_chapters else None

        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        all_f1 = {
            'semantic': semantic_acc['f1_score'],
            'style': style_acc['f1_score'],
        }
        if hybrid_acc:
            all_f1['hybrid'] = hybrid_acc['f1_score']
        if azure_acc:
            all_f1['azure'] = azure_acc['f1_score']
        if docx_native_acc:
            all_f1['docx_native'] = docx_native_acc['f1_score']
        best_method = max(all_f1, key=all_f1.get)
        best_f1_score = all_f1[best_method]

        # æ›´æ–°æ•°æ®åº“
        update_params = [
            json.dumps(chapters, ensure_ascii=False),
            annotator,
            datetime.now().isoformat(),
            len(chapters),
            semantic_acc['precision'], semantic_acc['recall'], semantic_acc['f1_score'],
            style_acc['precision'], style_acc['recall'], style_acc['f1_score'],
        ]

        # å¦‚æœæœ‰ hybrid ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if hybrid_acc:
            update_params.extend([hybrid_acc['precision'], hybrid_acc['recall'], hybrid_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ Azure ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if azure_acc:
            update_params.extend([azure_acc['precision'], azure_acc['recall'], azure_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ docx_native ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if docx_native_acc:
            update_params.extend([docx_native_acc['precision'], docx_native_acc['recall'], docx_native_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        update_params.extend([best_method, best_f1_score, document_id])

        db.execute_query("""
            UPDATE parser_debug_tests SET
                ground_truth = ?, annotator = ?, annotation_time = ?, ground_truth_count = ?,
                semantic_precision = ?, semantic_recall = ?, semantic_f1 = ?,
                style_precision = ?, style_recall = ?, style_f1 = ?,
                hybrid_precision = ?, hybrid_recall = ?, hybrid_f1 = ?,
                azure_precision = ?, azure_recall = ?, azure_f1 = ?,
                docx_native_precision = ?, docx_native_recall = ?, docx_native_f1 = ?,
                best_method = ?, best_f1_score = ?
            WHERE document_id = ?
        """, tuple(update_params))

        accuracy_result = {
            'semantic': semantic_acc,
            'style': style_acc,
            'best_method': best_method,
            'best_f1_score': best_f1_score
        }

        if hybrid_acc:
            accuracy_result['hybrid'] = hybrid_acc
        if azure_acc:
            accuracy_result['azure'] = azure_acc
        if docx_native_acc:
            accuracy_result['docx_native'] = docx_native_acc

        return jsonify({
            'success': True,
            'accuracy': accuracy_result
        })

    except Exception as e:
        logger.error(f"ä¿å­˜ground truthå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/history', methods=['GET'])
def get_history():
    """
    è·å–å†å²æµ‹è¯•åˆ—è¡¨

    æŸ¥è¯¢å‚æ•°:
        - limit: è¿”å›æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤20ï¼‰
        - has_ground_truth: æ˜¯å¦åªè¿”å›å·²æ ‡æ³¨çš„ï¼ˆå¯é€‰ï¼‰

    å“åº”:
        {
            "success": true,
            "tests": [...]
        }
    """
    try:
        limit = request.args.get('limit', 20, type=int)
        has_ground_truth = request.args.get('has_ground_truth', type=bool)

        db = get_knowledge_base_db()

        sql = "SELECT * FROM v_parser_debug_summary"
        params = []

        if has_ground_truth is not None:
            sql += " WHERE has_ground_truth = ?"
            params.append(1 if has_ground_truth else 0)

        sql += " LIMIT ?"
        params.append(limit)

        rows = db.execute_query(sql, tuple(params))

        tests = []
        for row in rows:
            tests.append(dict(row))

        return jsonify({
            'success': True,
            'tests': tests,
            'total': len(tests)
        })

    except Exception as e:
        logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/<document_id>/delete', methods=['DELETE'])
def delete_test(document_id):
    """åˆ é™¤æµ‹è¯•è®°å½•"""
    try:
        db = get_knowledge_base_db()

        # è·å–æ–‡ä»¶è·¯å¾„å¹¶åˆ é™¤æ–‡ä»¶
        row = db.execute_query(
            "SELECT file_path FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if row and row['file_path']:
            file_path = Path(row['file_path'])
            if file_path.exists():
                file_path.unlink()
                logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")

        # åˆ é™¤æ•°æ®åº“è®°å½•
        db.execute_query(
            "DELETE FROM parser_debug_tests WHERE document_id = ?",
            (document_id,)
        )

        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"åˆ é™¤æµ‹è¯•è®°å½•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/export/<document_id>', methods=['GET'])
def export_comparison_report(document_id):
    """
    å¯¼å‡ºå¯¹æ¯”æŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼‰

    å“åº”:
        å®Œæ•´çš„JSONæŠ¥å‘Šæ–‡ä»¶
    """
    try:
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT * FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æµ‹è¯•è®°å½•ä¸å­˜åœ¨'}), 404

        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report = {
            'document_id': document_id,
            'filename': row['filename'],
            'upload_time': row['upload_time'],
            'document_info': {
                'total_paragraphs': row['total_paragraphs'],
                'has_toc': bool(row['has_toc']),
                'toc_items_count': row['toc_items_count']
            },
            'results': {
                'semantic': json.loads(row['semantic_result']) if row['semantic_result'] else None,
                'style': json.loads(row['style_result']) if row['style_result'] else None,
                'hybrid': json.loads(row['hybrid_result']) if row.get('hybrid_result') else None,
                'azure': json.loads(row['azure_result']) if row.get('azure_result') else None,
                'docx_native': json.loads(row['docx_native_result']) if row.get('docx_native_result') else None,
            },
            'ground_truth': json.loads(row['ground_truth']) if row['ground_truth'] else None,
            'accuracy': None
        }

        # å¦‚æœæœ‰æ ‡æ³¨ï¼Œæ·»åŠ å‡†ç¡®ç‡æ•°æ®
        if row['ground_truth']:
            report['accuracy'] = {
                'semantic': {
                    'precision': row['semantic_precision'],
                    'recall': row['semantic_recall'],
                    'f1_score': row['semantic_f1']
                },
                'style': {
                    'precision': row['style_precision'],
                    'recall': row['style_recall'],
                    'f1_score': row['style_f1']
                },
                'best_method': row['best_method'],
                'best_f1_score': row['best_f1_score']
            }

            # æ·»åŠ hybridç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('hybrid_precision'):
                report['accuracy']['hybrid'] = {
                    'precision': row['hybrid_precision'],
                    'recall': row['hybrid_recall'],
                    'f1_score': row['hybrid_f1']
                }

            # æ·»åŠ azureç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('azure_precision'):
                report['accuracy']['azure'] = {
                    'precision': row['azure_precision'],
                    'recall': row['azure_recall'],
                    'f1_score': row['azure_f1']
                }

            # æ·»åŠ docx_nativeç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('docx_native_precision'):
                report['accuracy']['docx_native'] = {
                    'precision': row['docx_native_precision'],
                    'recall': row['docx_native_recall'],
                    'f1_score': row['docx_native_f1']
                }

        # ä¿å­˜ä¸ºä¸´æ—¶JSONæ–‡ä»¶
        config = get_config()
        temp_dir = config.get_path('data') / 'temp'
        temp_dir.mkdir(parents=True, exist_ok=True)

        report_file = temp_dir / f"parser_comparison_{document_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        return send_file(
            report_file,
            as_attachment=True,
            download_name=f"parser_comparison_{row['filename']}.json",
            mimetype='application/json'
        )

    except Exception as e:
        logger.error(f"å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# æ³¨å†Œè“å›¾åˆ°åº”ç”¨ï¼ˆéœ€è¦åœ¨app.pyä¸­è°ƒç”¨ï¼‰
def register_parser_debug_bp(app):
    """æ³¨å†Œè§£æè°ƒè¯•è“å›¾"""
    app.register_blueprint(api_parser_debug_bp)
    logger.info("è§£æè°ƒè¯•APIå·²æ³¨å†Œ")
