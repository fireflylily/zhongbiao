#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›®å½•è§£ææ–¹æ³•å¯¹æ¯”è°ƒè¯• API
åŠŸèƒ½ï¼š
- ä¸Šä¼ æ ‡ä¹¦æ–‡æ¡£å¹¶è¿è¡Œæ‰€æœ‰è§£ææ–¹æ³•
- å¯¹æ¯”ä¸åŒè§£ææ–¹æ³•çš„å‡†ç¡®ç‡
- æ”¯æŒäººå·¥æ ‡æ³¨æ­£ç¡®ç­”æ¡ˆ
- è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡ï¼ˆP/R/F1ï¼‰
"""

import os
import json
import uuid
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from flask import Blueprint, request, jsonify, send_file, Response
from werkzeug.utils import secure_filename
from docx import Document

from common import get_module_logger, get_config
from common.database import get_knowledge_base_db
from modules.tender_processing.structure_parser import DocumentStructureParser, ChapterNode
from modules.tender_processing.level_analyzer import LevelAnalyzer

# å°è¯•å¯¼å…¥ Azure è§£æå™¨
try:
    from modules.tender_processing.azure_parser import AzureDocumentParser, is_azure_available
    AZURE_PARSER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Azure è§£æå™¨ä¸å¯ç”¨: {e}")
    AZURE_PARSER_AVAILABLE = False

    def is_azure_available():
        return False

# å°è¯•å¯¼å…¥ Gemini è§£æå™¨
try:
    from modules.tender_processing.parsers.gemini_parser import GeminiParser
    GEMINI_PARSER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Gemini è§£æå™¨ä¸å¯ç”¨: {e}")
    GEMINI_PARSER_AVAILABLE = False

logger = get_module_logger("api_parser_debug")

api_parser_debug_bp = Blueprint('api_parser_debug', __name__, url_prefix='/api/parser-debug')


class ParserDebugger:
    """è§£ææ–¹æ³•å¯¹æ¯”è°ƒè¯•å™¨"""

    def __init__(self, doc_path: str):
        """
        åˆå§‹åŒ–è°ƒè¯•å™¨

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„
        """
        self.doc_path = doc_path
        self.parser = DocumentStructureParser()
        self.doc = Document(doc_path)

        # æ–‡æ¡£ä¿¡æ¯
        self.total_paragraphs = len(self.doc.paragraphs)
        self.has_toc = False
        self.toc_items_count = 0
        self.toc_start_idx = None
        self.toc_end_idx = None

        # è®¡ç®—æ–‡æ¡£æ€»å­—æ•° (å»é™¤ç©ºæ ¼)
        self.total_chars = sum(len(p.text.replace(' ', '').replace('\t', '')) for p in self.doc.paragraphs)

        # é¢„å…ˆæ£€æµ‹ç›®å½•
        self._detect_toc_info()

    def _detect_toc_info(self):
        """æ£€æµ‹ç›®å½•ä¿¡æ¯"""
        try:
            toc_idx = self.parser._find_toc_section(self.doc)
            if toc_idx is not None:
                self.has_toc = True
                self.toc_start_idx = toc_idx
                toc_items, toc_end_idx = self.parser._parse_toc_items(self.doc, toc_idx)
                self.toc_items_count = len(toc_items)
                self.toc_end_idx = toc_end_idx
                logger.info(f"æ£€æµ‹åˆ°ç›®å½•: {self.toc_items_count} é¡¹ï¼Œä½äºæ®µè½ {toc_idx}-{toc_end_idx}")
        except Exception as e:
            logger.warning(f"ç›®å½•æ£€æµ‹å¤±è´¥: {e}")

    def get_document_info(self) -> Dict:
        """è·å–æ–‡æ¡£åŸºæœ¬ä¿¡æ¯"""
        return {
            'filename': Path(self.doc_path).name,
            'total_paragraphs': self.total_paragraphs,
            'total_chars': self.total_chars,  # æ–‡æ¡£æ€»å­—æ•°
            'has_toc': self.has_toc,
            'toc_items_count': self.toc_items_count,
            'toc_start_idx': self.toc_start_idx,
            'toc_end_idx': self.toc_end_idx
        }

    def run_all_methods(self) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰è§£ææ–¹æ³•

        Returns:
            {
                'semantic': {...},
                'style': {...},
                'azure': {...},  # å¯é€‰
                'docx_native': {...},
                'gemini': {...}  # å¯é€‰
            }
        """
        results = {}

        # æ–¹æ³•4: Azure Form Recognizerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if is_azure_available() and AZURE_PARSER_AVAILABLE:
            results['azure'] = self._run_with_timing(
                self._run_azure_parser,
                "Azure Form Recognizer"
            )
        else:
            results['azure'] = {
                'success': False,
                'error': 'Azure Form Recognizer æœªé…ç½®æˆ–SDKæœªå®‰è£…',
                'chapters': [],
                'method_name': 'Azure Form Recognizer',
                'performance': {'elapsed': 0}
            }

        # æ–¹æ³•5: Wordå¤§çº²çº§åˆ«è¯†åˆ«
        results['docx_native'] = self._run_with_timing(
            self._run_docx_native,
            "Wordå¤§çº²çº§åˆ«è¯†åˆ«"
        )

        # æ–¹æ³•6: Gemini AIè§£æå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if GEMINI_PARSER_AVAILABLE:
            try:
                gemini_parser = GeminiParser()
                if gemini_parser.is_available():
                    results['gemini'] = self._run_with_timing(
                        lambda: self._run_gemini_parser(gemini_parser),
                        "Gemini AIè§£æå™¨"
                    )
                else:
                    results['gemini'] = {
                        'success': False,
                        'error': 'Gemini APIå¯†é’¥æœªé…ç½®',
                        'chapters': [],
                        'method_name': 'Gemini AIè§£æå™¨',
                        'performance': {'elapsed': 0}
                    }
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–Geminiè§£æå™¨å¤±è´¥: {e}")
                results['gemini'] = {
                    'success': False,
                    'error': f'Geminiè§£æå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}',
                    'chapters': [],
                    'method_name': 'Gemini AIè§£æå™¨',
                    'performance': {'elapsed': 0}
                }
        else:
            results['gemini'] = {
                'success': False,
                'error': 'Gemini SDKæœªå®‰è£… (pip install google-generativeai)',
                'chapters': [],
                'method_name': 'Gemini AIè§£æå™¨',
                'performance': {'elapsed': 0}
            }

        return results

    def _run_with_timing(self, method_func, method_name: str) -> Dict:
        """
        è¿è¡Œæ–¹æ³•å¹¶è®¡æ—¶

        Args:
            method_func: è¦è¿è¡Œçš„æ–¹æ³•
            method_name: æ–¹æ³•åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            åŒ…å«ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        logger.info(f"å¼€å§‹è¿è¡Œ: {method_name}")
        start_time = time.time()

        try:
            result = method_func()
            elapsed = time.time() - start_time

            result['performance'] = {
                'elapsed': round(elapsed, 3),
                'elapsed_formatted': f"{elapsed:.3f}s"
            }

            logger.info(f"{method_name} å®Œæˆï¼Œè€—æ—¶ {elapsed:.3f}s")
            return result

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"{method_name} å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'chapters': [],
                'performance': {'elapsed': round(elapsed, 3)}
            }

    def _run_toc_exact_match(self) -> Dict:
        """æ–¹æ³•0: ç²¾ç¡®åŒ¹é…(åŸºäºç›®å½•) - ç›´æ¥ä½¿ç”¨ç›®å½•å®šä½ç« èŠ‚"""
        return self.parser.parse_by_toc_exact(self.doc_path)

    def _run_azure_parser(self) -> Dict:
        """æ–¹æ³•4: Azure Form Recognizer è§£æ"""
        try:
            azure_parser = AzureDocumentParser()
            result = azure_parser.parse_document_structure(self.doc_path)
            return result
        except Exception as e:
            logger.error(f"Azure è§£æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _run_docx_native(self) -> Dict:
        """æ–¹æ³•5: Wordå¤§çº²çº§åˆ«è¯†åˆ«ï¼ˆå¾®è½¯å®˜æ–¹APIï¼‰"""
        return self.parser.parse_by_outline_level(self.doc_path)

    def _run_gemini_parser(self, gemini_parser: 'GeminiParser') -> Dict:
        """æ–¹æ³•6: Gemini AIè§£æå™¨"""
        try:
            # è°ƒç”¨Geminiè§£æå™¨çš„parse_structureæ–¹æ³•
            result = gemini_parser.parse_structure(self.doc_path)

            # Geminiè¿”å›çš„ç»“æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼,åŒ…å«success, chapters, statisticsç­‰
            # ä½†éœ€è¦ç¡®ä¿chaptersæ˜¯dictæ ¼å¼è€Œä¸æ˜¯ChapterNodeå¯¹è±¡
            if result.get('success') and result.get('chapters'):
                # å¦‚æœchaptersæ˜¯ChapterNodeå¯¹è±¡åˆ—è¡¨,éœ€è¦è½¬æ¢ä¸ºdict
                chapters = result['chapters']
                if chapters and hasattr(chapters[0], 'to_dict'):
                    result['chapters'] = [ch.to_dict() if hasattr(ch, 'to_dict') else ch for ch in chapters]

            return result

        except Exception as e:
            logger.error(f"Geminiè§£æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _run_llm_level_analyzer(self, model_name: str = "deepseek-v3") -> Dict:
        """
        æ–¹æ³•7: LLMæ™ºèƒ½å±‚çº§åˆ†æ

        ä½¿ç”¨LLMç†è§£ç›®å½•è¯­ä¹‰ï¼Œæ™ºèƒ½åˆ¤æ–­æ¯ä¸ªç›®å½•é¡¹çš„å±‚çº§
        """
        try:
            # é¦–å…ˆéœ€è¦è·å–ç›®å½•é¡¹åˆ—è¡¨ï¼ˆä½¿ç”¨ç°æœ‰çš„TOCè§£æï¼‰
            toc_idx = self.parser._find_toc_section(self.doc)
            if toc_idx is None:
                return {
                    'success': False,
                    'error': 'æœªæ‰¾åˆ°ç›®å½•èŠ‚',
                    'chapters': [],
                    'method_name': 'LLMæ™ºèƒ½å±‚çº§åˆ†æ'
                }

            # è§£æç›®å½•é¡¹
            toc_items, toc_end_idx = self.parser._parse_toc_items(self.doc, toc_idx)
            if not toc_items:
                return {
                    'success': False,
                    'error': 'ç›®å½•è§£æå¤±è´¥',
                    'chapters': [],
                    'method_name': 'LLMæ™ºèƒ½å±‚çº§åˆ†æ'
                }

            logger.info(f"LLMå±‚çº§åˆ†æ: è§£æåˆ° {len(toc_items)} ä¸ªç›®å½•é¡¹")

            # ä½¿ç”¨LLMåˆ†æå±‚çº§
            level_analyzer = LevelAnalyzer()
            levels = level_analyzer.analyze_toc_hierarchy_with_llm(toc_items, model_name)

            # å°†å±‚çº§ç»“æœåº”ç”¨åˆ°ç›®å½•é¡¹
            for i, item in enumerate(toc_items):
                if i < len(levels):
                    item['level'] = levels[i]
                else:
                    item['level'] = 2  # é»˜è®¤2çº§

            # ğŸ”§ ä½¿ç”¨ _locate_chapters_by_toc å®šä½ç« èŠ‚å¹¶è®¡ç®—å­—æ•°
            # è¿™æ ·å¯ä»¥è·å–æ¯ä¸ªç« èŠ‚çš„ word_count
            # æ³¨æ„: _locate_chapters_by_toc å†…éƒ¨å·²ç»è°ƒç”¨äº† _build_chapter_treeï¼Œè¿”å›çš„å°±æ˜¯æ ‘å½¢ç»“æ„
            chapter_tree = self.parser._locate_chapters_by_toc(self.doc, toc_items, toc_end_idx)

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            chapters = [ch.to_dict() for ch in chapter_tree]

            # ğŸ†• æ·»åŠ ç« èŠ‚ç±»å‹åˆ†ç±»
            classified_chapters, key_sections = self.parser._classify_chapters(chapters)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = self.parser._calculate_statistics(chapter_tree)

            return {
                'success': True,
                'chapters': classified_chapters,
                'method_name': 'LLMæ™ºèƒ½å±‚çº§åˆ†æ',
                'model_used': model_name,
                'key_sections': key_sections,
                'statistics': {
                    'total_items': len(toc_items),
                    'total_words': stats.get('total_words', self.total_chars),
                    'level_1_count': sum(1 for l in levels if l == 1),
                    'level_2_count': sum(1 for l in levels if l == 2),
                    'level_3_count': sum(1 for l in levels if l == 3),
                }
            }

        except Exception as e:
            logger.error(f"LLMå±‚çº§åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def _build_chapter_tree_from_toc(self, toc_items: List[Dict]) -> List[Dict]:
        """
        ä»å¸¦å±‚çº§çš„ç›®å½•é¡¹åˆ—è¡¨æ„å»ºç« èŠ‚æ ‘

        Args:
            toc_items: å¸¦levelçš„ç›®å½•é¡¹åˆ—è¡¨

        Returns:
            å±‚çº§ç»“æ„çš„ç« èŠ‚åˆ—è¡¨
        """
        if not toc_items:
            return []

        chapters = []
        stack = []  # ç”¨äºè·Ÿè¸ªçˆ¶èŠ‚ç‚¹

        for item in toc_items:
            chapter = {
                'title': item.get('title', ''),
                'level': item.get('level', 1),
                'page_num': item.get('page_num'),
                'children': []
            }

            level = chapter['level']

            # æ¸…ç©ºæ¯”å½“å‰å±‚çº§é«˜æˆ–ç›¸ç­‰çš„èŠ‚ç‚¹
            while stack and stack[-1]['level'] >= level:
                stack.pop()

            if not stack:
                # æ²¡æœ‰çˆ¶èŠ‚ç‚¹ï¼Œæ·»åŠ åˆ°æ ¹
                chapters.append(chapter)
            else:
                # æ·»åŠ åˆ°æœ€è¿‘çš„çˆ¶èŠ‚ç‚¹
                stack[-1]['children'].append(chapter)

            stack.append(chapter)

        return chapters

    @staticmethod
    def calculate_accuracy(detected_chapters: List[Dict], ground_truth_chapters: List[Dict]) -> Dict:
        """
        è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡

        Args:
            detected_chapters: æ£€æµ‹åˆ°çš„ç« èŠ‚åˆ—è¡¨
            ground_truth_chapters: æ­£ç¡®ç­”æ¡ˆç« èŠ‚åˆ—è¡¨

        Returns:
            {
                'precision': 0.0-1.0,
                'recall': 0.0-1.0,
                'f1_score': 0.0-1.0,
                'matched_count': int,
                'detected_count': int,
                'ground_truth_count': int,
                'details': [...]
            }
        """
        if not ground_truth_chapters:
            return {
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'matched_count': 0,
                'detected_count': len(detected_chapters),
                'ground_truth_count': 0
            }

        # æ‰å¹³åŒ–ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å«å­ç« èŠ‚ï¼‰
        def flatten_chapters(chapters_list):
            flat = []
            for ch in chapters_list:
                flat.append(ch)
                if 'children' in ch and ch['children']:
                    flat.extend(flatten_chapters(ch['children']))
            return flat

        detected_flat = flatten_chapters(detected_chapters)
        truth_flat = flatten_chapters(ground_truth_chapters)

        # è§„èŒƒåŒ–æ ‡é¢˜ï¼ˆç”¨äºåŒ¹é…ï¼‰
        def normalize_title(title: str) -> str:
            import re
            # ç§»é™¤æ‰€æœ‰ç©ºæ ¼ã€ç¼–å·
            cleaned = re.sub(r'^\d+\.\s*', '', title)
            cleaned = re.sub(r'^\d+\.\d+\s*', '', cleaned)
            cleaned = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*', '', cleaned)
            cleaned = re.sub(r'\s+', '', cleaned)
            return cleaned.lower()

        # æ„å»ºçœŸå®ç­”æ¡ˆçš„æ ‡é¢˜é›†åˆ
        truth_titles = {normalize_title(ch['title']): ch for ch in truth_flat}
        detected_titles = {normalize_title(ch['title']): ch for ch in detected_flat}

        # è®¡ç®—åŒ¹é…
        matched_titles = set(truth_titles.keys()) & set(detected_titles.keys())
        matched_count = len(matched_titles)

        # è®¡ç®—æŒ‡æ ‡
        precision = matched_count / len(detected_flat) if detected_flat else 0.0
        recall = matched_count / len(truth_flat) if truth_flat else 0.0
        f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

        # è¯¦ç»†åŒ¹é…ä¿¡æ¯
        details = []
        for title in truth_titles.keys():
            if title in matched_titles:
                details.append({
                    'title': truth_titles[title]['title'],
                    'status': 'matched',
                    'detected': True
                })
            else:
                details.append({
                    'title': truth_titles[title]['title'],
                    'status': 'missed',
                    'detected': False
                })

        # æ£€æµ‹å¤šä½™çš„ï¼ˆè¯¯æ£€ï¼‰
        for title in detected_titles.keys():
            if title not in matched_titles:
                details.append({
                    'title': detected_titles[title]['title'],
                    'status': 'false_positive',
                    'detected': True
                })

        return {
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'f1_score': round(f1_score, 4),
            'matched_count': matched_count,
            'detected_count': len(detected_flat),
            'ground_truth_count': len(truth_flat),
            'details': details
        }


@api_parser_debug_bp.route('/parse-smart/<document_id>', methods=['POST'])
def parse_smart(document_id):
    """
    æ™ºèƒ½è§£æï¼šç»“æ„è¯†åˆ« + ç±»å‹åˆ†ç±»

    æµç¨‹ï¼š
    1. æ£€æŸ¥æ˜¯å¦æœ‰ç›®å½•
       - æœ‰ç›®å½• â†’ ç²¾ç¡®åŒ¹é…(toc_exact)
       - æ— ç›®å½• â†’ Wordå¤§çº²è¯†åˆ«(docx_native)
    2. æ£€æŸ¥ç»“æœæ˜¯å¦å¼‚å¸¸
       - æ­£å¸¸ â†’ å¯¹ç« èŠ‚è¿›è¡Œç±»å‹åˆ†ç±»
       - å¼‚å¸¸ â†’ å›é€€åˆ°LLMå±‚çº§åˆ†æ

    è¯·æ±‚å‚æ•°:
        - classify: æ˜¯å¦è¿›è¡Œç« èŠ‚ç±»å‹åˆ†ç±» (é»˜è®¤true)

    å“åº”:
        {
            "success": true,
            "result": {
                "chapters": [...],
                "method_used": "toc_exact",
                "fallback_from": null,
                "fallback_reason": null,
                "key_sections": {...}
            }
        }
    """
    try:
        # è·å–è¯·æ±‚å‚æ•°
        data = request.get_json() or {}
        classify_chapters = data.get('classify', True)

        # è·å–æ–‡ä»¶è·¯å¾„
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT file_path FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

        file_path = row['file_path']

        # åˆå§‹åŒ–è§£æå™¨
        parser = DocumentStructureParser()

        # è°ƒç”¨æ™ºèƒ½è§£ææ–¹æ³•
        start_time = time.time()
        result = parser.parse_smart(file_path, classify_chapters=classify_chapters)
        elapsed = time.time() - start_time

        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        result['performance'] = {
            'elapsed': round(elapsed, 3),
            'elapsed_formatted': f"{elapsed:.3f}s"
        }

        logger.info(f"æ™ºèƒ½è§£æå®Œæˆ: method={result.get('method_used')}, "
                   f"fallback={result.get('fallback_from')}, "
                   f"chapters={len(result.get('chapters', []))}, "
                   f"elapsed={elapsed:.3f}s")

        return jsonify({
            'success': True,
            'result': result
        })

    except Exception as e:
        logger.error(f"æ™ºèƒ½è§£æå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/upload', methods=['POST'])
def upload_document():
    """
    ä¸Šä¼ æ–‡æ¡£å¹¶è¿”å›document_idï¼ˆæµå¼è§£æè¯·ä½¿ç”¨ /upload-streamï¼‰

    è¯·æ±‚:
        - file: .docxæ–‡ä»¶

    å“åº”:
        {
            "success": true,
            "document_id": "uuid",
            "document_info": {...}
        }
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400

        file = request.files['file']
        if not file.filename:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶åä¸ºç©º'}), 400

        if not file.filename.endswith('.docx'):
            return jsonify({'success': False, 'error': 'ä»…æ”¯æŒ .docx æ ¼å¼æ–‡ä»¶'}), 400

        # ä¿å­˜æ–‡ä»¶
        document_id = str(uuid.uuid4())
        original_filename = file.filename

        config = get_config()
        upload_dir = config.get_path('data') / 'parser_debug'
        upload_dir.mkdir(parents=True, exist_ok=True)

        file_path = upload_dir / f"{document_id}.docx"
        file.save(str(file_path))

        logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")

        # è·å–æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
        debugger = ParserDebugger(str(file_path))
        document_info = debugger.get_document_info()
        document_info['filename'] = original_filename

        # åˆå§‹åŒ–æ•°æ®åº“è®°å½•ï¼ˆç»“æœä¸ºç©ºï¼Œç¨åç”±æµå¼APIæ›´æ–°ï¼‰
        db = get_knowledge_base_db()
        db.execute_query("""
            INSERT INTO parser_debug_tests (
                document_id, filename, file_path,
                total_paragraphs, has_toc, toc_items_count, toc_start_idx, toc_end_idx
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            document_id,
            original_filename,
            str(file_path),
            document_info['total_paragraphs'],
            document_info['has_toc'],
            document_info['toc_items_count'],
            document_info['toc_start_idx'],
            document_info['toc_end_idx']
        ))

        return jsonify({
            'success': True,
            'document_id': document_id,
            'document_info': document_info
        })

    except Exception as e:
        logger.error(f"ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/parse-single/<document_id>/<method>', methods=['POST'])
def parse_single_method(document_id, method):
    """
    è§£æå•ä¸ªæ–¹æ³•

    Args:
        document_id: æ–‡æ¡£ID
        method: è§£ææ–¹æ³• (toc_exact/azure/docx_native/gemini)

    è¿”å›:
        {
            "success": true,
            "result": {...}
        }
    """
    try:
        # è·å–æ–‡ä»¶è·¯å¾„
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT file_path FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

        file_path = row['file_path']
        debugger = ParserDebugger(file_path)

        # æ ¹æ®methodé€‰æ‹©å¯¹åº”çš„è§£æå™¨
        method_map = {
            'toc_exact': (debugger._run_toc_exact_match, 'ç²¾ç¡®åŒ¹é…(åŸºäºç›®å½•)'),
            'docx_native': (debugger._run_docx_native, 'Wordå¤§çº²çº§åˆ«è¯†åˆ«'),
            'azure': (debugger._run_azure_parser, 'Azure Form Recognizer'),
            'gemini': (lambda: debugger._run_gemini_parser(GeminiParser()) if GEMINI_PARSER_AVAILABLE else None, 'Gemini AIè§£æå™¨'),
            'llm_level': (debugger._run_llm_level_analyzer, 'LLMæ™ºèƒ½å±‚çº§åˆ†æ')
        }

        if method not in method_map:
            return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„è§£ææ–¹æ³•: {method}'}), 400

        method_func, method_name = method_map[method]

        # ç‰¹æ®Šå¤„ç†Azureå’ŒGemini
        if method == 'azure' and not (is_azure_available() and AZURE_PARSER_AVAILABLE):
            result = {
                'success': False,
                'error': 'Azure Form Recognizer æœªé…ç½®æˆ–SDKæœªå®‰è£…',
                'chapters': [],
                'method_name': method_name,
                'performance': {'elapsed': 0}
            }
        elif method == 'gemini' and not GEMINI_PARSER_AVAILABLE:
            result = {
                'success': False,
                'error': 'Gemini SDKæœªå®‰è£… (pip install google-generativeai)',
                'chapters': [],
                'method_name': method_name,
                'performance': {'elapsed': 0}
            }
        elif method == 'gemini':
            try:
                gemini_parser = GeminiParser()
                if not gemini_parser.is_available():
                    result = {
                        'success': False,
                        'error': 'Gemini APIå¯†é’¥æœªé…ç½®',
                        'chapters': [],
                        'method_name': method_name,
                        'performance': {'elapsed': 0}
                    }
                else:
                    result = debugger._run_with_timing(lambda: debugger._run_gemini_parser(gemini_parser), method_name)
            except Exception as e:
                logger.error(f"Geminiè§£æå¤±è´¥: {e}")
                result = {
                    'success': False,
                    'error': str(e),
                    'chapters': [],
                    'method_name': method_name,
                    'performance': {'elapsed': 0}
                }
        else:
            # æ‰§è¡Œè§£æ
            result = debugger._run_with_timing(method_func, method_name)

        # æ›´æ–°æ•°æ®åº“
        db.execute_query(f"""
            UPDATE parser_debug_tests
            SET {method}_result = ?,
                {method}_elapsed = ?,
                {method}_chapters_count = ?
            WHERE document_id = ?
        """, (
            json.dumps(result, ensure_ascii=False),
            result['performance']['elapsed'],
            len(result.get('chapters', [])),
            document_id
        ))

        logger.info(f"å•æ–¹æ³•è§£æå®Œæˆ: {method_name}, è€—æ—¶ {result['performance']['elapsed']}s")

        return jsonify({
            'success': True,
            'result': result
        })

    except Exception as e:
        logger.error(f"å•æ–¹æ³•è§£æå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/parse-stream/<document_id>', methods=['GET'])
def parse_document_stream(document_id):
    """
    æµå¼è§£ææ–‡æ¡£ - æ¯å®Œæˆä¸€ä¸ªè§£æå™¨å°±ç«‹å³è¿”å›ç»“æœ

    ä½¿ç”¨Server-Sent Events (SSE)æ ¼å¼

    äº‹ä»¶æ ¼å¼:
        data: {"method": "style", "result": {...}, "progress": "2/5"}
        data: {"method": "complete", "document_id": "xxx"}
    """
    def generate():
        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            db = get_knowledge_base_db()
            row = db.execute_query(
                "SELECT file_path FROM parser_debug_tests WHERE document_id = ?",
                (document_id,),
                fetch_one=True
            )

            if not row:
                yield f"data: {json.dumps({'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}, ensure_ascii=False)}\n\n"
                return

            file_path = row['file_path']
            debugger = ParserDebugger(file_path)

            # å®šä¹‰è¦è¿è¡Œçš„è§£æå™¨åˆ—è¡¨ï¼ˆæŒ‰æ–°é¡ºåºï¼šGemini â†’ Wordå¤§çº² â†’ ç²¾ç¡®åŒ¹é… â†’ Azure â†’ å…¶ä»–ï¼‰
            parsers = []

            # 1. Gemini AIè§£æå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if GEMINI_PARSER_AVAILABLE:
                try:
                    gemini_parser = GeminiParser()
                    if gemini_parser.is_available():
                        parsers.append(('gemini', lambda: debugger._run_gemini_parser(gemini_parser), 'Gemini AIè§£æå™¨'))
                except:
                    pass

            # 2. Wordå¤§çº²çº§åˆ«è¯†åˆ«
            parsers.append(('docx_native', debugger._run_docx_native, 'Wordå¤§çº²çº§åˆ«è¯†åˆ«'))

            # 3. ç²¾ç¡®åŒ¹é…(åŸºäºç›®å½•)
            parsers.append(('toc_exact', debugger._run_toc_exact_match, 'ç²¾ç¡®åŒ¹é…(åŸºäºç›®å½•)'))

            # 4. Azure Form Recognizerï¼ˆå¦‚æœå¯ç”¨ï¼‰
            parsers.append(('azure', lambda: debugger._run_azure_parser() if is_azure_available() and AZURE_PARSER_AVAILABLE else {
                'success': False, 'error': 'Azureæœªé…ç½®', 'chapters': [], 'method_name': 'Azure Form Recognizer', 'performance': {'elapsed': 0}
            }, 'Azure Form Recognizer'))

            # 5. LLMæ™ºèƒ½å±‚çº§åˆ†æ
            parsers.append(('llm_level', debugger._run_llm_level_analyzer, 'LLMæ™ºèƒ½å±‚çº§åˆ†æ'))

            total = len(parsers)
            results_dict = {
                'semantic': {
                    'success': False,
                    'error': 'è¯­ä¹‰é”šç‚¹è§£æå·²ç¦ç”¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ä¸­ï¼‰',
                    'chapters': [],
                    'method_name': 'è¯­ä¹‰é”šç‚¹è§£æ',
                    'performance': {'elapsed': 0}
                }
            }

            # é€ä¸ªè¿è¡Œè§£æå™¨å¹¶æµå¼è¿”å›ç»“æœ
            for idx, (method_key, method_func, method_name) in enumerate(parsers, 1):
                try:
                    logger.info(f"[æµå¼è§£æ {idx}/{total}] å¼€å§‹è¿è¡Œ: {method_name}")
                    result = debugger._run_with_timing(method_func, method_name)
                    results_dict[method_key] = result

                    # ç«‹å³å‘é€ç»“æœç»™å‰ç«¯
                    event_data = {
                        'method': method_key,
                        'method_name': method_name,
                        'result': result,
                        'progress': f"{idx}/{total}",
                        'progress_percent': int((idx / total) * 100)
                    }
                    yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"

                    # ç«‹å³æ›´æ–°æ•°æ®åº“
                    db.execute_query(f"""
                        UPDATE parser_debug_tests
                        SET {method_key}_result = ?,
                            {method_key}_elapsed = ?,
                            {method_key}_chapters_count = ?
                        WHERE document_id = ?
                    """, (
                        json.dumps(result, ensure_ascii=False),
                        result['performance']['elapsed'],
                        len(result.get('chapters', [])),
                        document_id
                    ))

                except Exception as e:
                    logger.error(f"è§£æå™¨ {method_name} å¤±è´¥: {e}")
                    error_result = {
                        'success': False,
                        'error': str(e),
                        'chapters': [],
                        'method_name': method_name,
                        'performance': {'elapsed': 0}
                    }
                    results_dict[method_key] = error_result

                    event_data = {
                        'method': method_key,
                        'method_name': method_name,
                        'result': error_result,
                        'progress': f"{idx}/{total}",
                        'progress_percent': int((idx / total) * 100)
                    }
                    yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"

            # åŒæ­¥è¯­ä¹‰é”šç‚¹è§£æç»“æœåˆ°æ•°æ®åº“
            db.execute_query("""
                UPDATE parser_debug_tests
                SET semantic_result = ?, semantic_elapsed = ?, semantic_chapters_count = ?
                WHERE document_id = ?
            """, (
                json.dumps(results_dict['semantic'], ensure_ascii=False),
                0,
                0,
                document_id
            ))

            # å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'method': 'complete', 'document_id': document_id}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"æµå¼è§£æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

    return Response(generate(), mimetype='text/event-stream')


@api_parser_debug_bp.route('/<document_id>', methods=['GET'])
def get_test_result(document_id):
    """
    è·å–æµ‹è¯•ç»“æœ

    å“åº”:
        {
            "success": true,
            "document_info": {...},
            "results": {...},
            "ground_truth": {...},
            "accuracy": {...}
        }
    """
    try:
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT * FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æµ‹è¯•è®°å½•ä¸å­˜åœ¨'}), 404

        # è§£æç»“æœ
        results = {
            'toc_exact': json.loads(row['toc_exact_result']) if row.get('toc_exact_result') else None,
            'semantic': json.loads(row['semantic_result']) if row['semantic_result'] else None,
            'style': json.loads(row['style_result']) if row['style_result'] else None,
            'hybrid': json.loads(row['hybrid_result']) if row.get('hybrid_result') else None,
            'azure': json.loads(row['azure_result']) if row.get('azure_result') else None,
            'docx_native': json.loads(row['docx_native_result']) if row.get('docx_native_result') else None,
            'gemini': json.loads(row['gemini_result']) if row.get('gemini_result') else None,
            'llm_level': json.loads(row['llm_level_result']) if row.get('llm_level_result') else None,
        }

        document_info = {
            'filename': row['filename'],
            'total_paragraphs': row['total_paragraphs'],
            'has_toc': bool(row['has_toc']),
            'toc_items_count': row['toc_items_count'],
            'upload_time': row['upload_time']
        }

        ground_truth = json.loads(row['ground_truth']) if row['ground_truth'] else None

        # å¦‚æœæœ‰ground_truthï¼Œè¿”å›å‡†ç¡®ç‡æ•°æ®
        accuracy = None
        if ground_truth:
            accuracy = {
                'toc_exact': {
                    'precision': row.get('toc_exact_precision'),
                    'recall': row.get('toc_exact_recall'),
                    'f1_score': row.get('toc_exact_f1')
                } if row.get('toc_exact_precision') else None,
                'semantic': {
                    'precision': row['semantic_precision'],
                    'recall': row['semantic_recall'],
                    'f1_score': row['semantic_f1']
                },
                'style': {
                    'precision': row['style_precision'],
                    'recall': row['style_recall'],
                    'f1_score': row['style_f1']
                },
                'hybrid': {
                    'precision': row.get('hybrid_precision'),
                    'recall': row.get('hybrid_recall'),
                    'f1_score': row.get('hybrid_f1')
                } if row.get('hybrid_precision') else None,
                'azure': {
                    'precision': row.get('azure_precision'),
                    'recall': row.get('azure_recall'),
                    'f1_score': row.get('azure_f1')
                } if row.get('azure_precision') else None,
                'docx_native': {
                    'precision': row.get('docx_native_precision'),
                    'recall': row.get('docx_native_recall'),
                    'f1_score': row.get('docx_native_f1')
                } if row.get('docx_native_precision') else None,
                'gemini': {
                    'precision': row.get('gemini_precision'),
                    'recall': row.get('gemini_recall'),
                    'f1_score': row.get('gemini_f1')
                } if row.get('gemini_precision') else None,
                'llm_level': {
                    'precision': row.get('llm_level_precision'),
                    'recall': row.get('llm_level_recall'),
                    'f1_score': row.get('llm_level_f1')
                } if row.get('llm_level_precision') else None,
                'best_method': row['best_method'],
                'best_f1_score': row['best_f1_score']
            }

        return jsonify({
            'success': True,
            'document_id': document_id,
            'document_info': document_info,
            'results': results,
            'ground_truth': ground_truth,
            'accuracy': accuracy
        })

    except Exception as e:
        logger.error(f"è·å–æµ‹è¯•ç»“æœå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/<document_id>/ground-truth', methods=['POST'])
def save_ground_truth(document_id):
    """
    ä¿å­˜äººå·¥æ ‡æ³¨çš„æ­£ç¡®ç­”æ¡ˆ

    è¯·æ±‚:
        {
            "chapters": [...],  # æ­£ç¡®çš„ç« èŠ‚åˆ—è¡¨
            "annotator": "ç”¨æˆ·å"
        }

    å“åº”:
        {
            "success": true,
            "accuracy": {...}  # è‡ªåŠ¨è®¡ç®—çš„å‡†ç¡®ç‡
        }
    """
    try:
        data = request.get_json()
        if not data or 'chapters' not in data:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ç« èŠ‚æ•°æ®'}), 400

        chapters = data['chapters']
        annotator = data.get('annotator', 'unknown')

        # è·å–ç°æœ‰æµ‹è¯•ç»“æœ
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT toc_exact_result, semantic_result, style_result, hybrid_result, azure_result, docx_native_result, gemini_result, llm_level_result FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æµ‹è¯•è®°å½•ä¸å­˜åœ¨'}), 404

        # è§£æå„æ–¹æ³•çš„ç»“æœ
        toc_exact_chapters = json.loads(row['toc_exact_result'])['chapters'] if row.get('toc_exact_result') else []
        semantic_chapters = json.loads(row['semantic_result'])['chapters'] if row['semantic_result'] else []
        style_chapters = json.loads(row['style_result'])['chapters'] if row['style_result'] else []
        hybrid_chapters = json.loads(row['hybrid_result'])['chapters'] if row.get('hybrid_result') else []
        azure_chapters = json.loads(row['azure_result'])['chapters'] if row.get('azure_result') else []
        docx_native_chapters = json.loads(row['docx_native_result'])['chapters'] if row.get('docx_native_result') else []
        gemini_chapters = json.loads(row['gemini_result'])['chapters'] if row.get('gemini_result') else []
        llm_level_chapters = json.loads(row['llm_level_result'])['chapters'] if row.get('llm_level_result') else []

        # è®¡ç®—å„æ–¹æ³•çš„å‡†ç¡®ç‡
        toc_exact_acc = ParserDebugger.calculate_accuracy(toc_exact_chapters, chapters) if toc_exact_chapters else None
        semantic_acc = ParserDebugger.calculate_accuracy(semantic_chapters, chapters)
        style_acc = ParserDebugger.calculate_accuracy(style_chapters, chapters)
        hybrid_acc = ParserDebugger.calculate_accuracy(hybrid_chapters, chapters) if hybrid_chapters else None
        azure_acc = ParserDebugger.calculate_accuracy(azure_chapters, chapters) if azure_chapters else None
        docx_native_acc = ParserDebugger.calculate_accuracy(docx_native_chapters, chapters) if docx_native_chapters else None
        gemini_acc = ParserDebugger.calculate_accuracy(gemini_chapters, chapters) if gemini_chapters else None
        llm_level_acc = ParserDebugger.calculate_accuracy(llm_level_chapters, chapters) if llm_level_chapters else None

        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        all_f1 = {
            'semantic': semantic_acc['f1_score'],
            'style': style_acc['f1_score'],
        }
        if toc_exact_acc:
            all_f1['toc_exact'] = toc_exact_acc['f1_score']
        if hybrid_acc:
            all_f1['hybrid'] = hybrid_acc['f1_score']
        if azure_acc:
            all_f1['azure'] = azure_acc['f1_score']
        if docx_native_acc:
            all_f1['docx_native'] = docx_native_acc['f1_score']
        if gemini_acc:
            all_f1['gemini'] = gemini_acc['f1_score']
        if llm_level_acc:
            all_f1['llm_level'] = llm_level_acc['f1_score']
        best_method = max(all_f1, key=all_f1.get)
        best_f1_score = all_f1[best_method]

        # æ›´æ–°æ•°æ®åº“
        update_params = [
            json.dumps(chapters, ensure_ascii=False),
            annotator,
            datetime.now().isoformat(),
            len(chapters),
            semantic_acc['precision'], semantic_acc['recall'], semantic_acc['f1_score'],
            style_acc['precision'], style_acc['recall'], style_acc['f1_score'],
        ]

        # å¦‚æœæœ‰ toc_exact ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if toc_exact_acc:
            update_params.extend([toc_exact_acc['precision'], toc_exact_acc['recall'], toc_exact_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ hybrid ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if hybrid_acc:
            update_params.extend([hybrid_acc['precision'], hybrid_acc['recall'], hybrid_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ Azure ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if azure_acc:
            update_params.extend([azure_acc['precision'], azure_acc['recall'], azure_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ docx_native ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if docx_native_acc:
            update_params.extend([docx_native_acc['precision'], docx_native_acc['recall'], docx_native_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ Gemini ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if gemini_acc:
            update_params.extend([gemini_acc['precision'], gemini_acc['recall'], gemini_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        # å¦‚æœæœ‰ LLMå±‚çº§åˆ†æ ç»“æœï¼Œæ·»åŠ å…¶å‡†ç¡®ç‡
        if llm_level_acc:
            update_params.extend([llm_level_acc['precision'], llm_level_acc['recall'], llm_level_acc['f1_score']])
        else:
            update_params.extend([None, None, None])

        update_params.extend([best_method, best_f1_score, document_id])

        db.execute_query("""
            UPDATE parser_debug_tests SET
                ground_truth = ?, annotator = ?, annotation_time = ?, ground_truth_count = ?,
                semantic_precision = ?, semantic_recall = ?, semantic_f1 = ?,
                style_precision = ?, style_recall = ?, style_f1 = ?,
                toc_exact_precision = ?, toc_exact_recall = ?, toc_exact_f1 = ?,
                hybrid_precision = ?, hybrid_recall = ?, hybrid_f1 = ?,
                azure_precision = ?, azure_recall = ?, azure_f1 = ?,
                docx_native_precision = ?, docx_native_recall = ?, docx_native_f1 = ?,
                gemini_precision = ?, gemini_recall = ?, gemini_f1 = ?,
                llm_level_precision = ?, llm_level_recall = ?, llm_level_f1 = ?,
                best_method = ?, best_f1_score = ?
            WHERE document_id = ?
        """, tuple(update_params))

        accuracy_result = {
            'semantic': semantic_acc,
            'style': style_acc,
            'best_method': best_method,
            'best_f1_score': best_f1_score
        }

        if toc_exact_acc:
            accuracy_result['toc_exact'] = toc_exact_acc
        if hybrid_acc:
            accuracy_result['hybrid'] = hybrid_acc
        if azure_acc:
            accuracy_result['azure'] = azure_acc
        if docx_native_acc:
            accuracy_result['docx_native'] = docx_native_acc
        if gemini_acc:
            accuracy_result['gemini'] = gemini_acc
        if llm_level_acc:
            accuracy_result['llm_level'] = llm_level_acc

        return jsonify({
            'success': True,
            'accuracy': accuracy_result
        })

    except Exception as e:
        logger.error(f"ä¿å­˜ground truthå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/history', methods=['GET'])
def get_history():
    """
    è·å–å†å²æµ‹è¯•åˆ—è¡¨

    æŸ¥è¯¢å‚æ•°:
        - limit: è¿”å›æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤20ï¼‰
        - has_ground_truth: æ˜¯å¦åªè¿”å›å·²æ ‡æ³¨çš„ï¼ˆå¯é€‰ï¼‰

    å“åº”:
        {
            "success": true,
            "tests": [...]
        }
    """
    try:
        limit = request.args.get('limit', 20, type=int)
        has_ground_truth = request.args.get('has_ground_truth', type=bool)

        db = get_knowledge_base_db()

        sql = "SELECT * FROM v_parser_debug_summary"
        params = []

        if has_ground_truth is not None:
            sql += " WHERE has_ground_truth = ?"
            params.append(1 if has_ground_truth else 0)

        sql += " LIMIT ?"
        params.append(limit)

        rows = db.execute_query(sql, tuple(params))

        tests = []
        for row in rows:
            tests.append(dict(row))

        return jsonify({
            'success': True,
            'tests': tests,
            'total': len(tests)
        })

    except Exception as e:
        logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/<document_id>/delete', methods=['DELETE'])
def delete_test(document_id):
    """åˆ é™¤æµ‹è¯•è®°å½•"""
    try:
        db = get_knowledge_base_db()

        # è·å–æ–‡ä»¶è·¯å¾„å¹¶åˆ é™¤æ–‡ä»¶
        row = db.execute_query(
            "SELECT file_path FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if row and row['file_path']:
            file_path = Path(row['file_path'])
            if file_path.exists():
                file_path.unlink()
                logger.info(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")

        # åˆ é™¤æ•°æ®åº“è®°å½•
        db.execute_query(
            "DELETE FROM parser_debug_tests WHERE document_id = ?",
            (document_id,)
        )

        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"åˆ é™¤æµ‹è¯•è®°å½•å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/export/<document_id>', methods=['GET'])
def export_comparison_report(document_id):
    """
    å¯¼å‡ºå¯¹æ¯”æŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼‰

    å“åº”:
        å®Œæ•´çš„JSONæŠ¥å‘Šæ–‡ä»¶
    """
    try:
        db = get_knowledge_base_db()
        row = db.execute_query(
            "SELECT * FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æµ‹è¯•è®°å½•ä¸å­˜åœ¨'}), 404

        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report = {
            'document_id': document_id,
            'filename': row['filename'],
            'upload_time': row['upload_time'],
            'document_info': {
                'total_paragraphs': row['total_paragraphs'],
                'has_toc': bool(row['has_toc']),
                'toc_items_count': row['toc_items_count']
            },
            'results': {
                'toc_exact': json.loads(row['toc_exact_result']) if row.get('toc_exact_result') else None,
                'semantic': json.loads(row['semantic_result']) if row['semantic_result'] else None,
                'style': json.loads(row['style_result']) if row['style_result'] else None,
                'hybrid': json.loads(row['hybrid_result']) if row.get('hybrid_result') else None,
                'azure': json.loads(row['azure_result']) if row.get('azure_result') else None,
                'docx_native': json.loads(row['docx_native_result']) if row.get('docx_native_result') else None,
                'gemini': json.loads(row['gemini_result']) if row.get('gemini_result') else None,
                'llm_level': json.loads(row['llm_level_result']) if row.get('llm_level_result') else None,
            },
            'ground_truth': json.loads(row['ground_truth']) if row['ground_truth'] else None,
            'accuracy': None
        }

        # å¦‚æœæœ‰æ ‡æ³¨ï¼Œæ·»åŠ å‡†ç¡®ç‡æ•°æ®
        if row['ground_truth']:
            report['accuracy'] = {
                'semantic': {
                    'precision': row['semantic_precision'],
                    'recall': row['semantic_recall'],
                    'f1_score': row['semantic_f1']
                },
                'style': {
                    'precision': row['style_precision'],
                    'recall': row['style_recall'],
                    'f1_score': row['style_f1']
                },
                'best_method': row['best_method'],
                'best_f1_score': row['best_f1_score']
            }

            # æ·»åŠ toc_exactç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('toc_exact_precision'):
                report['accuracy']['toc_exact'] = {
                    'precision': row['toc_exact_precision'],
                    'recall': row['toc_exact_recall'],
                    'f1_score': row['toc_exact_f1']
                }

            # æ·»åŠ hybridç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('hybrid_precision'):
                report['accuracy']['hybrid'] = {
                    'precision': row['hybrid_precision'],
                    'recall': row['hybrid_recall'],
                    'f1_score': row['hybrid_f1']
                }

            # æ·»åŠ azureç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('azure_precision'):
                report['accuracy']['azure'] = {
                    'precision': row['azure_precision'],
                    'recall': row['azure_recall'],
                    'f1_score': row['azure_f1']
                }

            # æ·»åŠ docx_nativeç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('docx_native_precision'):
                report['accuracy']['docx_native'] = {
                    'precision': row['docx_native_precision'],
                    'recall': row['docx_native_recall'],
                    'f1_score': row['docx_native_f1']
                }

            # æ·»åŠ geminiç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('gemini_precision'):
                report['accuracy']['gemini'] = {
                    'precision': row['gemini_precision'],
                    'recall': row['gemini_recall'],
                    'f1_score': row['gemini_f1']
                }

            # æ·»åŠ llm_levelç»“æœ(å¦‚æœå­˜åœ¨)
            if row.get('llm_level_precision'):
                report['accuracy']['llm_level'] = {
                    'precision': row['llm_level_precision'],
                    'recall': row['llm_level_recall'],
                    'f1_score': row['llm_level_f1']
                }

        # ä¿å­˜ä¸ºä¸´æ—¶JSONæ–‡ä»¶
        config = get_config()
        temp_dir = config.get_path('data') / 'temp'
        temp_dir.mkdir(parents=True, exist_ok=True)

        report_file = temp_dir / f"parser_comparison_{document_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        return send_file(
            report_file,
            as_attachment=True,
            download_name=f"parser_comparison_{row['filename']}.json",
            mimetype='application/json'
        )

    except Exception as e:
        logger.error(f"å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@api_parser_debug_bp.route('/preview/<document_id>', methods=['GET'])
def preview_document(document_id):
    """
    è·å–æ–‡æ¡£é¢„è§ˆä¿¡æ¯ï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰

    Args:
        document_id: æ–‡æ¡£ID

    Returns:
        æ–‡æ¡£ä¿¡æ¯ï¼ˆåŒ…å«æ–‡ä»¶è·¯å¾„ï¼Œç”¨äº DocumentPreview ç»„ä»¶ï¼‰
    """
    try:
        db = get_knowledge_base_db()

        # è·å–æ–‡ä»¶è·¯å¾„
        row = db.execute_query(
            "SELECT file_path, filename FROM parser_debug_tests WHERE document_id = ?",
            (document_id,),
            fetch_one=True
        )

        if not row:
            return jsonify({'success': False, 'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

        file_path = Path(row['file_path'])
        if not file_path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404

        # è¿”å›æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆä¾› DocumentPreview ç»„ä»¶ä½¿ç”¨ï¼‰
        return jsonify({
            'success': True,
            'file_path': str(file_path),
            'filename': row['filename']
        })

    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£é¢„è§ˆä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# æ³¨å†Œè“å›¾åˆ°åº”ç”¨ï¼ˆéœ€è¦åœ¨app.pyä¸­è°ƒç”¨ï¼‰
def register_parser_debug_bp(app):
    """æ³¨å†Œè§£æè°ƒè¯•è“å›¾"""
    app.register_blueprint(api_parser_debug_bp)
    logger.info("è§£æè°ƒè¯•APIå·²æ³¨å†Œ")
