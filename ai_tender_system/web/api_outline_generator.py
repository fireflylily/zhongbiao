#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€æœ¯æ–¹æ¡ˆå¤§çº²ç”ŸæˆAPI
æä¾›æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆçš„Webæ¥å£
"""

import os
import json
import traceback
from datetime import datetime
from pathlib import Path
from flask import Blueprint, request, jsonify, send_file, Response, stream_with_context
from werkzeug.utils import secure_filename

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent.parent))
from common import get_module_logger, get_config
from modules.outline_generator import (
    RequirementAnalyzer,
    OutlineGenerator,
    ProductMatcher,
    ProposalAssembler,
    WordExporter
)

# åˆ›å»ºè“å›¾
api_outline_bp = Blueprint('api_outline', __name__, url_prefix='/api')

# å…¨å±€å˜é‡
logger = get_module_logger("api_outline_generator")
config = get_config()

# å…è®¸çš„æ–‡ä»¶æ‰©å±•å
ALLOWED_EXTENSIONS = {'doc', 'docx', 'pdf', 'xlsx', 'xls'}


def allowed_file(filename: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦å…è®¸"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


@api_outline_bp.route('/generate-proposal', methods=['POST'])
def generate_proposal():
    """
    ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆAPI

    è¯·æ±‚å‚æ•°ï¼ˆmultipart/form-dataï¼‰:
    - tender_file: æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶
    - product_file: äº§å“æ–‡æ¡£æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    - company_id: å…¬å¸IDï¼ˆå¯é€‰ï¼‰
    - output_prefix: è¾“å‡ºæ–‡ä»¶åå‰ç¼€ï¼ˆå¯é€‰ï¼Œé»˜è®¤"æŠ€æœ¯æ–¹æ¡ˆ"ï¼‰
    - include_analysis: æ˜¯å¦åŒ…å«éœ€æ±‚åˆ†ææŠ¥å‘Šï¼ˆå¯é€‰ï¼Œé»˜è®¤falseï¼‰
    - include_mapping: æ˜¯å¦ç”Ÿæˆéœ€æ±‚åŒ¹é…è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤falseï¼‰
    - include_summary: æ˜¯å¦ç”Ÿæˆç”ŸæˆæŠ¥å‘Šï¼ˆå¯é€‰ï¼Œé»˜è®¤falseï¼‰

    è¿”å›:
    {
        "success": true,
        "requirements_count": 50,
        "sections_count": 5,
        "matches_count": 45,
        "output_files": {
            "proposal": "/downloads/xxx.docx",
            "analysis": "/downloads/xxx.docx",
            "mapping": "/downloads/xxx.xlsx",
            "summary": "/downloads/xxx.txt"
        }
    }
    """
    try:
        logger.info("æ”¶åˆ°æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆè¯·æ±‚")

        # 1. éªŒè¯è¯·æ±‚ - æ”¯æŒä¸¤ç§æ–‡ä»¶æ¥æºï¼šç›´æ¥ä¸Šä¼ æˆ–ä»HITLä¼ é€’
        use_hitl_file = request.form.get('use_hitl_technical_file', 'false').lower() == 'true'
        project_id = request.form.get('hitl_task_id') or request.form.get('project_id')

        if use_hitl_file and project_id:
            # ä½¿ç”¨HITLä¼ é€’çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶
            logger.info(f"ä½¿ç”¨HITLé¡¹ç›®çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶: project_id={project_id}")

            # åœ¨technical_filesç›®å½•ä¸‹æœç´¢é¡¹ç›®ç›®å½•ï¼ˆä¸ä¾èµ–æ—¥æœŸï¼‰
            technical_files_base = config.get_path('data') / 'uploads' / 'technical_files'

            # é€’å½’æŸ¥æ‰¾é¡¹ç›®ç›®å½•
            tender_path = None
            for year_dir in technical_files_base.glob('*'):
                if not year_dir.is_dir():
                    continue
                for month_dir in year_dir.glob('*'):
                    if not month_dir.is_dir():
                        continue
                    project_dir = month_dir / str(project_id)
                    if project_dir.exists():
                        # æŸ¥æ‰¾æŠ€æœ¯éœ€æ±‚æ–‡ä»¶ï¼ˆç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰
                        technical_files = list(project_dir.glob('*.*'))
                        if technical_files:
                            tender_path = technical_files[0]
                            logger.info(f"æ‰¾åˆ°HITLæŠ€æœ¯éœ€æ±‚æ–‡ä»¶: {tender_path.name}, è·¯å¾„: {tender_path}")
                            break
                if tender_path:
                    break

            if not tender_path:
                return jsonify({
                    'success': False,
                    'error': f'æœªæ‰¾åˆ°é¡¹ç›®çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶: project_id={project_id}'
                }), 400
        else:
            # ä½¿ç”¨ä¸Šä¼ çš„æ–‡ä»¶
            if 'tender_file' not in request.files:
                return jsonify({
                    'success': False,
                    'error': 'ç¼ºå°‘æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶'
                }), 400

            tender_file = request.files['tender_file']

            if tender_file.filename == '':
                return jsonify({
                    'success': False,
                    'error': 'æœªé€‰æ‹©æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶'
                }), 400

            if not allowed_file(tender_file.filename):
                return jsonify({
                    'success': False,
                    'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  .doc, .docx, .pdf, .xlsx æˆ– .xls æ–‡ä»¶'
                }), 400

            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            upload_dir = config.get_path('uploads')
            upload_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            tender_filename = secure_filename(f"{timestamp}_{tender_file.filename}")
            tender_path = upload_dir / tender_filename

            tender_file.save(str(tender_path))
            logger.info(f"éœ€æ±‚æ–‡æ¡£å·²ä¿å­˜: {tender_path}")

        # 2. è·å–å‚æ•°
        company_id = request.form.get('companyId')
        project_name = request.form.get('projectName', '')  # è·å–é¡¹ç›®åç§°
        output_prefix = request.form.get('output_prefix', 'æŠ€æœ¯æ–¹æ¡ˆ')

        options = {
            'include_analysis': request.form.get('includeAnalysis', 'false').lower() == 'true',
            'include_mapping': request.form.get('includeMapping', 'false').lower() == 'true',
            'include_summary': request.form.get('includeSummary', 'false').lower() == 'true'
        }

        logger.info(f"ç”Ÿæˆé€‰é¡¹: {options}")

        # 4. é˜¶æ®µ1ï¼šéœ€æ±‚åˆ†æ
        logger.info("å¼€å§‹é˜¶æ®µ1ï¼šéœ€æ±‚åˆ†æ...")
        analyzer = RequirementAnalyzer()
        analysis_result = analyzer.analyze_document(str(tender_path))

        logger.info("éœ€æ±‚åˆ†æå®Œæˆ")

        # 5. é˜¶æ®µ2ï¼šå¤§çº²ç”Ÿæˆ
        logger.info("å¼€å§‹é˜¶æ®µ2ï¼šå¤§çº²ç”Ÿæˆ...")
        outline_gen = OutlineGenerator()
        outline_data = outline_gen.generate_outline(
            analysis_result,
            project_name=output_prefix
        )

        logger.info("å¤§çº²ç”Ÿæˆå®Œæˆ")

        # 6. é˜¶æ®µ3ï¼šäº§å“æ–‡æ¡£åŒ¹é…
        logger.info("å¼€å§‹é˜¶æ®µ3ï¼šäº§å“æ–‡æ¡£åŒ¹é…...")
        matcher = ProductMatcher()
        matched_docs = matcher.match_documents(
            analysis_result.get('requirement_categories', []),
            company_id=int(company_id) if company_id else None
        )

        logger.info(f"äº§å“æ–‡æ¡£åŒ¹é…å®Œæˆï¼Œå…±åŒ¹é… {sum(len(v) for v in matched_docs.values())} ä»½æ–‡æ¡£")

        # 7. é˜¶æ®µ4ï¼šæ–¹æ¡ˆç»„è£…
        logger.info("å¼€å§‹é˜¶æ®µ4ï¼šæ–¹æ¡ˆç»„è£…...")
        assembler = ProposalAssembler()
        proposal = assembler.assemble_proposal(
            outline_data,
            analysis_result,
            matched_docs,
            options
        )

        logger.info("æ–¹æ¡ˆç»„è£…å®Œæˆ")

        # 8. å¯¼å‡ºæ–‡ä»¶
        logger.info("å¼€å§‹å¯¼å‡ºæ–‡ä»¶...")
        exporter = WordExporter()

        output_dir = config.get_path('output')
        output_dir.mkdir(parents=True, exist_ok=True)

        output_files = {}

        # æ™ºèƒ½æ–‡ä»¶å‘½åï¼šæœ‰é¡¹ç›®åç§°æ—¶ä½¿ç”¨"é¡¹ç›®å_ç±»å‹_æ—¶é—´"ï¼Œæ— é¡¹ç›®åç§°æ—¶ä½¿ç”¨"å‰ç¼€_æ—¶é—´"
        if project_name:
            proposal_filename = f"{project_name}_æŠ€æœ¯æ–¹æ¡ˆ_{timestamp}.docx"
            analysis_filename = f"{project_name}_éœ€æ±‚åˆ†æ_{timestamp}.docx"
            mapping_filename = f"{project_name}_éœ€æ±‚åŒ¹é…è¡¨_{timestamp}.xlsx"
            summary_filename = f"{project_name}_ç”ŸæˆæŠ¥å‘Š_{timestamp}.txt"
        else:
            proposal_filename = f"{output_prefix}_{timestamp}.docx"
            analysis_filename = f"{output_prefix}_éœ€æ±‚åˆ†æ_{timestamp}.docx"
            mapping_filename = f"{output_prefix}_éœ€æ±‚åŒ¹é…è¡¨_{timestamp}.xlsx"
            summary_filename = f"{output_prefix}_ç”ŸæˆæŠ¥å‘Š_{timestamp}.txt"

        # å¯¼å‡ºä¸»æ–¹æ¡ˆ
        proposal_path = output_dir / proposal_filename
        exporter.export_proposal(proposal, str(proposal_path))
        output_files['proposal'] = f"/api/downloads/{proposal_filename}"

        logger.info(f"ä¸»æ–¹æ¡ˆå·²å¯¼å‡º: {proposal_path}")

        # å¯¼å‡ºé™„ä»¶
        if options['include_analysis']:
            analysis_path = output_dir / analysis_filename
            exporter.export_analysis_report(analysis_result, str(analysis_path))
            output_files['analysis'] = f"/api/downloads/{analysis_filename}"

            logger.info(f"éœ€æ±‚åˆ†ææŠ¥å‘Šå·²å¯¼å‡º: {analysis_path}")

        if options['include_mapping']:
            mapping_path = output_dir / mapping_filename

            mapping_data = []
            for attachment in proposal.get('attachments', []):
                if attachment['type'] == 'mapping':
                    mapping_data = attachment['data']
                    break

            exporter.export_mapping_table(mapping_data, str(mapping_path))
            output_files['mapping'] = f"/api/downloads/{mapping_filename}"

            logger.info(f"éœ€æ±‚åŒ¹é…è¡¨å·²å¯¼å‡º: {mapping_path}")

        if options['include_summary']:
            summary_path = output_dir / summary_filename

            summary_data = {}
            for attachment in proposal.get('attachments', []):
                if attachment['type'] == 'summary':
                    summary_data = attachment['data']
                    break

            exporter.export_summary_report(summary_data, str(summary_path))
            output_files['summary'] = f"/api/downloads/{summary_filename}"

            logger.info(f"ç”ŸæˆæŠ¥å‘Šå·²å¯¼å‡º: {summary_path}")

        # 9. ç»Ÿè®¡ä¿¡æ¯
        requirements_count = analysis_result.get('document_summary', {}).get('total_requirements', 0)
        sections_count = len(outline_data.get('chapters', []))
        matches_count = sum(len(docs) for docs in matched_docs.values())

        # 10. è¿”å›ç»“æœ
        response = {
            'success': True,
            'requirements_count': requirements_count,
            'features_count': 0,  # æš‚æ—¶ä¸º0
            'sections_count': sections_count,
            'matches_count': matches_count,
            'output_files': output_files
        }

        logger.info("æŠ€æœ¯æ–¹æ¡ˆç”ŸæˆæˆåŠŸ")
        return jsonify(response)

    except Exception as e:
        logger.error(f"æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
        error_trace = traceback.format_exc()

        return jsonify({
            'success': False,
            'error': str(e),
            'trace': error_trace if config.get('debug', False) else None
        }), 500


@api_outline_bp.route('/generate-proposal-stream', methods=['POST'])
def generate_proposal_stream():
    """
    ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆAPIï¼ˆæµå¼SSEç‰ˆæœ¬ï¼‰
    å®æ—¶æ¨é€ç”Ÿæˆè¿›åº¦

    è¯·æ±‚å‚æ•°ï¼ˆmultipart/form-dataï¼‰:
    - tender_file: æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶
    - product_file: äº§å“æ–‡æ¡£æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    - outputPrefix: è¾“å‡ºæ–‡ä»¶åå‰ç¼€
    - companyId: å…¬å¸ID
    - projectName: é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼Œä»HITLä¼ é€’ï¼‰
    - technicalFileTaskId: HITLä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
    - includeAnalysis: æ˜¯å¦åŒ…å«éœ€æ±‚åˆ†æ
    - includeMapping: æ˜¯å¦ç”ŸæˆåŒ¹é…è¡¨
    - includeSummary: æ˜¯å¦ç”Ÿæˆæ€»ç»“æŠ¥å‘Š

    è¿”å›: text/event-stream
    """

    def generate_events():
        """ç”ŸæˆSSEäº‹ä»¶æµ"""
        try:
            # å‘é€åˆå§‹è¿›åº¦
            yield f"data: {json.dumps({'stage': 'init', 'progress': 0, 'message': 'å‡†å¤‡ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ...'}, ensure_ascii=False)}\n\n"

            # 1. å‚æ•°è§£æï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
            yield f"data: {json.dumps({'stage': 'init', 'progress': 5, 'message': 'è§£æè¯·æ±‚å‚æ•°...'}, ensure_ascii=False)}\n\n"

            tender_file = request.files.get('tender_file')
            product_file = request.files.get('product_file')
            output_prefix = request.form.get('outputPrefix', 'æŠ€æœ¯æ–¹æ¡ˆ')
            company_id = request.form.get('companyId')
            project_name = request.form.get('projectName', '')
            project_id = request.form.get('technicalFileTaskId', '') or request.form.get('projectId', '')

            # ç”Ÿæˆé€‰é¡¹
            options = {
                'include_analysis': request.form.get('includeAnalysis', 'false').lower() == 'true',
                'include_mapping': request.form.get('includeMapping', 'false').lower() == 'true',
                'include_summary': request.form.get('includeSummary', 'false').lower() == 'true'
            }

            # 2. è·å–æŠ€æœ¯éœ€æ±‚æ–‡ä»¶è·¯å¾„
            if project_id:
                # ä»HITLé¡¹ç›®åŠ è½½
                yield f"data: {json.dumps({'stage': 'init', 'progress': 10, 'message': f'ä»æŠ•æ ‡é¡¹ç›®åŠ è½½æŠ€æœ¯éœ€æ±‚æ–‡ä»¶ (project_id={project_id})...'}, ensure_ascii=False)}\n\n"

                # æœç´¢HITLæŠ€æœ¯éœ€æ±‚æ–‡ä»¶
                technical_files_base = config.get_path('upload') / 'technical_files'
                tender_path = None

                logger.info(f"æœç´¢HITLé¡¹ç›®æ–‡ä»¶, project_id: {project_id}, æœç´¢è·¯å¾„: {technical_files_base}")

                for year_dir in technical_files_base.glob('*'):
                    if not year_dir.is_dir():
                        continue
                    logger.debug(f"æ£€æŸ¥å¹´ä»½ç›®å½•: {year_dir}")
                    for month_dir in year_dir.glob('*'):
                        if not month_dir.is_dir():
                            continue
                        logger.debug(f"æ£€æŸ¥æœˆä»½ç›®å½•: {month_dir}")
                        project_dir = month_dir / str(project_id)
                        logger.debug(f"æ£€æŸ¥é¡¹ç›®ç›®å½•: {project_dir}, æ˜¯å¦å­˜åœ¨: {project_dir.exists()}")
                        if project_dir.exists():
                            # æŸ¥æ‰¾æŠ€æœ¯éœ€æ±‚æ–‡ä»¶ï¼ˆç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰
                            technical_files = list(project_dir.glob('*.*'))
                            logger.debug(f"æ‰¾åˆ°çš„æ–‡ä»¶: {technical_files}")
                            if technical_files:
                                tender_path = technical_files[0]
                                logger.info(f"æ‰¾åˆ°HITLæŠ€æœ¯éœ€æ±‚æ–‡ä»¶: {tender_path.name}, è·¯å¾„: {tender_path}")
                                break
                    if tender_path:
                        break

                if not tender_path:
                    # æ·»åŠ è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    logger.error(f'æœªæ‰¾åˆ°é¡¹ç›®çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶: project_id={project_id}')
                    logger.error(f'æœç´¢è·¯å¾„: {technical_files_base}')
                    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é¡¹ç›®ç›®å½•
                    available_projects = []
                    for year_dir in technical_files_base.glob('*'):
                        if year_dir.is_dir():
                            for month_dir in year_dir.glob('*'):
                                if month_dir.is_dir():
                                    for proj_dir in month_dir.glob('*'):
                                        if proj_dir.is_dir():
                                            available_projects.append(str(proj_dir))
                    logger.error(f'å¯ç”¨çš„é¡¹ç›®ç›®å½•: {available_projects}')
                    raise ValueError(f'æœªæ‰¾åˆ°é¡¹ç›®çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶: project_id={project_id}')
            elif tender_file:
                # ä¸Šä¼ æ–‡ä»¶
                yield f"data: {json.dumps({'stage': 'init', 'progress': 10, 'message': 'ä¿å­˜ä¸Šä¼ çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶...'}, ensure_ascii=False)}\n\n"
                if not allowed_file(tender_file.filename):
                    raise ValueError('æ–‡ä»¶ç±»å‹ä¸æ”¯æŒ')

                upload_dir = config.get_path('uploads') / 'tender_processing' / datetime.now().strftime('%Y/%m')
                upload_dir.mkdir(parents=True, exist_ok=True)

                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = secure_filename(f"{timestamp}_{tender_file.filename}")
                tender_path = upload_dir / filename
                tender_file.save(str(tender_path))
            else:
                raise ValueError('æœªæä¾›æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶')

            # 3. é˜¶æ®µ1ï¼šéœ€æ±‚åˆ†æ
            yield f"data: {json.dumps({'stage': 'analysis', 'progress': 15, 'message': 'ğŸ” æ­£åœ¨åˆ†ææŠ€æœ¯éœ€æ±‚æ–‡æ¡£...'}, ensure_ascii=False)}\n\n"

            analyzer = RequirementAnalyzer()
            analysis_result = analyzer.analyze_document(str(tender_path))

            yield f"data: {json.dumps({'stage': 'analysis', 'progress': 30, 'message': 'âœ“ éœ€æ±‚åˆ†æå®Œæˆ'}, ensure_ascii=False)}\n\n"

            # å‘é€å®Œæ•´çš„éœ€æ±‚åˆ†æç»“æœä¾›å‰ç«¯å±•ç¤º
            try:
                # ç¡®ä¿analysis_resultå¯ä»¥è¢«JSONåºåˆ—åŒ–
                analysis_result_serializable = json.loads(json.dumps(analysis_result, ensure_ascii=False, default=str))
                yield f"data: {json.dumps({'stage': 'analysis_completed', 'analysis_result': analysis_result_serializable}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logger.warning(f"æ— æ³•åºåˆ—åŒ–éœ€æ±‚åˆ†æç»“æœ: {e}, è·³è¿‡å‰ç«¯å±•ç¤º")
                # ç»§ç»­æ‰§è¡Œ,ä¸å½±å“åç»­æµç¨‹

            # 4. é˜¶æ®µ2ï¼šå¤§çº²ç”Ÿæˆ
            yield f"data: {json.dumps({'stage': 'outline', 'progress': 35, 'message': 'ğŸ“ æ­£åœ¨ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆå¤§çº²...'}, ensure_ascii=False)}\n\n"

            outline_gen = OutlineGenerator()
            outline_data = outline_gen.generate_outline(
                analysis_result,
                project_name=output_prefix
            )

            yield f"data: {json.dumps({'stage': 'outline', 'progress': 55, 'message': 'âœ“ å¤§çº²ç”Ÿæˆå®Œæˆ'}, ensure_ascii=False)}\n\n"

            # å‘é€å®Œæ•´çš„å¤§çº²æ•°æ®ä¾›å‰ç«¯å±•ç¤º
            try:
                # ç¡®ä¿outline_dataå¯ä»¥è¢«JSONåºåˆ—åŒ–
                outline_data_serializable = json.loads(json.dumps(outline_data, ensure_ascii=False, default=str))
                yield f"data: {json.dumps({'stage': 'outline_completed', 'outline_data': outline_data_serializable}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logger.warning(f"æ— æ³•åºåˆ—åŒ–å¤§çº²æ•°æ®: {e}, è·³è¿‡å‰ç«¯å±•ç¤º")
                # ç»§ç»­æ‰§è¡Œ,ä¸å½±å“åç»­æµç¨‹

            # 5. é˜¶æ®µ3ï¼šäº§å“æ–‡æ¡£åŒ¹é…
            yield f"data: {json.dumps({'stage': 'matching', 'progress': 60, 'message': 'ğŸ”— æ­£åœ¨åŒ¹é…äº§å“æ–‡æ¡£...'}, ensure_ascii=False)}\n\n"

            matcher = ProductMatcher()
            matched_docs = matcher.match_documents(
                analysis_result.get('requirement_categories', []),
                company_id=int(company_id) if company_id else None
            )

            matches_count = sum(len(v) for v in matched_docs.values())
            yield f"data: {json.dumps({'stage': 'matching', 'progress': 70, 'message': f'âœ“ æ–‡æ¡£åŒ¹é…å®Œæˆï¼ˆåŒ¹é…åˆ° {matches_count} ä»½æ–‡æ¡£ï¼‰'}, ensure_ascii=False)}\n\n"

            # 6. é˜¶æ®µ4ï¼šæ–¹æ¡ˆç»„è£…
            yield f"data: {json.dumps({'stage': 'assembly', 'progress': 75, 'message': 'âš™ï¸ æ­£åœ¨ç»„è£…æŠ€æœ¯æ–¹æ¡ˆ...'}, ensure_ascii=False)}\n\n"

            assembler = ProposalAssembler()
            proposal = assembler.assemble_proposal(
                outline_data,
                analysis_result,
                matched_docs,
                options
            )

            yield f"data: {json.dumps({'stage': 'assembly', 'progress': 85, 'message': 'âœ“ æ–¹æ¡ˆç»„è£…å®Œæˆ'}, ensure_ascii=False)}\n\n"

            # 7. å¯¼å‡ºæ–‡ä»¶
            yield f"data: {json.dumps({'stage': 'export', 'progress': 90, 'message': 'ğŸ’¾ æ­£åœ¨å¯¼å‡ºæ–‡ä»¶...'}, ensure_ascii=False)}\n\n"

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = config.get_path('output')
            output_dir.mkdir(parents=True, exist_ok=True)

            exporter = WordExporter()
            output_files = {}

            # æ–‡ä»¶å‘½å
            if project_name:
                proposal_filename = f"{project_name}_æŠ€æœ¯æ–¹æ¡ˆ_{timestamp}.docx"
                analysis_filename = f"{project_name}_éœ€æ±‚åˆ†æ_{timestamp}.docx"
                mapping_filename = f"{project_name}_éœ€æ±‚åŒ¹é…è¡¨_{timestamp}.xlsx"
                summary_filename = f"{project_name}_ç”ŸæˆæŠ¥å‘Š_{timestamp}.txt"
            else:
                proposal_filename = f"{output_prefix}_{timestamp}.docx"
                analysis_filename = f"{output_prefix}_éœ€æ±‚åˆ†æ_{timestamp}.docx"
                mapping_filename = f"{output_prefix}_éœ€æ±‚åŒ¹é…è¡¨_{timestamp}.xlsx"
                summary_filename = f"{output_prefix}_ç”ŸæˆæŠ¥å‘Š_{timestamp}.txt"

            # å¯¼å‡ºä¸»æ–¹æ¡ˆ
            proposal_path = output_dir / proposal_filename
            exporter.export_proposal(proposal, str(proposal_path))
            output_files['proposal'] = f"/api/downloads/{proposal_filename}"

            # å¯¼å‡ºé™„ä»¶
            if options['include_analysis']:
                analysis_path = output_dir / analysis_filename
                exporter.export_analysis_report(analysis_result, str(analysis_path))
                output_files['analysis'] = f"/api/downloads/{analysis_filename}"

            if options['include_mapping']:
                mapping_path = output_dir / mapping_filename
                mapping_data = []
                for attachment in proposal.get('attachments', []):
                    if attachment['type'] == 'mapping':
                        mapping_data = attachment['data']
                        break
                exporter.export_mapping_table(mapping_data, str(mapping_path))
                output_files['mapping'] = f"/api/downloads/{mapping_filename}"

            if options['include_summary']:
                summary_path = output_dir / summary_filename
                summary_data = {}
                for attachment in proposal.get('attachments', []):
                    if attachment['type'] == 'summary':
                        summary_data = attachment['data']
                        break
                exporter.export_summary_report(summary_data, str(summary_path))
                output_files['summary'] = f"/api/downloads/{summary_filename}"

            # ç»Ÿè®¡ä¿¡æ¯
            requirements_count = analysis_result.get('document_summary', {}).get('total_requirements', 0)
            sections_count = len(outline_data.get('chapters', []))

            # 8. å®Œæˆ
            result = {
                'stage': 'completed',
                'progress': 100,
                'message': 'âœ… æŠ€æœ¯æ–¹æ¡ˆç”ŸæˆæˆåŠŸï¼',
                'success': True,
                'requirements_count': requirements_count,
                'features_count': 0,
                'sections_count': sections_count,
                'matches_count': matches_count,
                'output_file': str(proposal_path),  # âœ… æ·»åŠ æ–‡ä»¶ç³»ç»Ÿå®Œæ•´è·¯å¾„ï¼Œä¸å•†åŠ¡åº”ç­”/ç‚¹å¯¹ç‚¹åº”ç­”ä¿æŒä¸€è‡´
                'output_files': output_files  # ä¿ç•™ä¸‹è½½URLå­—å…¸
            }

            yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
            logger.info("æŠ€æœ¯æ–¹æ¡ˆç”ŸæˆæˆåŠŸï¼ˆSSEæµå¼ï¼‰")

        except Exception as e:
            logger.error(f"æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆå¤±è´¥ï¼ˆSSEæµå¼ï¼‰: {e}", exc_info=True)
            error_data = {
                'stage': 'error',
                'progress': 0,
                'message': f'ç”Ÿæˆå¤±è´¥: {str(e)}',
                'success': False,
                'error': str(e)
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

    return Response(
        stream_with_context(generate_events()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no'
        }
    )


@api_outline_bp.route('/downloads/<filename>', methods=['GET'])
def download_file(filename):
    """
    ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶

    Args:
        filename: æ–‡ä»¶å

    Returns:
        æ–‡ä»¶å†…å®¹
    """
    try:
        output_dir = config.get_path('output')
        file_path = output_dir / filename

        if not file_path.exists():
            return jsonify({
                'success': False,
                'error': 'æ–‡ä»¶ä¸å­˜åœ¨'
            }), 404

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åè®¾ç½®MIMEç±»å‹
        ext = filename.rsplit('.', 1)[1].lower() if '.' in filename else ''

        mime_types = {
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            'txt': 'text/plain',
            'pdf': 'application/pdf'
        }

        mimetype = mime_types.get(ext, 'application/octet-stream')

        return send_file(
            str(file_path),
            as_attachment=True,
            download_name=filename,
            mimetype=mimetype
        )

    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}", exc_info=True)

        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@api_outline_bp.route('/generate-proposal-stream-v2', methods=['POST'])
def generate_proposal_stream_v2():
    """
    ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆAPIï¼ˆæµå¼SSEç‰ˆæœ¬ V2 - æ”¯æŒå®æ—¶å†…å®¹æ¨é€ï¼‰
    å®æ—¶æ¨é€ç”Ÿæˆè¿›åº¦å’Œç« èŠ‚å†…å®¹

    è¯·æ±‚å‚æ•°ï¼ˆmultipart/form-dataï¼‰:
    - tender_file: æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶
    - outputPrefix: è¾“å‡ºæ–‡ä»¶åå‰ç¼€
    - companyId: å…¬å¸ID
    - projectName: é¡¹ç›®åç§°
    - projectId: é¡¹ç›®IDï¼ˆä»HITLä¼ é€’ï¼‰
    - includeAnalysis: æ˜¯å¦åŒ…å«éœ€æ±‚åˆ†æ
    - includeMapping: æ˜¯å¦ç”ŸæˆåŒ¹é…è¡¨
    - includeSummary: æ˜¯å¦ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    - useStreamingContent: æ˜¯å¦ä½¿ç”¨æµå¼å†…å®¹ç”Ÿæˆï¼ˆé»˜è®¤trueï¼‰

    è¿”å›: text/event-stream
    """

    def generate_events():
        """ç”ŸæˆSSEäº‹ä»¶æµ"""
        try:
            # åˆå§‹åŒ–
            yield f"data: {json.dumps({'stage': 'init', 'progress': 0, 'message': 'å‡†å¤‡ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ...'}, ensure_ascii=False)}\n\n"

            # å‚æ•°è§£æ
            yield f"data: {json.dumps({'stage': 'init', 'progress': 5, 'message': 'è§£æè¯·æ±‚å‚æ•°...'}, ensure_ascii=False)}\n\n"

            tender_file = request.files.get('tender_file')
            output_prefix = request.form.get('outputPrefix', 'æŠ€æœ¯æ–¹æ¡ˆ')
            company_id = request.form.get('companyId')
            project_name = request.form.get('projectName', '')
            project_id = request.form.get('projectId', '')
            use_streaming_content = request.form.get('useStreamingContent', 'true').lower() == 'true'

            # ç”Ÿæˆé€‰é¡¹
            options = {
                'include_analysis': request.form.get('includeAnalysis', 'false').lower() == 'true',
                'include_mapping': request.form.get('includeMapping', 'false').lower() == 'true',
                'include_summary': request.form.get('includeSummary', 'false').lower() == 'true'
            }

            # è·å–æŠ€æœ¯éœ€æ±‚æ–‡ä»¶è·¯å¾„
            if project_id:
                yield f"data: {json.dumps({'stage': 'init', 'progress': 10, 'message': f'ä»æŠ•æ ‡é¡¹ç›®åŠ è½½æŠ€æœ¯éœ€æ±‚æ–‡ä»¶...'}, ensure_ascii=False)}\n\n"

                technical_files_base = config.get_path('upload') / 'technical_files'
                tender_path = None

                for year_dir in technical_files_base.glob('*'):
                    if not year_dir.is_dir():
                        continue
                    for month_dir in year_dir.glob('*'):
                        if not month_dir.is_dir():
                            continue
                        project_dir = month_dir / str(project_id)
                        if project_dir.exists():
                            technical_files = list(project_dir.glob('*.*'))
                            if technical_files:
                                tender_path = technical_files[0]
                                logger.info(f"æ‰¾åˆ°HITLæŠ€æœ¯éœ€æ±‚æ–‡ä»¶: {tender_path}")
                                break
                    if tender_path:
                        break

                if not tender_path:
                    raise ValueError(f'æœªæ‰¾åˆ°é¡¹ç›®çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶: project_id={project_id}')
            elif tender_file:
                yield f"data: {json.dumps({'stage': 'init', 'progress': 10, 'message': 'ä¿å­˜ä¸Šä¼ çš„æŠ€æœ¯éœ€æ±‚æ–‡ä»¶...'}, ensure_ascii=False)}\n\n"

                if not allowed_file(tender_file.filename):
                    raise ValueError('æ–‡ä»¶ç±»å‹ä¸æ”¯æŒ')

                upload_dir = config.get_path('uploads') / 'tender_processing' / datetime.now().strftime('%Y/%m')
                upload_dir.mkdir(parents=True, exist_ok=True)

                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = secure_filename(f"{timestamp}_{tender_file.filename}")
                tender_path = upload_dir / filename
                tender_file.save(str(tender_path))
            else:
                raise ValueError('æœªæä¾›æŠ€æœ¯éœ€æ±‚æ–‡æ¡£æ–‡ä»¶')

            # é˜¶æ®µ1ï¼šéœ€æ±‚åˆ†æ
            yield f"data: {json.dumps({'stage': 'analysis', 'progress': 15, 'message': 'ğŸ” æ­£åœ¨åˆ†ææŠ€æœ¯éœ€æ±‚æ–‡æ¡£...'}, ensure_ascii=False)}\n\n"

            analyzer = RequirementAnalyzer()
            analysis_result = analyzer.analyze_document(str(tender_path))

            yield f"data: {json.dumps({'stage': 'analysis', 'progress': 30, 'message': 'âœ“ éœ€æ±‚åˆ†æå®Œæˆ'}, ensure_ascii=False)}\n\n"

            # é˜¶æ®µ2ï¼šå¤§çº²ç”Ÿæˆ
            yield f"data: {json.dumps({'stage': 'outline', 'progress': 35, 'message': 'ğŸ“ æ­£åœ¨ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆå¤§çº²...'}, ensure_ascii=False)}\n\n"

            outline_gen = OutlineGenerator()
            outline_data = outline_gen.generate_outline(analysis_result, project_name=output_prefix)

            yield f"data: {json.dumps({'stage': 'outline', 'progress': 55, 'message': 'âœ“ å¤§çº²ç”Ÿæˆå®Œæˆ'}, ensure_ascii=False)}\n\n"

            # é˜¶æ®µ3ï¼šäº§å“æ–‡æ¡£åŒ¹é…
            yield f"data: {json.dumps({'stage': 'matching', 'progress': 60, 'message': 'ğŸ”— æ­£åœ¨åŒ¹é…äº§å“æ–‡æ¡£...'}, ensure_ascii=False)}\n\n"

            matcher = ProductMatcher()
            matched_docs = matcher.match_documents(
                analysis_result.get('requirement_categories', []),
                company_id=int(company_id) if company_id else None
            )

            matches_count = sum(len(v) for v in matched_docs.values())
            yield f"data: {json.dumps({'stage': 'matching', 'progress': 70, 'message': f'âœ“ æ–‡æ¡£åŒ¹é…å®Œæˆï¼ˆåŒ¹é…åˆ° {matches_count} ä»½æ–‡æ¡£ï¼‰'}, ensure_ascii=False)}\n\n"

            # é˜¶æ®µ4ï¼šæ–¹æ¡ˆç»„è£…ï¼ˆæµå¼ï¼‰
            yield f"data: {json.dumps({'stage': 'assembly', 'progress': 75, 'message': 'âš™ï¸ æ­£åœ¨ç»„è£…æŠ€æœ¯æ–¹æ¡ˆ...'}, ensure_ascii=False)}\n\n"

            assembler = ProposalAssembler()

            # é€‰æ‹©æµå¼æˆ–éæµå¼ç»„è£…
            if use_streaming_content:
                logger.info("ä½¿ç”¨æµå¼å†…å®¹ç”Ÿæˆæ¨¡å¼")
                proposal = None

                for event in assembler.assemble_proposal_stream(outline_data, analysis_result, matched_docs, options):
                    event_type = event.get('type')

                    if event_type == 'chapter_start':
                        # æ¨é€ç« èŠ‚å¼€å§‹
                        chapter_start_data = {
                            'stage': 'content_generation',
                            'event': 'chapter_start',
                            'chapter_number': event.get('chapter_number', ''),
                            'chapter_title': event.get('chapter_title', ''),
                            'message': f"ğŸ“„ å¼€å§‹ç”Ÿæˆ {event.get('chapter_number', '')} {event.get('chapter_title', '')}..."
                        }
                        yield f"data: {json.dumps(chapter_start_data, ensure_ascii=False)}\n\n"

                    elif event_type == 'content_chunk':
                        # æ¨é€å†…å®¹ç‰‡æ®µ
                        content_chunk_data = {
                            'stage': 'content_generation',
                            'event': 'content_chunk',
                            'chapter_number': event.get('chapter_number', ''),
                            'content': event.get('chunk', '')
                        }
                        yield f"data: {json.dumps(content_chunk_data, ensure_ascii=False)}\n\n"

                    elif event_type == 'chapter_end':
                        # æ¨é€ç« èŠ‚å®Œæˆ
                        chapter_end_data = {
                            'stage': 'content_generation',
                            'event': 'chapter_end',
                            'chapter_number': event.get('chapter_number', ''),
                            'message': f"âœ“ {event.get('chapter_title', '')} ç”Ÿæˆå®Œæˆ"
                        }
                        yield f"data: {json.dumps(chapter_end_data, ensure_ascii=False)}\n\n"

                    elif event_type == 'completed':
                        # ä¿å­˜æ–¹æ¡ˆæ•°æ®
                        proposal = event['proposal']

                    elif event_type == 'error':
                        raise Exception(event['error'])

                if not proposal:
                    raise ValueError("æµå¼ç»„è£…æœªè¿”å›å®Œæ•´æ–¹æ¡ˆ")

                yield f"data: {json.dumps({'stage': 'assembly', 'progress': 85, 'message': 'âœ“ æ–¹æ¡ˆç»„è£…å®Œæˆï¼ˆæµå¼ï¼‰'}, ensure_ascii=False)}\n\n"
            else:
                # ä½¿ç”¨éæµå¼ç»„è£…ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                proposal = assembler.assemble_proposal(outline_data, analysis_result, matched_docs, options)
                yield f"data: {json.dumps({'stage': 'assembly', 'progress': 85, 'message': 'âœ“ æ–¹æ¡ˆç»„è£…å®Œæˆ'}, ensure_ascii=False)}\n\n"

            # å¯¼å‡ºæ–‡ä»¶
            yield f"data: {json.dumps({'stage': 'export', 'progress': 90, 'message': 'ğŸ’¾ æ­£åœ¨å¯¼å‡ºæ–‡ä»¶...'}, ensure_ascii=False)}\n\n"

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = config.get_path('output')
            output_dir.mkdir(parents=True, exist_ok=True)

            exporter = WordExporter()
            output_files = {}

            # æ–‡ä»¶å‘½å
            if project_name:
                proposal_filename = f"{project_name}_æŠ€æœ¯æ–¹æ¡ˆ_{timestamp}.docx"
                analysis_filename = f"{project_name}_éœ€æ±‚åˆ†æ_{timestamp}.docx"
                mapping_filename = f"{project_name}_éœ€æ±‚åŒ¹é…è¡¨_{timestamp}.xlsx"
                summary_filename = f"{project_name}_ç”ŸæˆæŠ¥å‘Š_{timestamp}.txt"
            else:
                proposal_filename = f"{output_prefix}_{timestamp}.docx"
                analysis_filename = f"{output_prefix}_éœ€æ±‚åˆ†æ_{timestamp}.docx"
                mapping_filename = f"{output_prefix}_éœ€æ±‚åŒ¹é…è¡¨_{timestamp}.xlsx"
                summary_filename = f"{output_prefix}_ç”ŸæˆæŠ¥å‘Š_{timestamp}.txt"

            # å¯¼å‡ºä¸»æ–¹æ¡ˆ
            proposal_path = output_dir / proposal_filename
            exporter.export_proposal(proposal, str(proposal_path))
            output_files['proposal'] = f"/api/downloads/{proposal_filename}"

            # å¯¼å‡ºé™„ä»¶
            if options['include_analysis']:
                analysis_path = output_dir / analysis_filename
                exporter.export_analysis_report(analysis_result, str(analysis_path))
                output_files['analysis'] = f"/api/downloads/{analysis_filename}"

            if options['include_mapping']:
                mapping_path = output_dir / mapping_filename
                mapping_data = []
                for attachment in proposal.get('attachments', []):
                    if attachment['type'] == 'mapping':
                        mapping_data = attachment['data']
                        break
                exporter.export_mapping_table(mapping_data, str(mapping_path))
                output_files['mapping'] = f"/api/downloads/{mapping_filename}"

            if options['include_summary']:
                summary_path = output_dir / summary_filename
                summary_data = {}
                for attachment in proposal.get('attachments', []):
                    if attachment['type'] == 'summary':
                        summary_data = attachment['data']
                        break
                exporter.export_summary_report(summary_data, str(summary_path))
                output_files['summary'] = f"/api/downloads/{summary_filename}"

            # ç»Ÿè®¡ä¿¡æ¯
            requirements_count = analysis_result.get('document_summary', {}).get('total_requirements', 0)
            sections_count = len(outline_data.get('chapters', []))

            # å®Œæˆ
            result = {
                'stage': 'completed',
                'progress': 100,
                'message': 'âœ… æŠ€æœ¯æ–¹æ¡ˆç”ŸæˆæˆåŠŸï¼',
                'success': True,
                'requirements_count': requirements_count,
                'features_count': 0,
                'sections_count': sections_count,
                'matches_count': matches_count,
                'output_file': str(proposal_path),
                'output_files': output_files,
                'streaming_mode': use_streaming_content
            }

            yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
            logger.info("æŠ€æœ¯æ–¹æ¡ˆç”ŸæˆæˆåŠŸï¼ˆSSEæµå¼V2ï¼‰")

        except Exception as e:
            logger.error(f"æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆå¤±è´¥ï¼ˆSSEæµå¼V2ï¼‰: {e}", exc_info=True)
            error_data = {
                'stage': 'error',
                'progress': 0,
                'message': f'ç”Ÿæˆå¤±è´¥: {str(e)}',
                'success': False,
                'error': str(e)
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

    return Response(
        stream_with_context(generate_events()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no'
        }
    )


# å¯¼å‡ºè“å›¾
__all__ = ['api_outline_bp']
