#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æ¡£å¡«å†™å™¨ - SmartDocumentFiller
åŸºäºè§„åˆ™å¼•æ“å’Œæ¨¡å¼è¯†åˆ«çš„é€šç”¨æ–‡æ¡£å¡«å†™è§£å†³æ–¹æ¡ˆ

ç‰¹æ€§ï¼š
1. è‡ªåŠ¨è¯†åˆ«5ç§å¡«ç©ºæ¨¡å¼
2. æ™ºèƒ½å­—æ®µæ˜ å°„å’Œå˜ä½“è¯†åˆ«
3. ä¿æŒåŸæ–‡æ¡£æ ¼å¼
4. è¯¦ç»†çš„åŒ¹é…å’Œå¡«å……æ—¥å¿—
5. æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„è®¾è®¡

ä½œè€…ï¼šAI Tender System
ç‰ˆæœ¬ï¼š2.0
æ—¥æœŸï¼š2025-10-19
"""

import re
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from docx import Document
from docx.text.paragraph import Paragraph

# å¯¼å…¥å…¬å…±æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from common import get_module_logger

# å¯¼å…¥Wordæ–‡æ¡£å·¥å…·
from .utils import WordDocumentUtils


class SmartDocumentFiller:
    """æ™ºèƒ½æ–‡æ¡£å¡«å†™å™¨ä¸»ç±»"""

    def __init__(self):
        self.logger = get_module_logger("smart_filler")

        # åˆå§‹åŒ–å­æ¨¡å—
        self.pattern_matcher = PatternMatcher()
        self.field_recognizer = FieldRecognizer()
        self.content_filler = ContentFiller(self.logger)

        self.logger.info("æ™ºèƒ½æ–‡æ¡£å¡«å†™å™¨åˆå§‹åŒ–å®Œæˆ")

    def fill_document(self,
                     doc: Document,
                     data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¡«å†™æ–‡æ¡£ä¸»æ–¹æ³•

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            data: æ•°æ®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦å¡«å……çš„ä¿¡æ¯

        Returns:
            å¡«å……ç»Ÿè®¡ä¿¡æ¯
        """
        # æ ‡å‡†åŒ–æ•°æ®é”®åï¼ˆå…¼å®¹æ—§æ•°æ®æ ¼å¼ï¼‰
        data = self._normalize_data_keys(data)

        stats = {
            'total_filled': 0,
            'pattern_counts': {},
            'unfilled_fields': [],
            'errors': []
        }

        self.logger.info("="*60)
        self.logger.info("å¼€å§‹æ™ºèƒ½æ–‡æ¡£å¡«å……")
        self.logger.info(f"å¯ç”¨æ•°æ®å­—æ®µ: {list(data.keys())}")

        # âœ… ä¸“é—¨æ£€æµ‹ purchaserName å­—æ®µ
        if 'purchaserName' in data:
            self.logger.info(f"âœ… æ£€æµ‹åˆ°purchaserNameå­—æ®µ: {data.get('purchaserName')}")
        else:
            self.logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°purchaserNameå­—æ®µ")

        # âœ… ä¸“é—¨æ£€æµ‹ companyName å­—æ®µï¼ˆå“åº”äººåç§°æ˜ å°„åˆ°æ­¤å­—æ®µï¼‰
        if 'companyName' in data:
            self.logger.info(f"âœ… æ£€æµ‹åˆ°companyNameå­—æ®µ: {data.get('companyName')}")
            self.logger.info(f"   â†’ æ­¤å­—æ®µåº”å¡«å……åˆ°: å“åº”äººåç§°/åº”ç­”äººåç§°/ä¾›åº”å•†åç§°ç­‰")
        else:
            self.logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°companyNameå­—æ®µ")

        self.logger.info("="*60)

        # å¤„ç†æ‰€æœ‰æ®µè½
        for para_idx, paragraph in enumerate(doc.paragraphs):
            if not paragraph.text.strip():
                continue

            # åŒ¹é…å¹¶å¡«å……
            result = self._process_paragraph(paragraph, data, para_idx)

            # æ›´æ–°ç»Ÿè®¡
            if result['filled']:
                stats['total_filled'] += 1
                pattern = result['pattern']
                stats['pattern_counts'][pattern] = stats['pattern_counts'].get(pattern, 0) + 1
            elif result['unfilled_fields']:
                stats['unfilled_fields'].extend(result['unfilled_fields'])

            if result['errors']:
                stats['errors'].extend(result['errors'])

        # å¤„ç†æ‰€æœ‰è¡¨æ ¼
        self.logger.info("å¼€å§‹å¤„ç†è¡¨æ ¼...")
        table_para_idx = len(doc.paragraphs)  # è¡¨æ ¼æ®µè½ä»è¿™ä¸ªç´¢å¼•å¼€å§‹ç¼–å·
        for table_idx, table in enumerate(doc.tables):
            self.logger.debug(f"å¤„ç†è¡¨æ ¼#{table_idx}")
            for row_idx, row in enumerate(table.rows):
                for cell_idx, cell in enumerate(row.cells):
                    for cell_para_idx, paragraph in enumerate(cell.paragraphs):
                        if not paragraph.text.strip():
                            continue

                        # åŒ¹é…å¹¶å¡«å……ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ®µè½å¤„ç†é€»è¾‘ï¼‰
                        para_id = f"table{table_idx}_row{row_idx}_cell{cell_idx}_para{cell_para_idx}"
                        result = self._process_paragraph(paragraph, data, table_para_idx)
                        table_para_idx += 1

                        # æ›´æ–°ç»Ÿè®¡
                        if result['filled']:
                            stats['total_filled'] += 1
                            pattern = result['pattern']
                            stats['pattern_counts'][pattern] = stats['pattern_counts'].get(pattern, 0) + 1
                            self.logger.debug(f"  è¡¨æ ¼å¡«å……æˆåŠŸ: {para_id}")
                        elif result['unfilled_fields']:
                            stats['unfilled_fields'].extend(result['unfilled_fields'])

                        if result['errors']:
                            stats['errors'].extend(result['errors'])

        # è¿‡æ»¤æœªå¡«å……å­—æ®µï¼Œæ’é™¤è¯¯è¯†åˆ«å†…å®¹
        if stats['unfilled_fields']:
            filtered_unfilled = self._filter_invalid_fields(stats['unfilled_fields'])
            stats['filtered_unfilled_fields'] = filtered_unfilled  # æ·»åŠ è¿‡æ»¤åçš„å­—æ®µ
            stats['original_unfilled_count'] = len(stats['unfilled_fields'])  # ä¿ç•™åŸå§‹æ•°é‡ç”¨äºè°ƒè¯•
        else:
            stats['filtered_unfilled_fields'] = []
            stats['original_unfilled_count'] = 0

        # è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
        self._print_stats(stats)

        # åå¤„ç†ï¼šæ¸…ç†å¤šä½™å ä½ç¬¦
        # self._post_process(doc)

        return stats

    def _process_paragraph(self,
                          paragraph: Paragraph,
                          data: Dict[str, Any],
                          para_idx: int) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ®µè½

        Returns:
            {
                'filled': bool,
                'pattern': str,
                'unfilled_fields': list,
                'errors': list
            }
        """
        text = paragraph.text.strip()
        result = {
            'filled': False,
            'pattern': None,
            'unfilled_fields': [],
            'errors': []
        }

        self.logger.debug(f"æ®µè½#{para_idx}: {text[:80]}")

        # 2. æŒ‰ä¼˜å…ˆçº§å°è¯•å¡«å……
        # æ³¨æ„ï¼šæ¯æ¬¡å¡«å……åé‡æ–°æ£€æµ‹æ¨¡å¼ï¼Œå› ä¸ºæ–‡æœ¬å·²æ”¹å˜ï¼Œä½ç½®ä¿¡æ¯ä¼šå¤±æ•ˆ
        filled_patterns = []

        for pattern_type in ['combo', 'bracket', 'date', 'colon', 'space_fill']:
            try:
                # æ¯æ¬¡éƒ½é‡æ–°æ£€æµ‹å½“å‰æ–‡æœ¬çš„æ¨¡å¼
                # æ³¨æ„ï¼šä¸è¦stripï¼Œå› ä¸ºä½ç½®ä¿¡æ¯å¿…é¡»ä¸paragraph.textä¸€è‡´
                text = paragraph.text
                patterns = self.pattern_matcher.detect_patterns(text)

                if pattern_type not in patterns or not patterns[pattern_type]:
                    continue

                self.logger.debug(f"  å°è¯• {pattern_type} æ¨¡å¼ï¼Œæ£€æµ‹åˆ° {len(patterns[pattern_type])} ä¸ªåŒ¹é…")

                # dateæ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼šé¿å…ä¸colonå†²çª
                if pattern_type == 'colon' and 'date' in filled_patterns:
                    # å¦‚æœdateå·²ç»å¡«å……ï¼Œè·³è¿‡colonä¸­çš„æ—¥æœŸå­—æ®µ
                    colon_has_date = any('æ—¥æœŸ' in m['field'] for m in patterns['colon'])
                    if colon_has_date:
                        self.logger.debug("  è·³è¿‡colonæ¨¡å¼ä¸­çš„æ—¥æœŸå­—æ®µï¼ˆå·²ç”±dateæ¨¡å¼å¤„ç†ï¼‰")
                        patterns['colon'] = [m for m in patterns['colon'] if 'æ—¥æœŸ' not in m['field']]
                        if not patterns['colon']:
                            continue

                filled = self._fill_by_pattern(
                    paragraph,
                    text,
                    pattern_type,
                    patterns[pattern_type],
                    data
                )

                if filled:
                    filled_patterns.append(pattern_type)
                    self.logger.info(f"  âœ… ä½¿ç”¨ {pattern_type} æ¨¡å¼æˆåŠŸå¡«å……")

            except Exception as e:
                error_msg = f"æ®µè½#{para_idx} å¡«å……å¤±è´¥({pattern_type}): {e}"
                self.logger.error(error_msg)
                result['errors'].append(error_msg)

        # å¦‚æœæœ‰ä»»ä½•æ¨¡å¼å¡«å……æˆåŠŸ
        if filled_patterns:
            result['filled'] = True
            result['pattern'] = '+'.join(filled_patterns)  # è®°å½•ä½¿ç”¨äº†å“ªäº›æ¨¡å¼
            return result

        # 3. è®°å½•æœªå¡«å……çš„å­—æ®µ
        if patterns:
            for pattern_type, matches in patterns.items():
                for match in matches:
                    result['unfilled_fields'].append({
                        'para_idx': para_idx,
                        'text': text[:100],
                        'pattern': pattern_type,
                        'field': match
                    })

        return result

    def _fill_by_pattern(self,
                        paragraph: Paragraph,
                        text: str,
                        pattern_type: str,
                        matches: List,
                        data: Dict[str, Any]) -> bool:
        """æ ¹æ®æ¨¡å¼ç±»å‹å¡«å……"""

        if pattern_type == 'combo':
            return self.content_filler.fill_combo_field(paragraph, text, matches, data)

        elif pattern_type == 'bracket':
            return self.content_filler.fill_bracket_field(paragraph, text, matches, data)

        elif pattern_type == 'colon':
            return self.content_filler.fill_colon_field(paragraph, text, matches, data)

        elif pattern_type == 'space_fill':
            return self.content_filler.fill_space_field(paragraph, text, matches, data)

        elif pattern_type == 'date':
            return self.content_filler.fill_date_field(paragraph, text, matches, data)

        return False

    def _normalize_data_keys(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ‡å‡†åŒ–æ•°æ®é”®åï¼Œå…¼å®¹æ—§æ ¼å¼

        å°†æ—§çš„å­—æ®µåæ˜ å°„åˆ°æ–°çš„æ ‡å‡†å­—æ®µå:
        - fixedPhone -> phone
        - registeredAddress -> address
        - officeAddress -> address
        ç­‰ç­‰
        """
        normalized = data.copy()

        # å­—æ®µåæ˜ å°„è¡¨ï¼ˆæ—§å -> æ–°åï¼‰
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼šå…ˆæ£€æŸ¥é«˜ä¼˜å…ˆçº§çš„æº
        key_mappings = {
            'fixedPhone': 'phone',
            'mobilePhone': 'mobile',
            'contactPhone': 'phone',
        }

        # å¤šæºæ˜ å°„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        multi_source_mappings = {
            'address': ['address', 'officeAddress', 'registeredAddress'],  # ä¼˜å…ˆä½¿ç”¨officeAddress
            'phone': ['phone', 'fixedPhone', 'mobilePhone'],
        }

        # åº”ç”¨å•é”®æ˜ å°„
        for old_key, new_key in key_mappings.items():
            if old_key in normalized and new_key not in normalized:
                normalized[new_key] = normalized[old_key]
                self.logger.debug(f"é”®åæ˜ å°„: {old_key} -> {new_key}")

        # åº”ç”¨å¤šæºæ˜ å°„ï¼ˆæ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰å€¼çš„æºï¼‰
        for target_key, source_keys in multi_source_mappings.items():
            if target_key not in normalized or not normalized.get(target_key):
                for source_key in source_keys:
                    if source_key in normalized and normalized.get(source_key):
                        normalized[target_key] = normalized[source_key]
                        self.logger.debug(f"å¤šæºæ˜ å°„: {source_key} -> {target_key} (å€¼: {normalized[target_key]})")
                        break

        return normalized

    def _print_stats(self, stats: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡æŠ¥å‘Š"""
        self.logger.info("="*60)
        self.logger.info("å¡«å……ç»Ÿè®¡æŠ¥å‘Š")
        self.logger.info("="*60)
        self.logger.info(f"æ€»å¡«å……æ•°: {stats['total_filled']}")

        if stats['pattern_counts']:
            self.logger.info("æŒ‰æ¨¡å¼ç»Ÿè®¡:")
            for pattern, count in stats['pattern_counts'].items():
                self.logger.info(f"  - {pattern}: {count}")

        # è¿‡æ»¤æœªå¡«å……å­—æ®µï¼šæ’é™¤æ˜æ˜¾çš„è¯¯è¯†åˆ«å†…å®¹
        if stats['unfilled_fields']:
            filtered_unfilled = self._filter_invalid_fields(stats['unfilled_fields'])

            if filtered_unfilled:
                self.logger.warning(f"çœŸæ­£æœªå¡«å……å­—æ®µæ•°: {len(filtered_unfilled)}")
                for uf in filtered_unfilled[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    self.logger.warning(f"  - æ®µè½#{uf['para_idx']}: {uf['field']}")

            # æ˜¾ç¤ºè¿‡æ»¤æ‰çš„è¯¯è¯†åˆ«æ•°é‡
            filtered_count = len(stats['unfilled_fields']) - len(filtered_unfilled)
            if filtered_count > 0:
                self.logger.debug(f"è¿‡æ»¤æ‰çš„è¯¯è¯†åˆ«å­—æ®µæ•°: {filtered_count}")

        if stats['errors']:
            self.logger.error(f"é”™è¯¯æ•°: {len(stats['errors'])}")
            for err in stats['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                self.logger.error(f"  - {err}")

    def _filter_invalid_fields(self, unfilled_fields: List[Dict]) -> List[Dict]:
        """
        è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ•°æ®å­—æ®µçš„è¯¯è¯†åˆ«å†…å®¹

        Args:
            unfilled_fields: åŸå§‹æœªå¡«å……å­—æ®µåˆ—è¡¨

        Returns:
            è¿‡æ»¤åçš„åˆ—è¡¨ï¼ˆä»…åŒ…å«çœŸæ­£çš„æ•°æ®å­—æ®µï¼‰
        """
        # æ’é™¤å…³é”®è¯ï¼šè¯´æ˜æ€§æ–‡å­—ã€æ ‡é¢˜ã€æç¤º
        exclude_keywords = [
            # è¯´æ˜æ€§æ–‡å­—
            'è¡Œè´¿çŠ¯ç½ªè®°å½•', 'æ—¶é—´ä»', 'ä»¥ç›¸å…³', 'ä»¥ä¸­å›½', 'æœ¬æ¡', 'å³å¢å€¼ç¨',
            'æˆ‘å…¬å¸', 'æˆ‘æ–¹', 'æœ¬æŠ•æ ‡', 'æœ‰å…³çš„ä¸€åˆ‡', 'è¯·å¯„', 'é€šè®¯',
            # æç¤ºæ€§æ–‡å­—
            'ç›–ç« ', 'ç­¾å­—', 'å…¬ç« ', 'ç­¾å',
            # æ ¼å¼å’Œæ ‡é¢˜
            'æ ¼å¼', 'é™„ä»¶', 'æ­£æœ¬', 'å‰¯æœ¬', 'ç”µå­ç‰ˆ',
            # æ¡æ¬¾æ ‡é¢˜
            'æƒ…å†µ', 'æ‰¿è¯º', 'å£°æ˜', 'è¯´æ˜', 'æ³¨æ„', 'å¤‡æ³¨',
            # ç½‘å€å’Œç‰¹æ®Šå­—ç¬¦
            'http', 'www.', '://','æ»¡è¶³æ‹›æ ‡','è§é™„ä»¶',
            # æ–‡æ¡£ææ–™ç±»å…³é”®è¯ï¼ˆæ–‡æ¡£æ¸…å•é¡¹ï¼‰
            'èº«ä»½è¯', 'å¤å°ä»¶', 'è¯æ˜', 'åŸä»¶', 'æ‰«æä»¶', 'ææ–™', 'æ–‡ä»¶'
        ]

        # è¿‡æ»¤é€»è¾‘
        filtered = []
        for field_info in unfilled_fields:
            field = field_info.get('field', '')

            # å¦‚æœæ˜¯å­—å…¸ï¼ˆbracketã€colonæ¨¡å¼ï¼‰ï¼Œå–fieldå­—æ®µ
            if isinstance(field, dict):
                field_text = field.get('field', '')
            else:
                field_text = str(field)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯
            should_exclude = any(keyword in field_text for keyword in exclude_keywords)

            # æ£€æŸ¥é•¿åº¦ï¼ˆå­—æ®µåé€šå¸¸ä¸è¶…è¿‡15ä¸ªå­—ç¬¦ï¼‰
            if not should_exclude and len(field_text) > 15:
                should_exclude = True

            if not should_exclude:
                filtered.append(field_info)

        return filtered

    def _post_process(self, doc: Document):
        """
        åå¤„ç†ï¼šæ¸…ç†å¤šä½™çš„å ä½ç¬¦å’Œæ ¼å¼

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
        """
        self.logger.info("å¼€å§‹åå¤„ç†ï¼šæ¸…ç†å¤šä½™å ä½ç¬¦")
        cleaned_count = 0

        for paragraph in doc.paragraphs:
            text = paragraph.text

            # æ¸…ç†å¤šä½™çš„ä¸‹åˆ’çº¿
            text = re.sub(r'_{3,}', '', text)

            # æ¸…ç†å¤šä½™çš„ç©ºæ ¼ï¼ˆä¿ç•™è¡¨æ ¼å¯¹é½æ‰€éœ€çš„ç©ºæ ¼ï¼‰
            # æ”¹è¿›ï¼šåªåœ¨çœŸæ­£çš„è¡¨æ ¼å¸ƒå±€æ—¶è·³è¿‡æ¸…ç†
            is_table_layout = False
            if re.search(r'\s{8,}', text):
                # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„è¡¨æ ¼å¸ƒå±€ï¼š
                # 1. æ•´è¡Œéƒ½æ˜¯ç©ºæ ¼ï¼ˆçº¯å¯¹é½è¡Œï¼‰
                if text.strip() == '':
                    is_table_layout = True
                # 2. æœ‰å¤šç»„8+ç©ºæ ¼åˆ†éš”ï¼ˆè¡¨æ ¼åˆ—åˆ†éš”ï¼‰
                elif len(re.findall(r'\s{8,}', text)) >= 2:
                    is_table_layout = True
                # 3. å¦‚æœåŒ…å«å®é™…å†…å®¹ï¼ˆæ±‰å­—ã€æ‹¬å·ã€æ ‡ç‚¹ï¼‰ï¼Œå³ä½¿æœ‰é•¿ç©ºæ ¼ä¹Ÿåº”æ¸…ç†
                elif re.search(r'[\u4e00-\u9fa5ï¼ˆï¼‰()ï¼š:ï¼Œã€‚ã€]', text):
                    is_table_layout = False
                else:
                    is_table_layout = True

            if not is_table_layout:
                text = re.sub(r'\s{3,}', '  ', text)

            # æ¸…ç†å¤šä½™çš„å†’å·
            text = re.sub(r'[:ï¼š]{2,}', 'ï¼š', text)

            # æ ‡å‡†åŒ–å†’å·
            text = re.sub(r':', 'ï¼š', text)

            # å»é™¤å¤šä½™çš„å¹´æœˆæ—¥æ ‡è¯†
            text = re.sub(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)\s*å¹´\s*æœˆ\s*æ—¥', r'\1', text)

            if text != paragraph.text:
                self._update_paragraph_text(paragraph, text.strip())
                cleaned_count += 1

        self.logger.info(f"åå¤„ç†å®Œæˆï¼Œæ¸…ç†äº† {cleaned_count} ä¸ªæ®µè½")

    def _update_paragraph_text(self, paragraph: Paragraph, new_text: str):
        """
        æ›´æ–°æ®µè½æ–‡æœ¬ï¼Œä¿æŒåŸæœ‰æ ¼å¼

        Args:
            paragraph: æ®µè½å¯¹è±¡
            new_text: æ–°æ–‡æœ¬å†…å®¹
        """
        # ä¿å­˜ç¬¬ä¸€ä¸ªrunçš„æ ¼å¼
        if paragraph.runs:
            first_run = paragraph.runs[0]
            # ä¿å­˜æ ¼å¼å±æ€§
            font = first_run.font
            bold = font.bold
            italic = font.italic
            underline = font.underline
            font_size = font.size
            font_name = font.name

            # æ¸…ç©ºæ®µè½
            paragraph.clear()

            # æ·»åŠ æ–°æ–‡æœ¬å¹¶æ¢å¤æ ¼å¼
            new_run = paragraph.add_run(new_text)
            if bold is not None:
                new_run.font.bold = bold
            if italic is not None:
                new_run.font.italic = italic
            if underline is not None:
                new_run.font.underline = underline
            if font_size:
                new_run.font.size = font_size
            if font_name:
                new_run.font.name = font_name
        else:
            # å¦‚æœæ²¡æœ‰runsï¼Œç›´æ¥è®¾ç½®æ–‡æœ¬
            paragraph.text = new_text


class FieldClassifier:
    """å­—æ®µåˆ†ç±»å™¨ - æ ¹æ®å­—æ®µç±»å‹å†³å®šå¤„ç†ç­–ç•¥

    æ ¸å¿ƒè§„åˆ™ï¼š
    1. å•ä½ç›–ç« å¡«åç§° - å•ä½/å…¬å¸å­—æ®µå³ä½¿æœ‰ç›–ç« æ ‡è®°ä¹Ÿè¦å¡«å……
    2. ä¸ªäººç­¾å­—ç•™ç©ºç™½ - ä¸ªäººå­—æ®µæœ‰ç­¾å­—/ç›–ç« æ ‡è®°åˆ™ä¸å¡«å……
    """

    # å•ä½/å…¬å¸ç›¸å…³å­—æ®µï¼ˆå¯èƒ½å‡ºç°ç›–ç« ï¼‰
    UNIT_FIELDS = {
        'companyName',      # ä¾›åº”å•†åç§°ã€å…¬å¸åç§°
        'supplierName',     # ä¾›åº”å•†
        'vendorName',       # æŠ•æ ‡äºº
        'purchaserName',    # é‡‡è´­äººï¼ˆå¯èƒ½éœ€è¦ç›–ç« ï¼‰
    }

    # ä¸ªäººç›¸å…³å­—æ®µï¼ˆå¯èƒ½å‡ºç°ç­¾å­—/ç›–ç« ï¼‰
    PERSON_FIELDS = {
        'legalRepresentative',      # æ³•å®šä»£è¡¨äºº
        'representativeName',       # æˆæƒä»£è¡¨äººã€è¢«æˆæƒäºº
        'authorizedPerson',         # è¢«æˆæƒäºº
        'representativeTitle',      # èŒåŠ¡ï¼ˆä¸ä¸ªäººç›¸å…³ï¼‰
        'authorizedPersonId',       # èº«ä»½è¯å·ï¼ˆä¸ä¸ªäººç›¸å…³ï¼‰
    }

    # æ ¼å¼æ ‡è®°å®šä¹‰
    SEAL_MARKERS = [
        'ï¼ˆç›–ç« ï¼‰', 'ï¼ˆå…¬ç« ï¼‰', 'ï¼ˆç›–å…¬ç« ï¼‰', '(ç›–ç« )', '(å…¬ç« )', '(ç›–å…¬ç« )',
        'ï¼ˆç›–å•ä½ç« ï¼‰', 'ï¼ˆç›–ä¼ä¸šç« ï¼‰', 'ï¼ˆåŠ ç›–å…¬ç« ï¼‰', 'ï¼ˆåŠ ç›–å•ä½å…¬ç« ï¼‰',  # æ–°å¢ï¼šå•ä½ç›–ç« å˜ä½“
        '(ç›–å•ä½ç« )', '(ç›–ä¼ä¸šç« )', '(åŠ ç›–å…¬ç« )', '(åŠ ç›–å•ä½å…¬ç« )'  # åŠè§’ç‰ˆæœ¬
    ]
    SIGNATURE_MARKERS = ['ï¼ˆç­¾å­—ï¼‰', 'ï¼ˆç­¾åï¼‰', '(ç­¾å­—)', '(ç­¾å)', 'ï¼ˆç­¾ç« ï¼‰', '(ç­¾ç« )']
    COMBO_MARKERS = ['ï¼ˆç­¾å­—æˆ–ç›–ç« ï¼‰', 'ï¼ˆç­¾å­—åŠç›–ç« ï¼‰', 'ï¼ˆç­¾å­—å¹¶ç›–ç« ï¼‰',
                     '(ç­¾å­—æˆ–ç›–ç« )', '(ç­¾å­—åŠç›–ç« )', '(ç­¾å­—å¹¶ç›–ç« )']
    ALL_MARKERS = SEAL_MARKERS + SIGNATURE_MARKERS + COMBO_MARKERS

    @classmethod
    def classify_field(cls, standard_field: str) -> str:
        """åˆ†ç±»å­—æ®µ

        Args:
            standard_field: æ ‡å‡†å­—æ®µåï¼ˆå¦‚ 'companyName', 'legalRepresentative'ï¼‰

        Returns:
            å­—æ®µç±»å‹ï¼š'unit' | 'person' | 'general'
        """
        if not standard_field:
            return 'general'

        if standard_field in cls.UNIT_FIELDS:
            return 'unit'
        elif standard_field in cls.PERSON_FIELDS:
            return 'person'
        else:
            return 'general'

    @classmethod
    def should_fill(cls, field_text: str, standard_field: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¡«å……å­—æ®µ

        æ ¸å¿ƒè§„åˆ™ï¼š
        - ä¸ªäººå­—æ®µ + ç­¾å­—/ç›–ç« æ ‡è®° = ä¸å¡«å……ï¼ˆç•™ç©ºç™½ä¾›æ‰‹å†™ï¼‰
        - å•ä½å­—æ®µ + ä»»ä½•æ ‡è®° = å¡«å……ï¼ˆéœ€è¦å¡«å…¬å¸åï¼‰
        - å…¶ä»–å­—æ®µ = æ­£å¸¸å¡«å……
        - æœªè¯†åˆ«å­—æ®µ + äººå‘˜å…³é”®è¯ + ç­¾å­—æ ‡è®° = ä¸å¡«å……ï¼ˆå¦‚"æ³•å®šä»£è¡¨äººæˆ–å§”æ‰˜ä»£ç†äººï¼ˆç­¾å­—ï¼‰"ï¼‰
        - æ–‡æ¡£æ¸…å•é¡¹ï¼ˆåŒ…å«"èº«ä»½è¯"ã€"å¤å°ä»¶"ç­‰ï¼‰ = ä¸å¡«å……

        Args:
            field_text: åŸå§‹å­—æ®µæ–‡æœ¬ï¼ˆå¦‚ "æ³•å®šä»£è¡¨äººï¼ˆç­¾å­—æˆ–ç›–ç« ï¼‰"ï¼‰
            standard_field: æ ‡å‡†å­—æ®µåï¼ˆå¦‚ 'legalRepresentative'ï¼‰

        Returns:
            æ˜¯å¦åº”è¯¥å¡«å……
        """
        # âš ï¸ å…³é”®ä¿®å¤ï¼šå³ä½¿ std_field æ˜¯ Noneï¼Œä¹Ÿè¦æ£€æŸ¥ç­¾å­—/æ–‡æ¡£å…³é”®è¯
        # è¿™æ ·å¯ä»¥æ­£ç¡®å¤„ç†"æ³•å®šä»£è¡¨äººæˆ–å§”æ‰˜ä»£ç†äººï¼ˆç­¾å­—ï¼‰"ç­‰å¤åˆå­—æ®µ

        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«ç­¾å­—/ç›–ç« æ ‡è®°
        has_signature_marker = any(marker in field_text for marker in cls.ALL_MARKERS)

        # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«äººå‘˜ç›¸å…³å…³é”®è¯
        person_keywords = ['æ³•å®šä»£è¡¨äºº', 'æ³•äºº', 'æˆæƒä»£è¡¨', 'å§”æ‰˜ä»£ç†äºº', 'ä»£ç†äºº', 'è¢«æˆæƒäºº', 'ä»£è¡¨']
        has_person_keyword = any(keyword in field_text for keyword in person_keywords)

        # 3. å¦‚æœåŒæ—¶åŒ…å«äººå‘˜å…³é”®è¯å’Œç­¾å­—æ ‡è®°ï¼Œä¸å¡«å……ï¼ˆæ— è®ºæ˜¯å¦è¯†åˆ«ï¼‰
        if has_person_keyword and has_signature_marker:
            return False

        # 4. æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡æ¡£ææ–™å…³é”®è¯ï¼ˆæ–‡æ¡£æ¸…å•é¡¹ï¼‰
        document_keywords = ['èº«ä»½è¯', 'å¤å°ä»¶', 'è¯æ˜', 'åŸä»¶', 'æ‰«æä»¶', 'é™„ä»¶', 'ææ–™', 'æ–‡ä»¶']
        if any(keyword in field_text for keyword in document_keywords):
            return False

        # å¦‚æœ std_field æ˜¯ Noneï¼ˆæœªè¯†åˆ«å­—æ®µï¼‰ï¼Œé»˜è®¤ä¸å¡«å……ï¼ˆå®‰å…¨ç­–ç•¥ï¼‰
        if not standard_field:
            return False

        field_type = cls.classify_field(standard_field)

        # ä¸ªäººå­—æ®µï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç­¾å­—/ç›–ç« æ ‡è®°
        if field_type == 'person':
            # ä»»ä½•ç­¾å­—/ç›–ç« æ ‡è®°éƒ½ä¸å¡«å……
            has_marker = any(marker in field_text for marker in cls.ALL_MARKERS)
            return not has_marker  # æœ‰æ ‡è®°åˆ™ä¸å¡«å……ï¼Œç•™ç©ºç™½

        # å•ä½å­—æ®µå’Œæ™®é€šå­—æ®µéƒ½å¡«å……
        return True

    @classmethod
    def extract_format_marker(cls, text: str) -> str:
        """æå–æ ¼å¼æ ‡è®°

        Args:
            text: æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«æ ¼å¼æ ‡è®°ï¼‰

        Returns:
            æ‰¾åˆ°çš„æ ¼å¼æ ‡è®°ï¼Œå¦‚ "ï¼ˆç›–ç« ï¼‰"ï¼Œæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        for marker in cls.ALL_MARKERS:
            if marker in text:
                return marker
        return ""

    @classmethod
    def should_preserve_marker(cls, standard_field: str, marker: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™æ ¼å¼æ ‡è®°

        è§„åˆ™ï¼š
        - å•ä½å­—æ®µ + ç›–ç« /å…¬ç« æ ‡è®° = ä¿ç•™ï¼ˆå¡«å……åä»éœ€ç›–ç« ï¼‰
        - å…¶ä»–æƒ…å†µä¸ä¿ç•™

        Args:
            standard_field: æ ‡å‡†å­—æ®µå
            marker: æ ¼å¼æ ‡è®°

        Returns:
            æ˜¯å¦ä¿ç•™æ ‡è®°
        """
        field_type = cls.classify_field(standard_field)

        # å•ä½å­—æ®µä¿ç•™ç›–ç« /å…¬ç« æ ‡è®°
        if field_type == 'unit' and marker in cls.SEAL_MARKERS:
            return True

        return False

    @classmethod
    def is_format_marker(cls, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åªæ˜¯æ ¼å¼æ ‡è®°

        Args:
            text: è¦åˆ¤æ–­çš„æ–‡æœ¬

        Returns:
            æ˜¯å¦ä¸ºçº¯æ ¼å¼æ ‡è®°
        """
        # å»é™¤ç©ºæ ¼åæ£€æŸ¥
        clean_text = text.strip()
        return clean_text in cls.ALL_MARKERS


class PatternMatcher:
    """æ¨¡å¼åŒ¹é…å™¨ - è¯†åˆ«æ–‡æ¡£ä¸­çš„å„ç§å¡«ç©ºæ¨¡å¼"""

    def detect_patterns(self, text: str) -> Dict[str, List]:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„æ‰€æœ‰æ¨¡å¼

        Returns:
            {
                'combo': [...],      # ç»„åˆå­—æ®µ
                'bracket': [...],    # æ‹¬å·å ä½ç¬¦
                'colon': [...],      # å†’å·å¡«ç©º
                'space_fill': [...], # ç©ºæ ¼å¡«ç©ºï¼ˆå­—æ®µå + å¤šä¸ªç©ºæ ¼ï¼‰
                'date': [...]        # æ—¥æœŸæ ¼å¼
            }
        """
        patterns = {}

        # 1. ç»„åˆå­—æ®µæ¨¡å¼ï¼šï¼ˆxxxã€yyyï¼‰
        combo_matches = self._match_combo_pattern(text)
        if combo_matches:
            patterns['combo'] = combo_matches

        # 2. æ‹¬å·å ä½ç¬¦ï¼šï¼ˆxxxï¼‰
        bracket_matches = self._match_bracket_pattern(text)
        if bracket_matches:
            patterns['bracket'] = bracket_matches

        # 3. å†’å·å¡«ç©ºï¼šxxxï¼š___
        colon_matches = self._match_colon_pattern(text)
        if colon_matches:
            patterns['colon'] = colon_matches

        # 4. ç©ºæ ¼å¡«ç©ºï¼šå­—æ®µå + å¤šä¸ªç©ºæ ¼ï¼ˆæ–°å¢ï¼‰
        space_fill_matches = self._match_space_fill_pattern(text)
        if space_fill_matches:
            patterns['space_fill'] = space_fill_matches

        # 5. æ—¥æœŸæ ¼å¼
        date_matches = self._match_date_pattern(text)
        if date_matches:
            patterns['date'] = date_matches

        return patterns

    def _detect_abbreviation_field(self, text_inside_bracket: str) -> Optional[str]:
        """
        æ£€æµ‹æ˜¯å¦æ˜¯å¸¦ç©ºæ ¼å ä½ç¬¦çš„ç®€å†™å­—æ®µ

        åˆ¤æ–­è§„åˆ™ï¼š
        1. å¿…é¡»æœ‰è¶³å¤Ÿçš„ç©ºæ ¼å ä½ç¬¦ï¼ˆâ‰¥5ä¸ªï¼‰
        2. å»é™¤ç©ºæ ¼åæ ¸å¿ƒæ–‡æœ¬â‰¤3ä¸ªå­—
        3. æ ¸å¿ƒæ–‡æœ¬åŒ¹é…ç®€å†™æ˜ å°„è¡¨

        Examples:
            "                 é¡¹ç›®"  â†’ 'projectName' âœ…
            "è¯¥é¡¹ç›®"                 â†’ None âŒ
            "å…¬å¸ç« ç¨‹"               â†’ None âŒ

        Args:
            text_inside_bracket: æ‹¬å·å†…çš„æ–‡æœ¬ï¼ˆåŒ…å«å¯èƒ½çš„ç©ºæ ¼ï¼‰

        Returns:
            æ ‡å‡†å­—æ®µåï¼Œå¦‚ 'projectName'ï¼Œå¦‚æœä¸æ˜¯ç®€å†™åˆ™è¿”å› None
        """
        # è®¡ç®—ç©ºæ ¼æ•°é‡
        stripped = text_inside_bracket.strip()
        total_spaces = len(text_inside_bracket) - len(stripped)

        # å…³é”®æ¡ä»¶1ï¼šå¿…é¡»æœ‰è¶³å¤Ÿçš„ç©ºæ ¼ï¼ˆè¯´æ˜æ˜¯å ä½ç¬¦ï¼‰
        if total_spaces < 5:
            return None

        # å…³é”®æ¡ä»¶2ï¼šæ ¸å¿ƒæ–‡æœ¬å¿…é¡»å¾ˆçŸ­ï¼ˆâ‰¤3å­—ï¼‰
        if len(stripped) > 3:
            return None

        # ç®€å†™æ˜ å°„è¡¨
        abbr_map = {
            'é¡¹ç›®': 'projectName',
            'ç¼–å·': 'projectNumber',
            'å…¬å¸': 'companyName',
            'å•ä½': 'companyName',
            'åœ°å€': 'address',
            'ç”µè¯': 'phone',
            'é‚®ç®±': 'email',
            'ä¼ çœŸ': 'fax',
        }

        return abbr_map.get(stripped)

    def _match_combo_pattern(self, text: str) -> List[Dict]:
        """åŒ¹é…ç»„åˆå­—æ®µï¼šï¼ˆxxxã€yyyï¼‰æˆ–ï¼ˆxxxå’Œyyyï¼‰æˆ–[xxxã€yyyã€zzz]

        æ”¯æŒä¸‰ç§æ‹¬å·ç±»å‹ï¼š
        - å…¨è§’æ‹¬å·ï¼šï¼ˆé¡¹ç›®åç§°ã€æ‹›æ ‡ç¼–å·ï¼‰
        - åŠè§’åœ†æ‹¬å·ï¼š(é¡¹ç›®åç§°ã€æ‹›æ ‡ç¼–å·)
        - åŠè§’æ–¹æ‹¬å·ï¼š[é¡¹ç›®åç§°ã€æ‹›æ ‡ç¼–å·]

        æ”¯æŒå››ç§åˆ†éš”ç¬¦ï¼š
        - é¡¿å·ï¼šã€
        - é€—å·ï¼šï¼Œ
        - å’Œå­—ï¼šå’Œ
        - åŠå­—ï¼šåŠ
        """
        # æ‰©å±•æ­£åˆ™ä»¥æ”¯æŒ"å’Œ"ã€"åŠ"è¿æ¥è¯
        pattern = r'[ï¼ˆ(\[]([^ï¼‰)\]]+(?:[ã€ï¼Œå’ŒåŠ][^ï¼‰)\]]+)+)[ï¼‰)\]]'
        matches = []

        for match in re.finditer(pattern, text):
            combo_text = match.group(1)

            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å­—æ®µçš„ç»„åˆ
            if any(skip in combo_text for skip in ['å¦‚æœ‰', 'å¦‚æœ', 'åŒ…æ‹¬', 'æˆ–è€…']):
                continue

            # åˆ†å‰²ç»„åˆå­—æ®µï¼ˆæ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼‰
            fields = re.split(r'[ã€ï¼Œå’ŒåŠ]', combo_text)
            fields = [f.strip() for f in fields if f.strip()]  # è¿‡æ»¤ç©ºå­—æ®µ

            # è‡³å°‘éœ€è¦2ä¸ªå­—æ®µæ‰ç®—ç»„åˆå­—æ®µ
            if len(fields) < 2:
                continue

            matches.append({
                'full_match': match.group(0),
                'fields': fields,
                'start': match.start(),
                'end': match.end()
            })

        return matches

    def _match_bracket_pattern(self, text: str) -> List[Dict]:
        """åŒ¹é…æ‹¬å·å ä½ç¬¦ï¼šï¼ˆxxxï¼‰ã€(xxx)ã€[xxx]

        æ”¯æŒä¸‰ç§æ‹¬å·ç±»å‹ï¼š
        - å…¨è§’æ‹¬å·ï¼šï¼ˆé¡¹ç›®åç§°ï¼‰
        - åŠè§’åœ†æ‹¬å·ï¼š(é¡¹ç›®åç§°)
        - åŠè§’æ–¹æ‹¬å·ï¼š[é¡¹ç›®åç§°]
        """
        pattern = r'[ï¼ˆ(\[]([^ï¼‰)\]]+)[ï¼‰)\]]'
        matches = []

        for match in re.finditer(pattern, text):
            # ğŸ†• Step 1: å…ˆå°è¯•ç®€å†™å­—æ®µæ£€æµ‹ï¼ˆä½¿ç”¨åŸå§‹æ‹¬å·å†…æ–‡æœ¬ï¼‰
            text_inside_bracket = match.group(1)  # åŒ…å«ç©ºæ ¼çš„åŸå§‹æ–‡æœ¬
            abbr_field = self._detect_abbreviation_field(text_inside_bracket)

            if abbr_field:
                # åŒ¹é…åˆ°ç®€å†™å­—æ®µï¼Œç›´æ¥æ·»åŠ åˆ°ç»“æœ
                matches.append({
                    'full_match': match.group(0),
                    'field': text_inside_bracket.strip(),  # æ˜¾ç¤ºç”¨ï¼ˆå¦‚"é¡¹ç›®"ï¼‰
                    'standard_field': abbr_field,  # ç›´æ¥æä¾›æ ‡å‡†å­—æ®µå
                    'is_abbreviation': True,  # æ ‡è®°ä¸ºç®€å†™
                    'start': match.start(),
                    'end': match.end()
                })
                continue  # è·³è¿‡å¸¸è§„å­—æ®µè¯†åˆ«æµç¨‹

            # Step 2: å¸¸è§„å­—æ®µè¯†åˆ«æµç¨‹
            field_name = text_inside_bracket.strip()

            # è¿‡æ»¤æ‰ç»„åˆå­—æ®µï¼ˆå·²åœ¨comboä¸­å¤„ç†ï¼‰
            if 'ã€' in field_name or 'ï¼Œ' in field_name:
                continue

            # æ¸…ç†æç¤ºæ€§å‰ç¼€ï¼ˆå¦‚"è¯·å¡«å†™"ã€"è¯·è¾“å…¥"ç­‰ï¼‰
            prompt_prefixes = ['è¯·å¡«å†™', 'è¯·è¾“å…¥', 'å¾…å¡«å†™', 'å¡«å†™', 'è¾“å…¥']
            original_field_name = field_name
            for prefix in prompt_prefixes:
                if field_name.startswith(prefix):
                    field_name = field_name[len(prefix):].strip()
                    break

            # âœ… æ–°å¢ï¼šæ¸…ç†å†’å·å’Œåé¢çš„å ä½ç¬¦ï¼ˆç©ºæ ¼/ä¸‹åˆ’çº¿/æ–‡å­—å ä½ç¬¦ï¼‰
            # åŒ¹é… "å­—æ®µåï¼šå ä½ç¬¦" çš„æ ¼å¼
            # æ”¯æŒçš„å ä½ç¬¦ï¼šç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€XXXã€xxxã€å¾…å¡«ã€å¾…å¡«å†™ã€è¯·å¡«å†™ç­‰
            # åªåŒ¹é…å†’å·åé¢æ˜¯å ä½ç¬¦çš„æƒ…å†µï¼Œä¸ä¼šå½±å“å®é™…å†…å®¹ï¼ˆå¦‚"æˆç«‹æ—¥æœŸï¼š2020-01-01"ï¼‰
            colon_match = re.match(r'^([^ï¼š:]+)[ï¼š:]\s*([_\s]*|XXX|xxx|å¾…å¡«|å¾…å¡«å†™|è¯·å¡«å†™|å¾…ç¡®å®š|æš‚æ— )?$', field_name)
            if colon_match:
                field_name = colon_match.group(1).strip()

            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å­—æ®µçš„å†…å®¹
            skip_keywords = [
                'å¦‚æœ‰', 'å¦‚æœ', 'www.', 'http', 'è¯´æ˜', 'æ³¨æ„', 'å¤‡æ³¨',
                # æ–‡æ¡£ææ–™ç±»å…³é”®è¯
                'èº«ä»½è¯', 'å¤å°ä»¶', 'è¯æ˜', 'åŸä»¶', 'æ‰«æä»¶', 'é™„ä»¶', 'ææ–™', 'æ–‡ä»¶'
            ]
            if any(skip in field_name for skip in skip_keywords):
                continue

            # å¦‚æœæ¸…ç†åå­—æ®µåå¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡
            if len(field_name) < 2:
                continue

            # âœ… æ”¹è¿›ï¼šæ£€æµ‹æ˜¯å¦åŒ…å«å ä½ç¬¦ï¼ˆå†’å·åè·Ÿ3ä¸ªä»¥ä¸Šç©ºæ ¼æˆ–ä¸‹åˆ’çº¿ï¼‰
            # å¦‚æœåŒ…å«å ä½ç¬¦ï¼Œè¯´æ˜æ˜¯å¾…å¡«å†™å­—æ®µï¼Œä¸åº”è·³è¿‡
            has_placeholder = bool(re.search(r'[ï¼š:]\s*[_\s]{3,}', original_field_name))

            # âœ… å…³é”®æ£€æµ‹ï¼šåˆ¤æ–­æ‹¬å·å†…æ˜¯å­—æ®µåè¿˜æ˜¯å®é™…å†…å®¹
            # åªæœ‰å½“è¶…è¿‡8å­—ç¬¦ã€æœªæ¸…ç†ä»»ä½•å†…å®¹ã€ä¸”ä¸åŒ…å«å ä½ç¬¦æ—¶ï¼Œæ‰è·³è¿‡
            # å¸¸è§å­—æ®µåå¦‚"é¡¹ç›®åç§°"ã€"å…¬å¸åç§°"ã€"æ³•å®šä»£è¡¨äºº"ç­‰é€šå¸¸ä¸è¶…è¿‡8å­—ç¬¦
            if len(original_field_name) > 8 and original_field_name == field_name and not has_placeholder:
                # å¦‚æœè¶…è¿‡8å­—ç¬¦ï¼Œå¾ˆå¯èƒ½æ˜¯å®é™…å†…å®¹ï¼ˆå¦‚å…¬å¸å…¨ç§°ã€é•¿é¡¹ç›®åç­‰ï¼‰ï¼Œè·³è¿‡
                continue

            matches.append({
                'full_match': match.group(0),
                'field': field_name,
                'start': match.start(),
                'end': match.end()
            })

        return matches

    def _match_colon_pattern(self, text: str) -> List[Dict]:
        """åŒ¹é…å†’å·å¡«ç©ºï¼šxxxï¼š___  æˆ– xxxï¼šï¼ˆç©ºç™½ï¼‰æˆ– xxxï¼š"""
        # ä¿®æ”¹æ­£åˆ™ï¼šæ­£ç¡®å¤„ç†å†’å·åçš„å†…å®¹ï¼ˆåŒ…æ‹¬æ‹¬å·å†…çš„ä¸­æ–‡ï¼‰
        # ç­–ç•¥ï¼šå…ˆåŒ¹é…å†’å·å‰çš„å­—æ®µåï¼Œç„¶åæ•è·å†’å·ååˆ°è¡Œå°¾æˆ–ä¸‹ä¸€ä¸ªå­—æ®µçš„å†…å®¹
        # ä½¿ç”¨æ­£å‘å‰ç»æ¥è¯†åˆ«ä¸‹ä¸€ä¸ªå­—æ®µï¼ˆ2ä¸ªä»¥ä¸Šä¸­æ–‡å­—ç¬¦åè·Ÿå†’å·ï¼‰
        import logging
        logger = logging.getLogger("ai_tender_system.smart_filler")

        pattern = r'([^ï¼š:\n]{2,20})[:ï¼š]\s*(.*)(?=(?:\s{2,}[\u4e00-\u9fa5]{2,}[:ï¼š])|$)'
        matches = []

        for match in re.finditer(pattern, text):
            original_field_name = match.group(1).strip()
            after_colon = match.group(2).strip()  # å†’å·åçš„å†…å®¹

            # æå–çº¯å­—æ®µåï¼ˆç§»é™¤æ‹¬å·ç­‰ï¼‰ç”¨äºæ•°æ®åŒ¹é…
            clean_field_name = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', original_field_name).strip()

            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–æ˜æ˜¾ä¸æ˜¯å­—æ®µçš„å†…å®¹
            if len(clean_field_name) < 2:
                continue

            # âœ… å…³é”®æ£€æµ‹ï¼šåˆ¤æ–­å†’å·åæ˜¯å¦å·²æœ‰å®é™…å†…å®¹
            if after_colon:
                # å»é™¤ä¸‹åˆ’çº¿ã€ç©ºæ ¼ç­‰å ä½ç¬¦
                content_without_placeholder = re.sub(r'[_\s]+', '', after_colon)

                # ğŸ†• å¢å¼ºæ£€æµ‹ï¼šåŒºåˆ†"çœŸå®å†…å®¹"ã€"æ ¼å¼æ ‡è®°"å’Œ"ä¸‹ä¸€ä¸ªå­—æ®µå"
                if len(content_without_placeholder) > 0:
                    # æ£€æŸ¥æ˜¯å¦åªæ˜¯æ ¼å¼æ ‡è®°
                    if FieldClassifier.is_format_marker(content_without_placeholder):
                        # æ˜¯æ ¼å¼æ ‡è®°ï¼ˆå¦‚"ï¼ˆç›–ç« ï¼‰"ï¼‰ï¼Œä¸æ˜¯å†…å®¹ï¼Œåº”è¯¥ç»§ç»­å¤„ç†
                        pass
                    # å¦‚æœå†…å®¹ä»¥å†’å·ç»“å°¾ï¼Œè¯´æ˜æ˜¯ä¸‹ä¸€ä¸ªå­—æ®µåï¼ˆå¦‚"é‚®æ”¿ç¼–ç ï¼š"ï¼‰ï¼Œä¸ç®—å·²å¡«å†™
                    elif content_without_placeholder.endswith(('ï¼š', ':')):
                        # è¿™æ˜¯æ¨ªå‘å¤šå­—æ®µæ ¼å¼ï¼Œç»§ç»­å¤„ç†å½“å‰å­—æ®µ
                        pass
                    else:
                        # å¯èƒ½æ˜¯çœŸæ­£çš„å†…å®¹ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥
                        # å»é™¤æ ¼å¼æ ‡è®°åå†åˆ¤æ–­
                        real_content = content_without_placeholder
                        for marker in FieldClassifier.ALL_MARKERS:
                            real_content = real_content.replace(marker, '')

                        # ğŸ†• æ£€æŸ¥æ˜¯å¦æ˜¯è¯´æ˜æ€§æ–‡å­—å ä½ç¬¦ï¼ˆ2025-11-15æ–°å¢ï¼‰
                        # è¯†åˆ«ç±»ä¼¼"æ³¨: å¦‚æ§è‚¡è‚¡ä¸œ/æŠ•èµ„äººä¸ºè‡ªç„¶äººéœ€æä¾›å§“åå’Œèº«ä»½è¯å·"çš„è¯´æ˜æ–‡å­—
                        instruction_patterns = [
                            r'^æ³¨[:ï¼š]',                    # ä»¥"æ³¨:"å¼€å¤´
                            r'^è¯´æ˜[:ï¼š]',                  # ä»¥"è¯´æ˜:"å¼€å¤´
                            r'^å¤‡æ³¨[:ï¼š]',                  # ä»¥"å¤‡æ³¨:"å¼€å¤´
                            r'^æç¤º[:ï¼š]',                  # ä»¥"æç¤º:"å¼€å¤´
                            r'^å¦‚æœ.*éœ€è¦?æä¾›',            # "å¦‚æœ...éœ€æä¾›"æˆ–"å¦‚æœ...éœ€è¦æä¾›"
                            r'éœ€è¦?æä¾›.*èº«ä»½è¯',           # åŒ…å«"éœ€æä¾›...èº«ä»½è¯"
                            r'å¦‚.*ä¸º.*äºº.*éœ€',              # "å¦‚...ä¸ºè‡ªç„¶äººéœ€..."æˆ–"å¦‚...ä¸ºæ³•äººéœ€..."
                            r'å¦‚.*äºº.*æä¾›',                # "å¦‚...äºº...æä¾›"
                        ]

                        is_instruction = any(re.search(p, real_content) for p in instruction_patterns)

                        # å¦‚æœä¸æ˜¯è¯´æ˜æ€§æ–‡å­—ï¼Œä¸”å»é™¤æ ¼å¼æ ‡è®°åè¿˜æœ‰å†…å®¹ï¼ˆè¶…è¿‡2ä¸ªå­—ç¬¦ï¼‰ï¼Œæ‰è®¤ä¸ºæ˜¯å·²å¡«å†™
                        if not is_instruction and real_content.strip() and len(real_content.strip()) > 2:
                            # çœŸæ­£çš„å†…å®¹ï¼Œè·³è¿‡å·²å¡«å†™çš„å­—æ®µ
                            logger.debug(f"  [_match_colon_pattern] è·³è¿‡å·²å¡«å†™å­—æ®µ: '{clean_field_name}', å†…å®¹='{real_content[:30]}...'")
                            continue
                        elif is_instruction:
                            # æ˜¯è¯´æ˜æ€§æ–‡å­—å ä½ç¬¦ï¼Œåº”è¯¥ç»§ç»­å¤„ç†
                            logger.debug(f"  [_match_colon_pattern] è¯†åˆ«åˆ°è¯´æ˜æ€§å ä½ç¬¦: '{real_content[:50]}...'")
                            pass

            # è°ƒè¯•ï¼šæ‰“å°åŒ¹é…çš„å†…å®¹
            logger.debug(f"  [_match_colon_pattern] åŒ¹é…åˆ°: full_match='{match.group(0)}', after_colon='{after_colon}'")

            matches.append({
                'full_match': match.group(0),
                'field': clean_field_name,  # æ¸…ç†åçš„å­—æ®µåï¼ˆç”¨äºæ•°æ®åŒ¹é…ï¼‰
                'original_field': original_field_name,  # åŸå§‹å­—æ®µåï¼ˆåŒ…å«æ‹¬å·ï¼Œç”¨äºæ›¿æ¢ï¼‰
                'after_colon': after_colon,  # ä¿å­˜å†’å·åçš„å†…å®¹ï¼ˆå¯èƒ½åŒ…å«æ ¼å¼æ ‡è®°ï¼‰
                'start': match.start(),
                'end': match.end()
            })

        return matches

    def _match_space_fill_pattern(self, text: str) -> List[Dict]:
        """
        åŒ¹é…ç©ºæ ¼å¡«ç©ºæ¨¡å¼ï¼šå­—æ®µå + å¤šä¸ªç©ºæ ¼ï¼ˆæ— å†’å·ï¼‰

        ç¤ºä¾‹ï¼š
        - "åœ°å€                                          "
        - "ç”µè¯                                           ç”µå­å‡½ä»¶                                "
        - "æŠ•æ ‡äººåç§°ï¼ˆç›–ç« ï¼‰                             "

        æ³¨æ„ï¼š
        - å­—æ®µåé•¿åº¦åœ¨2-20ä¸ªå­—ä¹‹é—´ï¼ˆæ”¯æŒå¸¦æ‹¬å·çš„å­—æ®µï¼‰
        - åé¢è‡³å°‘5ä¸ªè¿ç»­ç©ºæ ¼ï¼ˆé¿å…è¯¯åŒ¹é…æ™®é€šæ–‡æœ¬ï¼‰
        - æ’é™¤å·²æœ‰å†’å·çš„å­—æ®µï¼ˆç”±colonæ¨¡å¼å¤„ç†ï¼‰
        """
        # åŒ¹é…ï¼š2-20ä¸ªæ±‰å­—/å­—æ¯/æ‹¬å· + è‡³å°‘5ä¸ªç©ºæ ¼
        # æ”¯æŒå¸¦æ‹¬å·çš„å­—æ®µåï¼Œå¦‚"æŠ•æ ‡äººåç§°ï¼ˆç›–ç« ï¼‰"
        # ä½¿ç”¨è´Ÿå‘åæŸ¥çœ‹æ–­è¨€ç¡®ä¿åé¢æ²¡æœ‰å†’å·
        pattern = r'([\u4e00-\u9fa5a-zA-Z/ï¼ˆï¼‰()]{2,20})(?![ï¼š:])(\s{5,})'
        matches = []

        for match in re.finditer(pattern, text):
            field_name = match.group(1).strip()
            spaces = match.group(2)

            # æ¸…ç†æ‹¬å·åç¼€ï¼ˆå¦‚"ï¼ˆç›–ç« ï¼‰"ã€"ï¼ˆå…¬ç« ï¼‰"ç­‰ï¼‰ç”¨äºæ•°æ®åŒ¹é…
            # æ³¨æ„ï¼šä¿ç•™åŸå§‹å­—æ®µåç”¨äºæ–‡æ¡£æ›¿æ¢
            clean_field_name = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]', '', field_name).strip()

            # è·³è¿‡æ˜æ˜¾ä¸æ˜¯å­—æ®µçš„å†…å®¹
            skip_keywords = [
                'æœ¬æ¡', 'æ—¶é—´ä»', 'ä»¥ç›¸å…³', 'æˆ‘å…¬å¸', 'å¦‚æœ‰', 'å¦‚æœ', 'è§é™„ä»¶', 'æ»¡è¶³',
                # æ–‡æ¡£ææ–™ç±»å…³é”®è¯
                'èº«ä»½è¯', 'å¤å°ä»¶', 'è¯æ˜', 'åŸä»¶', 'æ‰«æä»¶', 'é™„ä»¶', 'ææ–™', 'æ–‡ä»¶'
            ]
            if any(skip in clean_field_name for skip in skip_keywords):
                continue

            # è·³è¿‡æ¸…ç†åå­—æ®µåå¤ªçŸ­çš„
            if len(clean_field_name) < 2:
                continue

            # è·³è¿‡æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦å¼€å¤´çš„ï¼ˆæ£€æŸ¥æ¸…ç†åçš„å­—æ®µåï¼‰
            if clean_field_name[0].isdigit() or clean_field_name[0] in ['ï¼š', ':']:
                continue

            # æ£€æŸ¥æ˜¯å¦åœ¨å†’å·ä¹‹å‰ï¼ˆé¿å…ä¸colonæ¨¡å¼å†²çªï¼‰
            # å¦‚æœè¿™ä¸ªå­—æ®µåé¢ç´§è·Ÿç€å†’å·ï¼Œåº”è¯¥ç”±colonæ¨¡å¼å¤„ç†
            next_char_pos = match.end()
            if next_char_pos < len(text) and text[next_char_pos] in ['ï¼š', ':']:
                continue

            matches.append({
                'full_match': match.group(0),
                'field': clean_field_name,  # ç”¨äºæ•°æ®åŒ¹é…çš„æ¸…ç†åå­—æ®µå
                'original_field': field_name,  # ä¿ç•™æ‹¬å·çš„åŸå§‹å­—æ®µåï¼ˆç”¨äºæ›¿æ¢ï¼‰
                'start': match.start(),
                'end': match.end()
            })

        return matches

    def _match_date_pattern(self, text: str) -> List[Dict]:
        """
        åŒ¹é…æ—¥æœŸæ ¼å¼ï¼š
        - ____å¹´____æœˆ____æ—¥ (ä¸‹åˆ’çº¿å ä½ç¬¦)
        - XXXXå¹´XæœˆXæ—¥ (å­—æ¯Xå ä½ç¬¦)
        - æ—¥æœŸï¼š____å¹´____æœˆ____æ—¥
        - æ—¥æœŸï¼šXXXXå¹´XæœˆXæ—¥
        """
        patterns = [
            # å¸¦"æ—¥æœŸï¼š"å‰ç¼€çš„æ¨¡å¼
            r'æ—¥æœŸ\s*[:ï¼š]?\s*[_\s]*å¹´[_\s]*æœˆ[_\s]*æ—¥',  # æ—¥æœŸï¼š____å¹´____æœˆ____æ—¥
            r'æ—¥æœŸ\s*[:ï¼š]?\s*[X]{1,4}å¹´[X]{1,2}æœˆ[X]{1,2}æ—¥',  # æ—¥æœŸï¼šXXXXå¹´XæœˆXæ—¥
            # ä¸å¸¦å‰ç¼€çš„æ¨¡å¼
            r'[_\s]+å¹´[_\s]+æœˆ[_\s]+æ—¥',  # ____å¹´____æœˆ____æ—¥ï¼ˆå¤šä¸ªå ä½ç¬¦ï¼‰
            r'[X]{1,4}å¹´[X]{1,2}æœˆ[X]{1,2}æ—¥',  # XXXXå¹´XæœˆXæ—¥
        ]

        matches = []
        for pattern in patterns:
            for match in re.finditer(pattern, text):
                # åªåŒ¹é…åŒ…å«"å¹´æœˆæ—¥"çš„éƒ¨åˆ†
                matches.append({
                    'full_match': match.group(0),
                    'field': 'date',
                    'start': match.start(),
                    'end': match.end()
                })
                return matches  # åªå–ç¬¬ä¸€ä¸ªåŒ¹é…

        return matches


class FieldRecognizer:
    """å­—æ®µè¯†åˆ«å™¨ - è¯†åˆ«å­—æ®µåç§°å¹¶æ˜ å°„åˆ°æ•°æ®"""

    def __init__(self):
        # å­—æ®µå˜ä½“åº“ï¼šç»Ÿä¸€æ˜ å°„
        self.field_variants = {
            # ä¾›åº”å•†åç§°
            'companyName': [
                'ä¾›åº”å•†', 'ä¾›åº”å•†åç§°', 'ä¾›åº”å•†å…¨ç§°',  # æ·»åŠ "ä¾›åº”å•†"ç®€å†™
                'æŠ•æ ‡äºº', 'æŠ•æ ‡äººåç§°', 'æŠ•æ ‡äººå…¨ç§°',  # æ·»åŠ "æŠ•æ ‡äºº"ç®€å†™
                'å…¬å¸åç§°', 'å•ä½åç§°', 'åº”ç­”äººåç§°', 'ä¼ä¸šåç§°',
                'å“åº”äºº', 'å“åº”äººåç§°', 'å“åº”äººå…¨ç§°'  # æ·»åŠ "å“åº”äºº"ç®€å†™
            ],

            # é¡¹ç›®ä¿¡æ¯
            'projectName': ['é¡¹ç›®åç§°', 'é‡‡è´­é¡¹ç›®åç§°', 'æ‹›æ ‡é¡¹ç›®åç§°'],
            'projectNumber': ['é¡¹ç›®ç¼–å·', 'é‡‡è´­ç¼–å·', 'æ‹›æ ‡ç¼–å·', 'é¡¹ç›®å·', 'ç¼–å·'],

            # é‡‡è´­æ–¹ä¿¡æ¯
            'purchaserName': ['é‡‡è´­äºº', 'é‡‡è´­äººåç§°', 'æ‹›æ ‡äºº', 'æ‹›æ ‡äººåç§°', 'ç”²æ–¹', 'ç”²æ–¹åç§°'],

            # è”ç³»æ–¹å¼
            'address': ['åœ°å€', 'æ³¨å†Œåœ°å€', 'åŠå…¬åœ°å€', 'è”ç³»åœ°å€', 'é€šè®¯åœ°å€', 'ä¾›åº”å•†åœ°å€', 'å…¬å¸åœ°å€'],
            'phone': ['ç”µè¯', 'è”ç³»ç”µè¯', 'å›ºå®šç”µè¯', 'ç”µè¯å·ç '],
            'email': ['ç”µå­é‚®ä»¶', 'ç”µå­é‚®ç®±', 'é‚®ç®±', 'email', 'Email', 'E-mail', 'E-Mail', 'ç”µå­å‡½ä»¶'],
            'fax': ['ä¼ çœŸ', 'ä¼ çœŸå·ç ', 'ä¼ çœŸå·'],
            'postalCode': ['é‚®ç¼–', 'é‚®æ”¿ç¼–ç ', 'é‚®ç '],

            # äººå‘˜ä¿¡æ¯
            'legalRepresentative': [
                'æ³•å®šä»£è¡¨äºº', 'æ³•äººä»£è¡¨', 'æ³•äºº', 'æ³•äººå§“å', 'å§“å',
                'æ³•å®šä»£è¡¨äººå§“å', 'æ³•å®šä»£è¡¨äººåç§°', 'è´Ÿè´£äºº', 'è´Ÿè´£äººå§“å'
            ],
            'legalRepresentativePosition': ['æ³•äººèŒä½', 'æ³•å®šä»£è¡¨äººèŒä½', 'æ³•äººèŒåŠ¡'],
            'legalRepresentativeGender': ['æ€§åˆ«', 'æ³•äººæ€§åˆ«', 'æ³•å®šä»£è¡¨äººæ€§åˆ«'],
            'legalRepresentativeAge': ['å¹´é¾„', 'æ³•äººå¹´é¾„', 'æ³•å®šä»£è¡¨äººå¹´é¾„'],

            # è¢«æˆæƒäººä¿¡æ¯ï¼ˆæ‰©å±•å˜ä½“ï¼‰
            'representativeName': [
                'ä¾›åº”å•†è¢«æˆæƒäººå§“å', 'è¢«æˆæƒäººå§“å', 'æˆæƒäººå§“å',
                'ä¾›åº”å•†ä»£è¡¨', 'ä¾›åº”å•†ä»£è¡¨å§“å', 'ä»£è¡¨å§“å', 'ä»£è¡¨äºº',
                'ç­¾å­—äººå§“å', 'ç­¾å­—äºº',  # æ–°å¢ï¼šç­¾å­—äºº
                'å…¨æƒä»£è¡¨å§“å', 'å…¨æƒä»£è¡¨',  # æ–°å¢ï¼šå…¨æƒä»£è¡¨
                'æˆæƒä»£è¡¨', 'æˆæƒä»£è¡¨å§“å',  # æ–°å¢ï¼šæˆæƒä»£è¡¨
                'è¢«æˆæƒä»£è¡¨', 'è¢«æˆæƒä»£è¡¨å§“å'  # æ–°å¢ï¼šè¢«æˆæƒä»£è¡¨
            ],
            'representativeTitle': [
                'èŒåŠ¡', 'èŒç§°', 'èŒåŠ¡èŒç§°', 'èŒä½',
                'ç­¾å­—äººèŒåŠ¡', 'ç­¾å­—äººèŒç§°',  # æ–°å¢ï¼šç­¾å­—äººèŒåŠ¡/èŒç§°
                'å…¨æƒä»£è¡¨èŒåŠ¡', 'å…¨æƒä»£è¡¨èŒç§°',  # æ–°å¢ï¼šå…¨æƒä»£è¡¨èŒåŠ¡/èŒç§°
                'ä»£è¡¨èŒåŠ¡', 'ä»£è¡¨èŒç§°',  # æ–°å¢ï¼šä»£è¡¨èŒåŠ¡/èŒç§°
                'è¢«æˆæƒäººèŒåŠ¡', 'è¢«æˆæƒäººèŒç§°'  # æ–°å¢ï¼šè¢«æˆæƒäººèŒåŠ¡/èŒç§°
            ],
            'authorizedPersonId': ['è¢«æˆæƒäººèº«ä»½è¯', 'æˆæƒäººèº«ä»½è¯', 'èº«ä»½è¯å·', 'èº«ä»½è¯å·ç ', 'è¢«æˆæƒäººèº«ä»½è¯å·', 'æˆæƒäººèº«ä»½è¯å·'],

            # å…¬å¸åŸºæœ¬ä¿¡æ¯
            'establishDate': [
                'æˆç«‹æ—¶é—´', 'æˆç«‹æ—¥æœŸ', 'æ³¨å†Œæ—¥æœŸ', 'æ³¨å†Œæ—¶é—´',
                'æˆç«‹å’Œæˆ–æ³¨å†Œæ—¥æœŸ', 'æˆç«‹å’Œ/æˆ–æ³¨å†Œæ—¥æœŸ'  # æ–°å¢ï¼šå¤„ç†ç‰¹æ®Šæ ¼å¼
            ],
            'businessScope': ['ç»è¥èŒƒå›´', 'ä¸šåŠ¡èŒƒå›´', 'ç»è¥å†…å®¹'],
            'registeredCapital': ['æ³¨å†Œèµ„æœ¬', 'æ³¨å†Œèµ„é‡‘', 'å®æ”¶èµ„æœ¬'],
            'socialCreditCode': ['ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ', 'ç¤¾ä¼šä¿¡ç”¨ä»£ç ', 'ä¿¡ç”¨ä»£ç '],
            'companyType': ['å…¬å¸ç±»å‹', 'ä¼ä¸šç±»å‹', 'ç»„ç»‡å½¢å¼', 'å•ä½æ€§è´¨', 'ä¼ä¸šæ€§è´¨'],
            'businessTerm': ['ç»è¥æœŸé™', 'è¥ä¸šæœŸé™', 'ç»è¥å¹´é™'],

            # æ—¥æœŸ
            'date': [
                'æ—¥æœŸ', 'date',
                'ç­¾å­—æ—¥æœŸ', 'ç­¾ç½²æ—¥æœŸ', 'è½æ¬¾æ—¥æœŸ'  # æ–°å¢ï¼šå¤„ç†å„ç§ç­¾ç½²æ—¥æœŸæ ¼å¼
            ],

            # è‚¡æƒç»“æ„å­—æ®µï¼ˆ2025-10-30æ·»åŠ ï¼Œ2025-11-09å¢å¼ºï¼‰
            'actual_controller': [
                'å®é™…æ§åˆ¶äºº', 'å®é™…æ§åˆ¶äººå§“å', 'å®é™…æ§åˆ¶äººåç§°',
                'å®æ§äºº', 'å®æ§äººå§“å'
            ],
            'controlling_shareholder': [
                'æ§è‚¡è‚¡ä¸œ', 'æ§è‚¡è‚¡ä¸œåç§°', 'æ§è‚¡è‚¡ä¸œåŠå‡ºèµ„æ¯”ä¾‹',
                'ç¬¬ä¸€å¤§è‚¡ä¸œ', 'æœ€å¤§è‚¡ä¸œ',
                'ä¾›åº”å•†çš„æ§è‚¡è‚¡ä¸œ/æŠ•èµ„äººåç§°åŠå‡ºèµ„æ¯”ä¾‹',  # ğŸ†• æ”¯æŒé•¿å­—æ®µå
                'ä¾›åº”å•†çš„æ§è‚¡è‚¡ä¸œ'  # æ”¯æŒéƒ¨åˆ†åŒ¹é…
            ],
            'shareholders_info': [
                'è‚¡ä¸œ', 'è‚¡ä¸œä¿¡æ¯', 'è‚¡ä¸œåç§°', 'æŠ•èµ„äººä¿¡æ¯',
                'è‚¡ä¸œåŠå‡ºèµ„æ¯”ä¾‹', 'æŠ•èµ„äººåç§°åŠå‡ºèµ„æ¯”ä¾‹',
                'ä¾›åº”å•†çš„æ§è‚¡è‚¡ä¸œ', 'ä¾›åº”å•†çš„éæ§è‚¡è‚¡ä¸œ',
                'ä¾›åº”å•†çš„éæ§è‚¡è‚¡ä¸œ/æŠ•èµ„äººåç§°åŠå‡ºèµ„æ¯”ä¾‹',  # ğŸ†• æ”¯æŒé•¿å­—æ®µå
                'è‚¡æƒç»“æ„'
            ],

            # ç®¡ç†å…³ç³»å­—æ®µï¼ˆ2025-10-30æ·»åŠ ï¼‰
            'managing_unit_name': [
                'ç®¡ç†å…³ç³»å•ä½', 'ç®¡ç†å…³ç³»å•ä½åç§°',
                'ä¸‹å±å•ä½', 'ä¸‹çº§å•ä½', 'åˆ†æ”¯æœºæ„'
            ],
            'managed_unit_name': [
                'è¢«ç®¡ç†å…³ç³»å•ä½', 'è¢«ç®¡ç†å…³ç³»å•ä½åç§°',
                'ä¸Šçº§å•ä½', 'ä¸»ç®¡å•ä½', 'éš¶å±å•ä½'
            ],
        }

        # åå‘æ˜ å°„ï¼šä»å˜ä½“åˆ°æ ‡å‡†å­—æ®µå
        self.reverse_map = {}
        for standard_name, variants in self.field_variants.items():
            for variant in variants:
                self.reverse_map[variant.lower()] = standard_name

    def recognize_field(self, field_text: str, enable_logging=False) -> Optional[str]:
        """
        è¯†åˆ«å­—æ®µåç§°ï¼Œè¿”å›æ ‡å‡†å­—æ®µå

        Args:
            field_text: åŸå§‹å­—æ®µæ–‡æœ¬
            enable_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼ˆç”¨äºè¯Šæ–­ï¼‰

        Returns:
            æ ‡å‡†å­—æ®µåï¼Œå¦‚ 'companyName', 'projectName' ç­‰
        """
        original_field_text = field_text

        # âœ… å…³é”®æ£€æŸ¥ï¼šå¦‚æœåŸå§‹å­—æ®µåŒ…å«ç­¾å­—/ç›–ç« ç›¸å…³å…³é”®è¯ï¼Œé€šå¸¸éœ€è¦è·³è¿‡
        # æ ¹æ®è§„åˆ™ï¼š"æ³•å®šä»£è¡¨äººï¼Œæˆæƒä»£è¡¨äººï¼Œåé¢å¸¦æœ‰"ç­¾å­—"å­—æ ·çš„ï¼Œä¸éœ€è¦åšå¡«ç©ºè§„åˆ™æˆ–æ›¿æ¢æ ¼å¼"
        signature_keywords = ['ç­¾å­—', 'ç­¾å', 'ç­¾ç« ', 'ç›–ç« å¤„']  # æ³¨æ„ï¼šä¸åŒ…æ‹¬å•ç‹¬çš„"ç›–ç« "ï¼Œå› ä¸ºæœ‰äº›å­—æ®µå¦‚"å•ä½åç§°ï¼ˆç›–ç« ï¼‰"éœ€è¦å¡«å……

        # ç‰¹æ®Šå¤„ç†ï¼šå¯¹äºä»¥ä¸‹å…³é”®äººå‘˜å­—æ®µï¼Œå¦‚æœåŒ…å«ç­¾å­—ç›¸å…³è¯ï¼Œåˆ™è·³è¿‡
        person_fields = ['æ³•å®šä»£è¡¨äºº', 'æ³•äººä»£è¡¨', 'æ³•äºº', 'æˆæƒä»£è¡¨', 'æˆæƒäºº', 'è¢«æˆæƒäºº', 'ä»£è¡¨äºº', 'è´Ÿè´£äºº']

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç­¾å­—ç›¸å…³å…³é”®è¯
        has_signature_keyword = any(keyword in original_field_text for keyword in signature_keywords)

        # ç‰¹æ®Šæ£€æŸ¥ï¼š"ï¼ˆç­¾å­—æˆ–ç›–ç« ï¼‰"è¿™ç§æ ¼å¼ä¹Ÿè¦è·³è¿‡
        if 'ç­¾å­—æˆ–ç›–ç« ' in original_field_text or 'ç­¾å­—åŠç›–ç« ' in original_field_text or 'ç­¾å­—å¹¶ç›–ç« ' in original_field_text:
            has_signature_keyword = True

        # æ£€æŸ¥æ˜¯å¦æ˜¯äººå‘˜å­—æ®µ
        is_person_field = any(field in original_field_text for field in person_fields)

        # å¦‚æœæ˜¯ç­¾å­—/ç›–ç« ç›¸å…³çš„äººå‘˜å­—æ®µï¼Œè¿”å›Noneè¡¨ç¤ºè·³è¿‡
        if has_signature_keyword and is_person_field:
            if enable_logging:
                from common import get_module_logger
                logger = get_module_logger("field_recognizer")
                logger.info(f"âš ï¸  è·³è¿‡ç­¾å­—/ç›–ç« å­—æ®µ: '{original_field_text}'")
            return None

        field_text = field_text.strip().lower()

        # ç§»é™¤å¸¸è§åç¼€
        field_text = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', field_text)  # ç§»é™¤æ‹¬å·
        field_text = field_text.replace('ï¼ˆç›–ç« ï¼‰', '').replace('ï¼ˆå…¬ç« ï¼‰', '')
        field_text = field_text.replace('ï¼ˆç­¾å­—ï¼‰', '').replace('ï¼ˆç­¾åï¼‰', '')
        field_text = field_text.strip()

        # âœ… å…³é”®ä¿®å¤ï¼šç§»é™¤å­—æ®µåä¸­çš„æ‰€æœ‰ç©ºæ ¼
        # å¤„ç†å¦‚"æ—¥      æœŸ"ï¼ˆæ—¥å’ŒæœŸä¹‹é—´æœ‰å¤šä¸ªç©ºæ ¼ï¼‰çš„æƒ…å†µ
        field_text = re.sub(r'\s+', '', field_text)

        # æŸ¥æ‰¾åŒ¹é…
        std_field = self.reverse_map.get(field_text)

        # âœ… è¯Šæ–­æ—¥å¿—ï¼šç‰¹åˆ«å…³æ³¨"å“åº”äººåç§°"ç›¸å…³å­—æ®µ
        if enable_logging or 'å“åº”' in original_field_text or 'åº”ç­”' in original_field_text:
            if std_field:
                # æ‰¾åˆ°åŒ¹é… - è¾“å‡ºinfoæ—¥å¿—
                pass  # ç”±ContentFillerè¾“å‡ºæˆåŠŸæ—¥å¿—
            else:
                # æœªæ‰¾åˆ°åŒ¹é… - è¾“å‡ºwarningæ—¥å¿—
                from common import get_module_logger
                logger = get_module_logger("field_recognizer")
                logger.warning(f"âš ï¸  å­—æ®µè¯†åˆ«å¤±è´¥: '{original_field_text}' â†’ (æ¸…ç†å) '{field_text}' â†’ æœªåŒ¹é…")

        return std_field

    def recognize_combo_fields(self, fields: List[str]) -> List[Optional[str]]:
        """è¯†åˆ«ç»„åˆå­—æ®µ"""
        return [self.recognize_field(f) for f in fields]


class ContentFiller:
    """å†…å®¹å¡«å……å™¨ - å®é™…æ‰§è¡Œå¡«å……æ“ä½œ"""

    def __init__(self, logger):
        self.logger = logger
        self.field_recognizer = FieldRecognizer()

    def fill_combo_field(self,
                        paragraph: Paragraph,
                        text: str,
                        matches: List[Dict],
                        data: Dict[str, Any]) -> bool:
        """å¡«å……ç»„åˆå­—æ®µï¼ˆä½¿ç”¨runç²¾ç¡®æ›¿æ¢ï¼‰"""
        if not matches:
            return False

        # æ„å»ºæ®µè½çš„runæ˜ å°„ï¼ˆåªæ„å»ºä¸€æ¬¡ï¼‰
        full_text, runs, char_to_run_map = WordDocumentUtils.build_paragraph_text_map(paragraph)
        filled_count = 0

        # å¤„ç†æ‰€æœ‰ç»„åˆå­—æ®µï¼ˆä»åå¾€å‰ï¼Œè¿™æ ·ä½ç½®ä¸ä¼šå¤±æ•ˆï¼‰
        for match in reversed(matches):
            fields = match['fields']

            # è¯†åˆ«å­—æ®µå
            standard_fields = self.field_recognizer.recognize_combo_fields(fields)

            # è·å–å€¼
            values = []
            all_found = True
            for std_field in standard_fields:
                if std_field and std_field in data:
                    values.append(str(data[std_field]))
                else:
                    all_found = False
                    break

            if not all_found:
                continue  # è·³è¿‡è¿™ä¸ªç»„åˆå­—æ®µ

            # æ£€æµ‹å¹¶ä¿æŒåŸæ‹¬å·ç±»å‹
            full_match = match['full_match']
            if 'ï¼ˆ' in full_match:
                bracket_open, bracket_close = 'ï¼ˆ', 'ï¼‰'
            elif '[' in full_match:
                bracket_open, bracket_close = '[', ']'
            else:
                bracket_open, bracket_close = '(', ')'

            # æ„å»ºæ›¿æ¢æ–‡æœ¬ï¼ˆä¿æŒåŸæ‹¬å·ç±»å‹ï¼‰
            replacement = f"{bracket_open}{'ã€'.join(values)}{bracket_close}"
            # ä½¿ç”¨runç²¾ç¡®æ›¿æ¢ï¼ˆä»åå¾€å‰å¤„ç†ï¼Œä¸éœ€è¦é‡æ–°æ„å»ºæ˜ å°„ï¼‰
            success = WordDocumentUtils.apply_replacement_to_runs(
                runs, char_to_run_map, match, replacement, self.logger
            )
            if success:
                filled_count += 1
                self.logger.info(f"    ç»„åˆå­—æ®µå¡«å……: {fields} â†’ {replacement}")

        return filled_count > 0

    def fill_bracket_field(self,
                          paragraph: Paragraph,
                          text: str,
                          matches: List[Dict],
                          data: Dict[str, Any]) -> bool:
        """å¡«å……æ‹¬å·å­—æ®µï¼ˆä½¿ç”¨runç²¾ç¡®æ›¿æ¢ï¼‰"""
        if not matches:
            return False

        # æ„å»ºæ®µè½çš„runæ˜ å°„ï¼ˆåªæ„å»ºä¸€æ¬¡ï¼‰
        full_text, runs, char_to_run_map = WordDocumentUtils.build_paragraph_text_map(paragraph)
        filled_count = 0

        # ä»åå¾€å‰æ›¿æ¢ï¼ˆé¿å…ä½ç½®åç§»ï¼Œä¸éœ€è¦é‡æ–°æ„å»ºæ˜ å°„ï¼‰
        for match in reversed(matches):
            field_name = match['field']

            # ğŸ†• æ”¯æŒç®€å†™å­—æ®µï¼šå¦‚æœæ˜¯ç®€å†™ï¼Œç›´æ¥ä½¿ç”¨æä¾›çš„standard_field
            if match.get('is_abbreviation'):
                std_field = match.get('standard_field')
                self.logger.info(f"    è¯†åˆ«åˆ°ç®€å†™å­—æ®µ: {field_name} â†’ {std_field}")
            else:
                # å¸¸è§„å­—æ®µè¯†åˆ«
                std_field = self.field_recognizer.recognize_field(field_name)

            if std_field and std_field in data:
                value = str(data[std_field])

                # è·³è¿‡ç©ºå€¼ï¼ˆé¿å…å¡«å……ç©ºç™½å†…å®¹ï¼‰
                if not value or value.strip() == '':
                    self.logger.debug(f"  è·³è¿‡ç©ºå€¼å­—æ®µ: {field_name}")
                    continue

                # æ£€æµ‹å¹¶ä¿æŒåŸæ‹¬å·ç±»å‹
                full_match = match['full_match']
                if 'ï¼ˆ' in full_match:
                    bracket_open, bracket_close = 'ï¼ˆ', 'ï¼‰'
                elif '[' in full_match:
                    bracket_open, bracket_close = '[', ']'
                else:
                    bracket_open, bracket_close = '(', ')'

                # ä¿ç•™æ‹¬å·æ ¼å¼å¹¶ä¿æŒåŸæ‹¬å·ç±»å‹
                replacement = f"{bracket_open}{value}{bracket_close}"

                # ä½¿ç”¨runç²¾ç¡®æ›¿æ¢
                success = WordDocumentUtils.apply_replacement_to_runs(
                    runs, char_to_run_map, match, replacement, self.logger
                )
                if success:
                    filled_count += 1
                    self.logger.info(f"    æ‹¬å·å­—æ®µå¡«å……: {field_name} â†’ {replacement}")

        return filled_count > 0

    def fill_colon_field(self,
                        paragraph: Paragraph,
                        text: str,
                        matches: List[Dict],
                        data: Dict[str, Any]) -> bool:
        """å¡«å……å†’å·å­—æ®µï¼ˆä½¿ç”¨runç²¾ç¡®æ›¿æ¢ï¼‰

        å¢å¼ºåŠŸèƒ½ï¼š
        1. ä½¿ç”¨FieldClassifieråˆ¤æ–­æ˜¯å¦å¡«å……
        2. æ­£ç¡®å¤„ç†æ ¼å¼æ ‡è®°ï¼ˆç›–ç« ã€ç­¾å­—ç­‰ï¼‰
        3. å•ä½ç›–ç« å­—æ®µä¿ç•™æ ‡è®°ï¼Œä¸ªäººç­¾å­—å­—æ®µè·³è¿‡
        """
        if not matches:
            return False

        # æ„å»ºæ®µè½çš„runæ˜ å°„ï¼ˆåªæ„å»ºä¸€æ¬¡ï¼‰
        full_text, runs, char_to_run_map = WordDocumentUtils.build_paragraph_text_map(paragraph)
        filled_count = 0

        # ä»åå¾€å‰å¤„ç†ï¼ˆé¿å…ä½ç½®åç§»ï¼‰
        for match in reversed(matches):
            field_name = match['field']  # æ¸…ç†åçš„å­—æ®µåï¼ˆç”¨äºæ•°æ®åŒ¹é…ï¼‰
            original_field_name = match.get('original_field', field_name)  # åŸå§‹å­—æ®µåï¼ˆåŒ…å«æ‹¬å·ï¼‰
            after_colon = match.get('after_colon', '')  # å†’å·åçš„å†…å®¹ï¼ˆå¯èƒ½åŒ…å«æ ¼å¼æ ‡è®°ï¼‰
            std_field = self.field_recognizer.recognize_field(field_name)

            # âš ï¸  å…³é”®ä¿®å¤ï¼šå°†å†’å·åçš„å†…å®¹ä¹Ÿä¼ å…¥should_fillï¼Œä»¥ä¾¿æ£€æµ‹ç­¾å­—æ ‡è®°
            # ä¾‹å¦‚ï¼š"æ³•å®šä»£è¡¨äººï¼ˆè´Ÿè´£äººï¼‰æˆ–å…¶å§”æ‰˜ä»£ç†äººï¼š                 ï¼ˆç­¾å­—ï¼‰"
            # original_field_nameåªåŒ…å«å†’å·å‰çš„éƒ¨åˆ†ï¼Œafter_colonåŒ…å«"ï¼ˆç­¾å­—ï¼‰"
            full_field_text = f"{original_field_name}ï¼š{after_colon}" if after_colon else original_field_name

            # ä½¿ç”¨FieldClassifieråˆ¤æ–­æ˜¯å¦åº”è¯¥å¡«å……
            if not FieldClassifier.should_fill(full_field_text, std_field):
                # ä¸ªäººç­¾å­—å­—æ®µä¸å¡«å……
                self.logger.info(f"    è·³è¿‡ç­¾å­—å­—æ®µ: {full_field_text}")
                continue

            if std_field and std_field in data:
                value = str(data[std_field])

                # è·³è¿‡ç©ºå€¼ï¼ˆé¿å…å¡«å……ç©ºç™½å†…å®¹ï¼‰
                if not value or value.strip() == '':
                    self.logger.debug(f"  è·³è¿‡ç©ºå€¼å­—æ®µ: {original_field_name}")
                    continue

                # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯æ—¥æœŸå­—æ®µï¼Œæ ¼å¼åŒ–ä¸ºä¸­æ–‡æ ¼å¼
                if std_field == 'date':
                    value = self._format_date(value)

                # ä¿ç•™å†’å·ï¼Œç§»é™¤ä¸‹åˆ’çº¿
                colon = 'ï¼š' if 'ï¼š' in match['full_match'] else ':'

                # æå–å¹¶å¤„ç†æ ¼å¼æ ‡è®°
                format_marker = ''
                if after_colon:
                    # è°ƒè¯•æ—¥å¿—ï¼šæŸ¥çœ‹after_colonçš„å†…å®¹
                    self.logger.debug(f"  after_colonå†…å®¹: '{after_colon}'")
                    # ä»after_colonä¸­æå–æ ¼å¼æ ‡è®°
                    marker = FieldClassifier.extract_format_marker(after_colon)
                    self.logger.debug(f"  æå–çš„æ ¼å¼æ ‡è®°: '{marker}'")
                    if marker and FieldClassifier.should_preserve_marker(std_field, marker):
                        # ç¡®ä¿æ ¼å¼æ ‡è®°å‰æœ‰é€‚å½“çš„ç©ºæ ¼
                        format_marker = f"{marker}"
                        self.logger.info(f"  ä¿ç•™æ ¼å¼æ ‡è®°: {marker}")
                    else:
                        self.logger.debug(f"  ä¸ä¿ç•™æ ¼å¼æ ‡è®°ï¼Œmarker={marker}, should_preserve={FieldClassifier.should_preserve_marker(std_field, marker) if marker else False}")

                # æ£€æŸ¥åé¢æ˜¯å¦ç´§è·Ÿç€å…¶ä»–å­—æ®µï¼Œå¦‚æœæ˜¯åˆ™æ·»åŠ ç©ºæ ¼åˆ†éš”
                next_char_pos = match['end']
                trailing_space = ''
                if next_char_pos < len(full_text):
                    # æ£€æŸ¥åé¢æ˜¯å¦æœ‰æ±‰å­—ï¼ˆè¡¨ç¤ºå¯èƒ½æ˜¯ä¸‹ä¸€ä¸ªå­—æ®µï¼‰
                    next_chars = full_text[next_char_pos:next_char_pos+5]
                    if next_chars and any('\u4e00' <= c <= '\u9fff' for c in next_chars[:2]):
                        trailing_space = '  '  # æ·»åŠ ä¸¤ä¸ªç©ºæ ¼åˆ†éš”

                # æ„å»ºæ›¿æ¢æ–‡æœ¬ï¼šä¿ç•™åŸå§‹å­—æ®µåã€å†’å·ã€å€¼ã€æ ¼å¼æ ‡è®°
                replacement = f"{original_field_name}{colon}{value}{format_marker}{trailing_space}"

                # ä½¿ç”¨runç²¾ç¡®æ›¿æ¢
                success = WordDocumentUtils.apply_replacement_to_runs(
                    runs, char_to_run_map, match, replacement, self.logger
                )
                if success:
                    filled_count += 1
                    self.logger.info(f"    å†’å·å­—æ®µå¡«å……: {original_field_name} â†’ {value}{format_marker}")

        return filled_count > 0

    def fill_space_field(self,
                        paragraph: Paragraph,
                        text: str,
                        matches: List[Dict],
                        data: Dict[str, Any]) -> bool:
        """å¡«å……ç©ºæ ¼å¡«ç©ºå­—æ®µï¼ˆä½¿ç”¨runç²¾ç¡®æ›¿æ¢ï¼‰

        å¢å¼ºåŠŸèƒ½ï¼š
        1. ä½¿ç”¨FieldClassifieråˆ¤æ–­æ˜¯å¦å¡«å……
        2. æ­£ç¡®å¤„ç†æ ¼å¼æ ‡è®°ï¼ˆç›–ç« ã€ç­¾å­—ç­‰ï¼‰
        """
        if not matches:
            return False

        # æ„å»ºæ®µè½çš„runæ˜ å°„ï¼ˆåªæ„å»ºä¸€æ¬¡ï¼‰
        full_text, runs, char_to_run_map = WordDocumentUtils.build_paragraph_text_map(paragraph)
        filled_count = 0

        # ä»åå¾€å‰å¤„ç†ï¼ˆé¿å…ä½ç½®åç§»ï¼‰
        for match in reversed(matches):
            field_name = match['field']  # æ¸…ç†åçš„å­—æ®µå
            original_field_name = match.get('original_field', field_name)  # åŸå§‹å­—æ®µåï¼ˆå¯èƒ½åŒ…å«æ‹¬å·ï¼‰
            std_field = self.field_recognizer.recognize_field(field_name)

            # ä½¿ç”¨FieldClassifieråˆ¤æ–­æ˜¯å¦åº”è¯¥å¡«å……
            if not FieldClassifier.should_fill(original_field_name, std_field):
                # ä¸ªäººç­¾å­—å­—æ®µä¸å¡«å……
                self.logger.info(f"    è·³è¿‡ç­¾å­—å­—æ®µ: {original_field_name}")
                continue

            if std_field and std_field in data:
                value = str(data[std_field])

                # è·³è¿‡ç©ºå€¼ï¼ˆé¿å…å¡«å……ç©ºç™½å†…å®¹ï¼‰
                if not value or value.strip() == '':
                    self.logger.debug(f"  è·³è¿‡ç©ºå€¼å­—æ®µ: {field_name}")
                    continue

                # æå–æ ¼å¼æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰
                format_marker = FieldClassifier.extract_format_marker(original_field_name)
                if format_marker and FieldClassifier.should_preserve_marker(std_field, format_marker):
                    # å•ä½å­—æ®µä¿ç•™ç›–ç« æ ‡è®°
                    replacement = f"{field_name}{format_marker}  {value}"
                else:
                    # æ™®é€šå­—æ®µæˆ–ä¸éœ€è¦ä¿ç•™æ ‡è®°
                    replacement = f"{field_name}  {value}"

                # ä½¿ç”¨runç²¾ç¡®æ›¿æ¢
                success = WordDocumentUtils.apply_replacement_to_runs(
                    runs, char_to_run_map, match, replacement, self.logger
                )
                if success:
                    filled_count += 1
                    self.logger.info(f"    ç©ºæ ¼å¡«ç©ºå­—æ®µå¡«å……: {original_field_name} â†’ {value}")

        return filled_count > 0

    def fill_date_field(self,
                       paragraph: Paragraph,
                       text: str,
                       matches: List[Dict],
                       data: Dict[str, Any]) -> bool:
        """å¡«å……æ—¥æœŸå­—æ®µï¼ˆä½¿ç”¨runç²¾ç¡®æ›¿æ¢ï¼‰"""
        if not matches or 'date' not in data:
            return False

        # æ„å»ºæ®µè½çš„runæ˜ å°„
        full_text, runs, char_to_run_map = WordDocumentUtils.build_paragraph_text_map(paragraph)

        date_value = str(data['date'])
        formatted_date = self._format_date(date_value)

        # æ›¿æ¢æ•´ä¸ªæ—¥æœŸæ¨¡å¼
        match = matches[0]

        # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰"æ—¥æœŸï¼š"
        prefix_start = max(0, match['start'] - 10)
        prefix_text = full_text[prefix_start:match['start']]

        if 'æ—¥æœŸ' in prefix_text:
            # æ‰¾åˆ°"æ—¥æœŸï¼š"çš„ä½ç½®
            date_label_pos = full_text.rfind('æ—¥æœŸ', 0, match['start'])
            colon_pos = full_text.find('ï¼š', date_label_pos, match['start'])
            if colon_pos == -1:
                colon_pos = full_text.find(':', date_label_pos, match['start'])

            if colon_pos != -1:
                # ä»å†’å·åå¼€å§‹æ›¿æ¢æ•´ä¸ªæ—¥æœŸéƒ¨åˆ†
                replacement_match = {
                    'start': colon_pos + 1,
                    'end': match['end'],
                    'text': full_text[colon_pos+1:match['end']]
                }
                success = WordDocumentUtils.apply_replacement_to_runs(
                    runs, char_to_run_map, replacement_match, formatted_date, self.logger
                )
            else:
                # ä»"æ—¥æœŸ"åå¼€å§‹æ›¿æ¢
                replacement_match = {
                    'start': date_label_pos,
                    'end': match['end'],
                    'text': full_text[date_label_pos:match['end']]
                }
                success = WordDocumentUtils.apply_replacement_to_runs(
                    runs, char_to_run_map, replacement_match, f"æ—¥æœŸï¼š{formatted_date}", self.logger
                )
        else:
            # æ²¡æœ‰å‰ç¼€ï¼Œç›´æ¥æ›¿æ¢
            success = WordDocumentUtils.apply_replacement_to_runs(
                runs, char_to_run_map, match, formatted_date, self.logger
            )

        if success:
            self.logger.info(f"    æ—¥æœŸå¡«å……: {formatted_date}")
        return success

    def _format_date(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸ"""
        # ç§»é™¤ç©ºæ ¼
        date_str = re.sub(r'\s+', '', date_str)

        # å°è¯•åŒ¹é…å¸¸è§æ ¼å¼
        patterns = [
            (r'(\d{4})-(\d{1,2})-(\d{1,2})', r'\1å¹´\2æœˆ\3æ—¥'),
            (r'(\d{4})/(\d{1,2})/(\d{1,2})', r'\1å¹´\2æœˆ\3æ—¥'),
            (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', r'\1å¹´\2æœˆ\3æ—¥'),
        ]

        for pattern, replacement in patterns:
            if re.match(pattern, date_str):
                return re.sub(pattern, replacement, date_str)

        # å·²ç»æ˜¯ä¸­æ–‡æ ¼å¼
        if 'å¹´' in date_str and 'æœˆ' in date_str:
            # âœ… æå–"å¹´æœˆæ—¥"éƒ¨åˆ†ï¼Œåˆ é™¤åé¢çš„æ—¶é—´ä¿¡æ¯
            # åŒ¹é…æ ¼å¼ï¼š2025å¹´08æœˆ27æ—¥ä¸‹åˆ14:30æ•´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ â†’ 2025å¹´08æœˆ27æ—¥
            date_match = re.match(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', date_str)
            if date_match:
                return date_match.group(1)  # åªè¿”å›"å¹´æœˆæ—¥"éƒ¨åˆ†
            return date_str

        return date_str

    def _update_paragraph(self, paragraph: Paragraph, new_text: str):
        """æ›´æ–°æ®µè½æ–‡æœ¬ï¼Œä¿æŒæ ¼å¼"""
        if paragraph.runs:
            first_run = paragraph.runs[0]
            # ä¿å­˜æ ¼å¼
            font = first_run.font
            bold = font.bold
            italic = font.italic
            underline = font.underline
            font_size = font.size
            font_name = font.name

            # æ¸…ç©ºæ®µè½
            paragraph.clear()

            # æ·»åŠ æ–°æ–‡æœ¬
            new_run = paragraph.add_run(new_text)
            if bold is not None:
                new_run.font.bold = bold
            if italic is not None:
                new_run.font.italic = italic
            if underline is not None:
                new_run.font.underline = underline
            if font_size:
                new_run.font.size = font_size
            if font_name:
                new_run.font.name = font_name
        else:
            paragraph.text = new_text


# å‘åå…¼å®¹ï¼šæä¾›ä¸æ—§APIç›¸åŒçš„æ¥å£
class InfoFillerV2(SmartDocumentFiller):
    """InfoFiller V2 - åŸºäº SmartDocumentFiller çš„å…¼å®¹å±‚"""

    def fill_info(self, doc: Document, company_info: Dict[str, Any],
                  project_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        å…¼å®¹æ—§ç‰ˆæœ¬çš„ fill_info æ¥å£
        """
        # åˆå¹¶æ•°æ®
        data = {**company_info, **project_info}

        # è°ƒç”¨æ–°æ–¹æ³•
        stats = self.fill_document(doc, data)

        # è½¬æ¢ä¸ºæ—§æ ¼å¼
        return {
            'total_replacements': stats['total_filled'],
            'replacement_rules': stats['pattern_counts'].get('bracket', 0),
            'fill_rules': stats['pattern_counts'].get('colon', 0),
            'combination_rules': stats['pattern_counts'].get('combo', 0),
            'skipped_fields': 0,
            'none': len(stats['unfilled_fields'])
        }
