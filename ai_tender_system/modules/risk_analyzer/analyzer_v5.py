#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£é™©åˆ†æå™¨ 5.0 ç‰ˆæœ¬ - ä¸“å®¶æ¥åŠ›å¼åˆ†ææµæ°´çº¿

å®ç°æµç¨‹ï¼š
1. ç›®å½•è§£æ (æç¤ºè¯ A) â†’ è¯†åˆ«æ’é™¤ç« èŠ‚
2. æ™ºèƒ½åˆ‡ç‰‡ â†’ ç›®å½•æ„ŸçŸ¥ + æ™ºèƒ½é™çº§
3. é£é™©æå– (æç¤ºè¯ B) â†’ æå–â˜…æ¡æ¬¾å’ŒåºŸæ ‡é¡¹
4. Todo ç”Ÿæˆ (æç¤ºè¯ C) â†’ ç”Ÿæˆå…·ä½“æ“ä½œå»ºè®®
5. åŒå‘å¯¹è´¦ (æç¤ºè¯ D) â†’ æ£€æŸ¥åº”ç­”åˆè§„æ€§ï¼ˆå¯é€‰ï¼‰
"""

import json
import time
import re
from typing import List, Dict, Optional, Callable
from pathlib import Path

import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from common.logger import get_module_logger
from common.llm_client import LLMClient
from modules.document_parser.parser_manager import ParserManager

from .schemas import RiskItem, RiskAnalysisResult, TodoItem
from .prompt_manager import PromptManager, PromptType
from .toc_extractor import TocExtractor, TocResult
from .smart_chunker import SmartChunker, DocumentChunk

logger = get_module_logger("risk_analyzer.v5")


class RiskAnalyzerV5:
    """
    é£é™©åˆ†æå™¨ 5.0 ç‰ˆæœ¬

    ç‰¹æ€§ï¼š
    - ç›®å½•æ„ŸçŸ¥åˆ‡ç‰‡ï¼šæœ‰ç›®å½•æ—¶ç²¾å‡†åˆ‡ç‰‡ï¼Œæ— ç›®å½•æ—¶æ™ºèƒ½é™çº§
    - ä¸“å®¶æ¥åŠ›å¼æç¤ºè¯ï¼šAâ†’Bâ†’C æ¥åŠ›å¤„ç†
    - å¢é‡ç»“æœå›è°ƒï¼šæ¯åˆ†æå®Œä¸€ä¸ªåˆ‡ç‰‡å°±å›è°ƒ
    - å®Œæ•´çš„ 5.0 å­—æ®µæ”¯æŒ
    """

    def __init__(self, model_name: str = 'deepseek-v3', chunk_size: int = 5000):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            model_name: AI æ¨¡å‹åç§°
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        self.model_name = model_name
        self.chunk_size = chunk_size

        self.llm = LLMClient(model_name)
        self.parser = ParserManager()
        self.prompt_manager = PromptManager()
        self.toc_extractor = TocExtractor(use_ai=True, model_name=model_name)

        self.total_tokens = 0

        logger.info(f"é£é™©åˆ†æå™¨ 5.0 åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {model_name}")

    def analyze(self,
                file_path: str,
                progress_callback: Optional[Callable[[int, str], None]] = None,
                item_callback: Optional[Callable[[List['RiskItem']], None]] = None
                ) -> RiskAnalysisResult:
        """
        åˆ†ææ‹›æ ‡æ–‡ä»¶é£é™©ï¼ˆ5.0 å®Œæ•´æµç¨‹ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒ (progress: int, message: str)
            item_callback: å¢é‡ç»“æœå›è°ƒ

        Returns:
            RiskAnalysisResult: åˆ†æç»“æœ
        """
        start_time = time.time()

        try:
            # ========== Stage 1: è§£ææ–‡æ¡£ ==========
            if progress_callback:
                progress_callback(5, "æ­£åœ¨è§£ææ–‡æ¡£...")

            text = self._parse_document(file_path)

            if not text or len(text.strip()) < 100:
                raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­")

            logger.info(f"æ–‡æ¡£è§£æå®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {len(text)}")

            # ========== Stage 2: ç›®å½•è§£æ (æç¤ºè¯ A) ==========
            if progress_callback:
                progress_callback(10, "æ­£åœ¨åˆ†æç›®å½•ç»“æ„...")

            toc_result = self.toc_extractor.extract(text)
            has_toc = toc_result.has_toc

            if has_toc:
                logger.info(f"ç›®å½•è§£ææˆåŠŸï¼Œå‘ç° {len(toc_result.entries)} ä¸ªç« èŠ‚ï¼Œ"
                           f"æ’é™¤ {len(toc_result.exclude_chapters)} ä¸ªåˆåŒç« èŠ‚")
            else:
                logger.info("æœªè¯†åˆ«åˆ°ç›®å½•ç»“æ„ï¼Œå°†ä½¿ç”¨æ™ºèƒ½é™çº§ç­–ç•¥")

            # ========== Stage 3: æ™ºèƒ½åˆ‡ç‰‡ ==========
            if progress_callback:
                progress_callback(15, "æ­£åœ¨æ™ºèƒ½åˆ‡ç‰‡...")

            chunker = SmartChunker(toc_result=toc_result, chunk_size=self.chunk_size)
            chunks = chunker.chunk_document(text)
            total_chunks = len(chunks)

            logger.info(f"æ™ºèƒ½åˆ‡ç‰‡å®Œæˆï¼Œå…± {total_chunks} ä¸ªåˆ‡ç‰‡")

            # ========== Stage 4: é£é™©æå– (æç¤ºè¯ B) ==========
            if progress_callback:
                progress_callback(20, "æ­£åœ¨æå–é£é™©é¡¹...")

            all_risk_items = []

            for i, chunk in enumerate(chunks):
                progress = 20 + int((i + 1) / total_chunks * 55)

                if progress_callback:
                    progress_callback(progress, f"æ­£åœ¨åˆ†æ: {chunk.title}")

                try:
                    items = self._analyze_chunk(chunk, has_toc=has_toc, chunk_index=i)
                    all_risk_items.extend(items)

                    logger.debug(f"åˆ‡ç‰‡ {i+1}/{total_chunks} ({chunk.title}) "
                                f"å‘ç° {len(items)} ä¸ªé£é™©é¡¹")

                    # å¢é‡å›è°ƒ
                    if item_callback and items:
                        item_callback(items)

                except Exception as e:
                    logger.warning(f"åˆ†æåˆ‡ç‰‡ {i+1} å¤±è´¥: {e}")
                    continue

            # ========== Stage 5: Todo ç”Ÿæˆ (æç¤ºè¯ C) ==========
            if progress_callback:
                progress_callback(80, "æ­£åœ¨ç”Ÿæˆæ“ä½œå»ºè®®...")

            if all_risk_items:
                todos = self._generate_todos(all_risk_items)
                # å°† todo åˆå¹¶åˆ°å¯¹åº”çš„ risk_item
                self._merge_todos(all_risk_items, todos)

            # ========== Stage 6: åˆå¹¶å»é‡ ==========
            if progress_callback:
                progress_callback(90, "æ­£åœ¨æ•´ç†ç»“æœ...")

            unique_items = self._deduplicate_items(all_risk_items)

            # ========== Stage 7: ç”Ÿæˆæ€»ç»“å’Œè¯„åˆ† ==========
            summary = self._generate_summary(unique_items)
            risk_score = self._calculate_risk_score(unique_items)

            analysis_time = time.time() - start_time

            if progress_callback:
                progress_callback(100, "åˆ†æå®Œæˆ")

            result = RiskAnalysisResult(
                risk_items=unique_items,
                summary=summary,
                risk_score=risk_score,
                total_chunks=total_chunks,
                analyzed_chunks=total_chunks,
                model_name=self.model_name,
                total_tokens=self.total_tokens,
                analysis_time=analysis_time,
                has_toc=has_toc,
                exclude_chapters=toc_result.exclude_chapters if has_toc else []
            )

            logger.info(f"é£é™©åˆ†æ 5.0 å®Œæˆï¼Œå‘ç° {len(unique_items)} ä¸ªé£é™©é¡¹ï¼Œ"
                       f"è€—æ—¶ {analysis_time:.2f}s")

            return result

        except Exception as e:
            logger.error(f"é£é™©åˆ†æå¤±è´¥: {e}")
            raise

    def _parse_document(self, file_path: str) -> str:
        """è§£ææ–‡æ¡£"""
        path = Path(file_path)
        if not path.is_absolute():
            from common.config import get_config
            config = get_config()
            base_dir = config.get_path('base')
            path = Path(base_dir) / file_path

        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")

        return self.parser.parse_document_simple(str(path))

    def _analyze_chunk(self,
                       chunk: DocumentChunk,
                       has_toc: bool,
                       chunk_index: int) -> List[RiskItem]:
        """åˆ†æå•ä¸ªåˆ‡ç‰‡ï¼ˆä½¿ç”¨æç¤ºè¯ Bï¼‰"""
        if not chunk.content or len(chunk.content.strip()) < 50:
            return []

        # è·å–æç¤ºè¯é…ç½®
        prompt = self.prompt_manager.get_prompt(
            PromptType.BID_EVALUATOR,
            chapter_title=chunk.title,
            chapter_content=chunk.content,
            has_toc=has_toc
        )

        config = self.prompt_manager.get_config(PromptType.BID_EVALUATOR)

        try:
            response = self.llm.call(
                prompt=prompt,
                system_prompt=config['system_prompt'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                purpose=config['purpose']
            )

            # è§£æå“åº”
            items_data = self._parse_json_response(response)

            if not isinstance(items_data, dict) or 'items' not in items_data:
                if isinstance(items_data, list):
                    items_list = items_data
                else:
                    return []
            else:
                items_list = items_data.get('items', [])

            # è½¬æ¢ä¸º RiskItem å¯¹è±¡
            return [
                RiskItem(
                    location=item.get('location', ''),
                    requirement=item.get('requirement', ''),
                    suggestion=item.get('suggestion', self._get_default_suggestion(item)),
                    risk_level=item.get('risk_level', 'medium'),
                    risk_type=item.get('risk_type', ''),
                    source_chunk=chunk_index,
                    # 5.0 æ–°å¢å­—æ®µ
                    original_text=item.get('original_text', ''),
                    position_index=item.get('position_index', ''),
                    deep_analysis=item.get('deep_analysis', '')
                )
                for item in items_list
                if item.get('requirement')
            ]

        except Exception as e:
            logger.error(f"åˆ‡ç‰‡åˆ†æå¤±è´¥: {e}")
            return []

    def _generate_todos(self, risk_items: List[RiskItem]) -> List[TodoItem]:
        """ç”Ÿæˆ Todo æ“ä½œï¼ˆä½¿ç”¨æç¤ºè¯ Cï¼‰"""
        if not risk_items:
            return []

        # å‡†å¤‡é£é™©é¡¹ JSON
        risk_items_json = json.dumps(
            [item.to_dict() for item in risk_items[:20]],  # é™åˆ¶æ•°é‡
            ensure_ascii=False,
            indent=2
        )

        prompt = self.prompt_manager.get_prompt(
            PromptType.TODO_GENERATOR,
            risk_items_json=risk_items_json
        )

        config = self.prompt_manager.get_config(PromptType.TODO_GENERATOR)

        try:
            response = self.llm.call(
                prompt=prompt,
                system_prompt=config['system_prompt'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens'],
                purpose=config['purpose']
            )

            data = self._parse_json_response(response)
            todos_data = data.get('todos', []) if isinstance(data, dict) else []

            return [TodoItem.from_dict(t) for t in todos_data]

        except Exception as e:
            logger.warning(f"Todo ç”Ÿæˆå¤±è´¥: {e}")
            return []

    def _merge_todos(self, risk_items: List[RiskItem], todos: List[TodoItem]):
        """å°† Todo åˆå¹¶åˆ°å¯¹åº”çš„ RiskItem"""
        for todo in todos:
            idx = todo.risk_item_index
            if 0 <= idx < len(risk_items):
                risk_items[idx].todo_action = todo.action

    def _get_default_suggestion(self, item: Dict) -> str:
        """ç”Ÿæˆé»˜è®¤çš„é¿å‘å»ºè®®"""
        risk_type = item.get('risk_type', '')
        if 'åºŸæ ‡' in risk_type or 'â˜…' in risk_type:
            return "åŠ¡å¿…æ»¡è¶³æ­¤æ¡æ¬¾è¦æ±‚ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´åºŸæ ‡"
        elif 'èµ„è´¨' in risk_type:
            return "ç¡®è®¤èµ„è´¨è¯ä¹¦æœ‰æ•ˆæœŸï¼Œå‡†å¤‡å½©è‰²æ‰«æä»¶"
        elif 'æŠ€æœ¯' in risk_type:
            return "ä»”ç»†æ ¸å¯¹æŠ€æœ¯å‚æ•°ï¼Œç¡®ä¿æ»¡è¶³æ‰€æœ‰æŒ‡æ ‡è¦æ±‚"
        else:
            return "ä»”ç»†é˜…è¯»è¦æ±‚ï¼Œç¡®ä¿å®Œæ•´å“åº”"

    def _parse_json_response(self, response: str) -> Dict:
        """è§£æ AI å“åº”ä¸­çš„ JSON"""
        if not response or not response.strip():
            return {}

        # æ¸…ç† markdown ä»£ç å—
        response = re.sub(r'^\s*```json\s*', '', response.strip())
        response = re.sub(r'\s*```\s*$', '', response.strip())

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # å°è¯•æå– JSON å¯¹è±¡
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass

        # å°è¯•æå– JSON æ•°ç»„
        json_match = re.search(r'\[[\s\S]*\]', response)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass

        return {}

    def _deduplicate_items(self, items: List[RiskItem]) -> List[RiskItem]:
        """å»é‡é£é™©é¡¹"""
        seen = set()
        unique = []

        for item in items:
            # ä½¿ç”¨ requirement çš„å‰80å­—ç¬¦ä½œä¸ºå»é‡é”®
            key = item.requirement[:80] if len(item.requirement) > 80 else item.requirement
            key = key.strip().lower()

            if key and key not in seen:
                seen.add(key)
                unique.append(item)

        # æŒ‰é£é™©ç­‰çº§æ’åº
        level_order = {'high': 0, 'medium': 1, 'low': 2}
        unique.sort(key=lambda x: level_order.get(x.risk_level, 1))

        return unique

    def _generate_summary(self, items: List[RiskItem]) -> str:
        """ç”Ÿæˆé£é™©æ€»ç»“"""
        if not items:
            return "æœªå‘ç°æ˜æ˜¾çš„åºŸæ ‡é£é™©é¡¹ã€‚å»ºè®®ä»”ç»†é˜…è¯»æ‹›æ ‡æ–‡ä»¶ï¼Œç¡®ä¿æ»¡è¶³æ‰€æœ‰è¦æ±‚ã€‚"

        high_count = sum(1 for item in items if item.risk_level == 'high')
        medium_count = sum(1 for item in items if item.risk_level == 'medium')
        low_count = sum(1 for item in items if item.risk_level == 'low')

        summary_parts = [f"å…±å‘ç° {len(items)} ä¸ªé£é™©é¡¹"]

        if high_count > 0:
            summary_parts.append(f"ğŸ”´é«˜é£é™© {high_count} é¡¹")
        if medium_count > 0:
            summary_parts.append(f"ğŸŸ¡ä¸­é£é™© {medium_count} é¡¹")
        if low_count > 0:
            summary_parts.append(f"ğŸ”µä½é£é™© {low_count} é¡¹")

        summary = "ï¼Œ".join(summary_parts) + "ã€‚"

        if high_count > 0:
            summary += "è¯·é‡ç‚¹å…³æ³¨é«˜é£é™©æ¡æ¬¾ï¼Œè¿™äº›å¯èƒ½ç›´æ¥å¯¼è‡´åºŸæ ‡ï¼"
        else:
            summary += "å»ºè®®é€é¡¹æ ¸å¯¹ï¼Œç¡®ä¿æŠ•æ ‡æ–‡ä»¶æ»¡è¶³æ‰€æœ‰è¦æ±‚ã€‚"

        return summary

    def _calculate_risk_score(self, items: List[RiskItem]) -> int:
        """è®¡ç®—é£é™©è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        if not items:
            return 0

        score = 0
        for item in items:
            if item.risk_level == 'high':
                score += 15
            elif item.risk_level == 'medium':
                score += 8
            else:
                score += 3

        return min(100, score)


# ä¾¿æ·å‡½æ•°
def analyze_document_v5(file_path: str,
                        model_name: str = 'deepseek-v3',
                        progress_callback: Optional[Callable[[int, str], None]] = None
                        ) -> RiskAnalysisResult:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨ 5.0 ç‰ˆæœ¬åˆ†ææ–‡æ¡£é£é™©

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        model_name: AI æ¨¡å‹åç§°
        progress_callback: è¿›åº¦å›è°ƒ

    Returns:
        RiskAnalysisResult
    """
    analyzer = RiskAnalyzerV5(model_name=model_name)
    return analyzer.analyze(file_path, progress_callback)
