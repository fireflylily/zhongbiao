#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒå‘å¯¹è´¦å¼•æ“ - æ£€æŸ¥æŠ•æ ‡åº”ç­”æ˜¯å¦åˆè§„
ä½¿ç”¨æç¤ºè¯ Dï¼ˆåˆè§„å®¡è®¡æ³•åŠ¡ï¼‰è¿›è¡Œå¯¹è´¦
"""

import json
import re
from typing import List, Dict, Optional, Callable
from pathlib import Path

import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from common.logger import get_module_logger
from common.llm_client import LLMClient
from modules.document_parser.parser_manager import ParserManager

from .schemas import RiskItem, ReconcileResult
from .prompt_manager import PromptManager, PromptType

logger = get_module_logger("risk_analyzer.reconciler")


class Reconciler:
    """
    åŒå‘å¯¹è´¦å¼•æ“

    å¯¹ç…§æ‹›æ ‡æ–‡ä»¶è¦æ±‚æ£€æŸ¥æŠ•æ ‡åº”ç­”çš„ç¬¦åˆæ€§
    """

    def __init__(self, model_name: str = 'deepseek-v3'):
        """
        åˆå§‹åŒ–å¯¹è´¦å¼•æ“

        Args:
            model_name: AI æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.llm = LLMClient(model_name)
        self.parser = ParserManager()
        self.prompt_manager = PromptManager()

        logger.info(f"åŒå‘å¯¹è´¦å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {model_name}")

    def reconcile(self,
                  risk_items: List[RiskItem],
                  response_file_path: str,
                  progress_callback: Optional[Callable[[int, str], None]] = None
                  ) -> List[ReconcileResult]:
        """
        æ‰§è¡ŒåŒå‘å¯¹è´¦

        Args:
            risk_items: é£é™©é¡¹åˆ—è¡¨
            response_file_path: åº”ç­”æ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒ

        Returns:
            å¯¹è´¦ç»“æœåˆ—è¡¨
        """
        if not risk_items:
            logger.warning("æ²¡æœ‰é£é™©é¡¹éœ€è¦å¯¹è´¦")
            return []

        # 1. è§£æåº”ç­”æ–‡ä»¶
        if progress_callback:
            progress_callback(5, "æ­£åœ¨è§£æåº”ç­”æ–‡ä»¶...")

        response_text = self._parse_document(response_file_path)

        if not response_text or len(response_text.strip()) < 100:
            raise ValueError("åº”ç­”æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­")

        logger.info(f"åº”ç­”æ–‡ä»¶è§£æå®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {len(response_text)}")

        # 2. é€é¡¹å¯¹è´¦
        results = []
        total_items = len(risk_items)

        for i, risk_item in enumerate(risk_items):
            progress = 10 + int((i + 1) / total_items * 85)

            if progress_callback:
                progress_callback(progress, f"æ­£åœ¨å¯¹è´¦ç¬¬ {i+1}/{total_items} é¡¹...")

            try:
                result = self._reconcile_item(risk_item, response_text, i)
                results.append(result)

                # æ›´æ–°é£é™©é¡¹çš„åˆè§„çŠ¶æ€
                risk_item.compliance_status = result.compliance_status
                risk_item.compliance_note = result.overall_assessment
                risk_item.match_score = result.match_score
                risk_item.response_text = result.response_content[:500]  # é™åˆ¶é•¿åº¦

                logger.debug(f"é£é™©é¡¹ {i+1} å¯¹è´¦å®Œæˆ: {result.compliance_status}")

            except Exception as e:
                logger.warning(f"å¯¹è´¦ç¬¬ {i+1} é¡¹å¤±è´¥: {e}")
                results.append(ReconcileResult(
                    risk_item_id=i,
                    compliance_status='unknown',
                    overall_assessment=f"å¯¹è´¦å¤±è´¥: {str(e)}"
                ))

        if progress_callback:
            progress_callback(100, "å¯¹è´¦å®Œæˆ")

        # 3. ç”Ÿæˆå¯¹è´¦æ±‡æ€»
        self._log_summary(results)

        return results

    def _parse_document(self, file_path: str) -> str:
        """è§£ææ–‡æ¡£"""
        path = Path(file_path)
        if not path.is_absolute():
            from common.config import get_config
            config = get_config()
            base_dir = config.get_path('base')
            path = Path(base_dir) / file_path

        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")

        return self.parser.parse_document_simple(str(path))

    def _reconcile_item(self,
                        risk_item: RiskItem,
                        response_text: str,
                        item_index: int) -> ReconcileResult:
        """å¯¹è´¦å•ä¸ªé£é™©é¡¹"""
        # 1. åœ¨åº”ç­”æ–‡ä»¶ä¸­æŸ¥æ‰¾ç›¸å…³å†…å®¹
        related_content = self._find_related_content(risk_item, response_text)

        # 2. æ„å»ºæç¤ºè¯
        prompt = self.prompt_manager.get_prompt(
            PromptType.COMPLIANCE_AUDITOR,
            bid_requirement=self._format_requirement(risk_item),
            response_content=related_content
        )

        config = self.prompt_manager.get_config(PromptType.COMPLIANCE_AUDITOR)

        # 3. è°ƒç”¨ AI
        response = self.llm.call(
            prompt=prompt,
            system_prompt=config['system_prompt'],
            temperature=config['temperature'],
            max_tokens=config['max_tokens'],
            purpose=config['purpose']
        )

        # 4. è§£æç»“æœ
        return self._parse_reconcile_response(response, item_index, related_content, risk_item)

    def _find_related_content(self, risk_item: RiskItem, response_text: str) -> str:
        """åœ¨åº”ç­”æ–‡ä»¶ä¸­æŸ¥æ‰¾ä¸é£é™©é¡¹ç›¸å…³çš„å†…å®¹"""
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(risk_item)

        if not keywords:
            return "(æœªæ‰¾åˆ°ç›¸å…³å†…å®¹)"

        # æœç´¢åŒ…å«å…³é”®è¯çš„æ®µè½
        paragraphs = response_text.split('\n\n')
        related = []
        scores = []

        for para in paragraphs:
            para = para.strip()
            if len(para) < 20:
                continue

            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = 0
            for kw in keywords:
                if kw in para:
                    score += 1

            if score > 0:
                related.append((score, para))

        # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰3ä¸ªæœ€ç›¸å…³çš„æ®µè½
        related.sort(key=lambda x: x[0], reverse=True)
        top_related = [para for score, para in related[:3]]

        if top_related:
            return '\n\n'.join(top_related)

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„æœç´¢
        return self._fuzzy_search(risk_item, response_text)

    def _extract_keywords(self, risk_item: RiskItem) -> List[str]:
        """æå–é£é™©é¡¹çš„å…³é”®è¯"""
        keywords = []

        # ä» requirement æå–
        text = risk_item.requirement + ' ' + risk_item.original_text

        # æå–ä¸­æ–‡è¯è¯­
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,8}', text)

        # è¿‡æ»¤å¸¸è§è¯
        stop_words = {'åº”å½“', 'å¿…é¡»', 'éœ€è¦', 'è¦æ±‚', 'æä¾›', 'å…·æœ‰', 'æ»¡è¶³', 'ç¬¦åˆ',
                      'æ‹›æ ‡', 'æŠ•æ ‡', 'æ–‡ä»¶', 'å•ä½', 'é¡¹ç›®', 'å†…å®¹', 'è¿›è¡Œ', 'ç›¸å…³'}

        keywords = [w for w in chinese_words if w not in stop_words and len(w) >= 2]

        # å»é‡å¹¶é™åˆ¶æ•°é‡
        seen = set()
        unique_keywords = []
        for kw in keywords:
            if kw not in seen:
                seen.add(kw)
                unique_keywords.append(kw)
                if len(unique_keywords) >= 10:
                    break

        return unique_keywords

    def _fuzzy_search(self, risk_item: RiskItem, response_text: str) -> str:
        """æ¨¡ç³Šæœç´¢ç›¸å…³å†…å®¹"""
        # ä½¿ç”¨é£é™©ç±»å‹ç›¸å…³çš„å…³é”®è¯
        risk_type = risk_item.risk_type

        if 'èµ„è´¨' in risk_type:
            search_terms = ['èµ„è´¨', 'è¯ä¹¦', 'è®¸å¯', 'è®¤è¯', 'èµ„æ ¼']
        elif 'æŠ€æœ¯' in risk_type:
            search_terms = ['æŠ€æœ¯', 'å‚æ•°', 'è§„æ ¼', 'æ€§èƒ½', 'æŒ‡æ ‡']
        elif 'å•†åŠ¡' in risk_type:
            search_terms = ['ä»·æ ¼', 'æŠ¥ä»·', 'ä»˜æ¬¾', 'äº¤ä»˜', 'è´¨ä¿']
        else:
            search_terms = ['å“åº”', 'æ‰¿è¯º', 'å£°æ˜', 'è¯´æ˜']

        paragraphs = response_text.split('\n\n')
        for para in paragraphs:
            if any(term in para for term in search_terms):
                return para[:1000]

        return "(æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·äººå·¥æ ¸æŸ¥)"

    def _format_requirement(self, risk_item: RiskItem) -> str:
        """æ ¼å¼åŒ–æ‹›æ ‡è¦æ±‚"""
        parts = []

        if risk_item.risk_type:
            parts.append(f"ã€{risk_item.risk_type}ã€‘")

        if risk_item.location:
            parts.append(f"ä½ç½®: {risk_item.location}")

        parts.append(f"è¦æ±‚: {risk_item.requirement}")

        if risk_item.original_text:
            parts.append(f"åŸæ–‡: {risk_item.original_text[:300]}")

        return '\n'.join(parts)

    def _parse_reconcile_response(self,
                                   response: str,
                                   item_index: int,
                                   response_content: str,
                                   risk_item: RiskItem) -> ReconcileResult:
        """è§£æå¯¹è´¦å“åº”"""
        if not response:
            return ReconcileResult(
                risk_item_id=item_index,
                compliance_status='unknown',
                overall_assessment="AI å“åº”ä¸ºç©º"
            )

        # æ¸…ç† markdown ä»£ç å—
        response = re.sub(r'^\s*```json\s*', '', response.strip())
        response = re.sub(r'\s*```\s*$', '', response.strip())

        try:
            data = json.loads(response)

            return ReconcileResult(
                risk_item_id=item_index,
                bid_requirement=risk_item.requirement,
                response_content=response_content[:500],
                compliance_status=data.get('compliance_status', 'unknown'),
                match_score=data.get('match_score', 0.0),
                issues=data.get('issues', []),
                overall_assessment=data.get('overall_assessment', ''),
                fix_suggestion=self._extract_fix_suggestion(data),
                fix_priority=data.get('fix_priority', 'normal')
            )

        except json.JSONDecodeError:
            logger.warning("å¯¹è´¦å“åº” JSON è§£æå¤±è´¥")
            return ReconcileResult(
                risk_item_id=item_index,
                compliance_status='unknown',
                overall_assessment="å“åº”è§£æå¤±è´¥"
            )

    def _extract_fix_suggestion(self, data: Dict) -> str:
        """æå–ä¿®å¤å»ºè®®"""
        issues = data.get('issues', [])
        if issues:
            suggestions = [issue.get('suggestion', '') for issue in issues if issue.get('suggestion')]
            return '; '.join(suggestions[:3])
        return ''

    def _log_summary(self, results: List[ReconcileResult]):
        """è®°å½•å¯¹è´¦æ±‡æ€»"""
        total = len(results)
        compliant = sum(1 for r in results if r.compliance_status == 'compliant')
        non_compliant = sum(1 for r in results if r.compliance_status == 'non_compliant')
        partial = sum(1 for r in results if r.compliance_status == 'partial')

        logger.info(f"åŒå‘å¯¹è´¦å®Œæˆ: å…± {total} é¡¹, "
                   f"ğŸŸ¢é€šè¿‡ {compliant}, ğŸ”´ä¸é€šè¿‡ {non_compliant}, ğŸŸ¡éƒ¨åˆ†ç¬¦åˆ {partial}")


# ä¾¿æ·å‡½æ•°
def reconcile_response(risk_items: List[RiskItem],
                       response_file_path: str,
                       model_name: str = 'deepseek-v3',
                       progress_callback: Optional[Callable[[int, str], None]] = None
                       ) -> List[ReconcileResult]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰§è¡ŒåŒå‘å¯¹è´¦

    Args:
        risk_items: é£é™©é¡¹åˆ—è¡¨
        response_file_path: åº”ç­”æ–‡ä»¶è·¯å¾„
        model_name: AI æ¨¡å‹åç§°
        progress_callback: è¿›åº¦å›è°ƒ

    Returns:
        å¯¹è´¦ç»“æœåˆ—è¡¨
    """
    reconciler = Reconciler(model_name=model_name)
    return reconciler.reconcile(risk_items, response_file_path, progress_callback)
