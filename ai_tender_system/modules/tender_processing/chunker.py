#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£æ™ºèƒ½åˆ†å—å¤„ç†å™¨
åŠŸèƒ½ï¼š
- è¯†åˆ«ç« èŠ‚ç»“æ„ï¼ˆæ ‡é¢˜å±‚çº§ï¼‰
- æå–è¡¨æ ¼å†…å®¹
- æ™ºèƒ½åˆ†å—ï¼ˆæŒ‰è¯­ä¹‰ã€å¤§å°ï¼‰
"""

import re
import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
import tiktoken

from common import get_module_logger

logger = get_module_logger("document_chunker")


@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—æ•°æ®ç±»"""
    chunk_index: int
    chunk_type: str  # title/paragraph/table/list
    content: str
    metadata: Dict

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'chunk_index': self.chunk_index,
            'chunk_type': self.chunk_type,
            'content': self.content,
            'metadata': self.metadata
        }


class DocumentChunker:
    """æ–‡æ¡£æ™ºèƒ½åˆ†å—å¤„ç†å™¨"""

    def __init__(self, max_chunk_size: int = 800, overlap_size: int = 100):
        """
        åˆå§‹åŒ–åˆ†å—å¤„ç†å™¨

        Args:
            max_chunk_size: æœ€å¤§åˆ†å—å¤§å°ï¼ˆtokenæ•°ï¼‰
            overlap_size: åˆ†å—é‡å å¤§å°ï¼ˆtokenæ•°ï¼‰
        """
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size

        # åˆå§‹åŒ–tokenizerï¼ˆç”¨äºè®¡ç®—tokenæ•°ï¼‰
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4ä½¿ç”¨çš„ç¼–ç 
        except Exception as e:
            logger.warning(f"åˆå§‹åŒ–tokenizerå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦è®¡æ•°: {e}")
            self.tokenizer = None

        # æ ‡é¢˜è¯†åˆ«æ¨¡å¼
        self.title_patterns = [
            # ä¸€çº§æ ‡é¢˜ï¼šç¬¬ä¸€ç« ã€ç¬¬1ç« ã€ä¸€ã€1ã€1.ã€ï¼ˆä¸€ï¼‰ç­‰
            r'^(?:ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« |ç¬¬\d+ç« |[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|\d+ã€|\d+\.|ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰)',
            # äºŒçº§æ ‡é¢˜ï¼š1.1ã€1.1.ã€ï¼ˆ1ï¼‰ç­‰
            r'^\d+\.\d+\.?|^ï¼ˆ\d+ï¼‰',
            # ä¸‰çº§æ ‡é¢˜ï¼š1.1.1ã€ï¼ˆä¸€ï¼‰ç­‰
            r'^\d+\.\d+\.\d+\.?|^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',
        ]

        # è¡¨æ ¼è¯†åˆ«æ¨¡å¼
        self.table_indicators = [
            'â”Œ', 'â”', 'â””', 'â”˜', 'â”œ', 'â”¤', 'â”¬', 'â”´', 'â”¼',  # è¡¨æ ¼è¾¹æ¡†å­—ç¬¦
            'â”‚', 'â”€', 'â•­', 'â•®', 'â•°', 'â•¯',
        ]

        # ç« èŠ‚é‡è¦æ€§å…³é”®è¯ï¼ˆç”¨äºåŸºäºç›®å½•çš„è¿‡æ»¤ï¼‰
        # éœ€è¦æå–çš„ç« èŠ‚å…³é”®è¯
        self.RELEVANT_SECTION_KEYWORDS = [
            # æŠ•æ ‡è¦æ±‚ç±»
            "æŠ•æ ‡é¡»çŸ¥", "ä¾›åº”å•†é¡»çŸ¥", "æŠ•æ ‡äººé¡»çŸ¥", "èµ„æ ¼è¦æ±‚", "èµ„è´¨è¦æ±‚",
            "æŠ•æ ‡é‚€è¯·", "æ‹›æ ‡å…¬å‘Š", "é¡¹ç›®æ¦‚å†µ",
            # æŠ€æœ¯è¦æ±‚ç±»
            "æŠ€æœ¯è¦æ±‚", "æŠ€æœ¯è§„æ ¼", "æŠ€æœ¯å‚æ•°", "æ€§èƒ½æŒ‡æ ‡", "é¡¹ç›®éœ€æ±‚",
            "éœ€æ±‚è¯´æ˜", "æŠ€æœ¯æ ‡å‡†", "åŠŸèƒ½è¦æ±‚", "æŠ€æœ¯è§„èŒƒ", "æŠ€æœ¯æ–¹æ¡ˆ",
            # å•†åŠ¡è¦æ±‚ç±»
            "å•†åŠ¡è¦æ±‚", "å•†åŠ¡æ¡æ¬¾", "ä»˜æ¬¾æ–¹å¼", "äº¤ä»˜è¦æ±‚", "è´¨ä¿è¦æ±‚",
            "ä»·æ ¼è¦æ±‚", "æŠ¥ä»·è¦æ±‚",
            # è¯„åˆ†æ ‡å‡†ç±»
            "è¯„åˆ†æ ‡å‡†", "è¯„æ ‡åŠæ³•", "è¯„åˆ†ç»†åˆ™", "æ‰“åˆ†æ ‡å‡†", "ç»¼åˆè¯„åˆ†",
            "è¯„å®¡æ ‡å‡†", "è¯„å®¡åŠæ³•",
        ]

        # æ— éœ€æå–çš„ç« èŠ‚å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        self.SKIP_SECTION_KEYWORDS = [
            # åˆåŒç±»
            "åˆåŒæ¡æ¬¾", "åˆåŒæ–‡æœ¬", "åˆåŒèŒƒæœ¬", "åˆåŒæ ¼å¼", "åˆåŒåè®®",
            "é€šç”¨æ¡æ¬¾", "ä¸“ç”¨æ¡æ¬¾", "åˆåŒä¸»è¦æ¡æ¬¾", "åˆåŒè‰ç¨¿", "æ‹Ÿç­¾åˆåŒ",
            # æ ¼å¼ç±»
            "æŠ•æ ‡æ–‡ä»¶æ ¼å¼", "æ–‡ä»¶æ ¼å¼", "æ ¼å¼è¦æ±‚", "ç¼–åˆ¶è¦æ±‚", "å°è£…è¦æ±‚",
            "å“åº”æ–‡ä»¶æ ¼å¼", "èµ„æ–™æ¸…å•", "åŒ…è£…è¦æ±‚", "å¯†å°è¦æ±‚",
            # æ³•å¾‹å£°æ˜ç±»
            "æ³•å¾‹å£°æ˜", "å…è´£å£°æ˜", "æŠ•æ ‡æ‰¿è¯º", "å»‰æ”¿æ‰¿è¯º", "ä¿å¯†åè®®",
            "è¯šä¿¡æ‰¿è¯º", "å£°æ˜å‡½",
            # é™„ä»¶ç±»
            "é™„ä»¶", "é™„è¡¨", "é™„å½•", "æ ·è¡¨", "æ¨¡æ¿", "æ ¼å¼èŒƒæœ¬", "ç©ºç™½è¡¨æ ¼",
        ]

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except (AttributeError, TypeError, ValueError):
                pass  # Tokenizerå¤±è´¥æ—¶ä½¿ç”¨ä¼°ç®—æ–¹æ³•
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1å­—çº¦1.5 tokenï¼Œè‹±æ–‡1è¯çº¦1 token
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        return int(chinese_chars * 1.5 + english_words)

    def detect_chunk_type(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬å—ç±»å‹

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            chunk_type: title/paragraph/table/list
        """
        text_stripped = text.strip()

        # æ£€æµ‹æ ‡é¢˜
        for pattern in self.title_patterns:
            if re.match(pattern, text_stripped):
                return 'title'

        # æ£€æµ‹è¡¨æ ¼
        if any(indicator in text_stripped for indicator in self.table_indicators):
            return 'table'

        # æ£€æµ‹åˆ—è¡¨ï¼ˆå¤šä¸ªé¡¹ç›®ç¬¦å·æˆ–ç¼–å·ï¼‰
        list_patterns = [
            r'^[â€¢Â·â—‹â—]\s',  # é¡¹ç›®ç¬¦å·
            r'^\d+[\.ã€ï¼‰]\s',  # æ•°å­—ç¼–å·
            r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s',  # åœ†åœˆæ•°å­—
            r'^[ï¼ˆ\(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ï¼‰\)]\s',  # æ‹¬å·ç¼–å·
        ]
        list_items = sum(1 for line in text_stripped.split('\n')
                        if any(re.match(p, line.strip()) for p in list_patterns))
        if list_items >= 2:
            return 'list'

        return 'paragraph'

    def extract_sections(self, text: str) -> List[Dict]:
        """
        æå–æ–‡æ¡£ç« èŠ‚ç»“æ„

        Args:
            text: æ–‡æ¡£å…¨æ–‡

        Returns:
            sections: ç« èŠ‚åˆ—è¡¨ï¼Œæ¯ä¸ªç« èŠ‚åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€å±‚çº§ç­‰ä¿¡æ¯
        """
        sections = []
        lines = text.split('\n')

        current_section = None
        current_content = []
        current_level = 0

        for line_num, line in enumerate(lines, 1):
            stripped = line.strip()
            if not stripped:
                if current_content:
                    current_content.append(line)
                continue

            # æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜
            title_level = self._detect_title_level(stripped)

            if title_level > 0:
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section:
                    current_section['content'] = '\n'.join(current_content)
                    current_section['end_line'] = line_num - 1
                    sections.append(current_section)

                # å¼€å§‹æ–°ç« èŠ‚
                current_section = {
                    'title': stripped,
                    'level': title_level,
                    'start_line': line_num,
                    'content': ''
                }
                current_content = []
                current_level = title_level
            else:
                # å†…å®¹è¡Œ
                current_content.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            current_section['content'] = '\n'.join(current_content)
            current_section['end_line'] = len(lines)
            sections.append(current_section)

        logger.info(f"è¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚")
        return sections

    def _detect_title_level(self, text: str) -> int:
        """
        æ£€æµ‹æ ‡é¢˜å±‚çº§

        Returns:
            0: ä¸æ˜¯æ ‡é¢˜
            1-3: æ ‡é¢˜å±‚çº§
        """
        for level, pattern in enumerate(self.title_patterns, 1):
            if re.match(pattern, text):
                return level
        return 0

    def extract_tables(self, text: str) -> List[Dict]:
        """
        æå–æ–‡æ¡£ä¸­çš„è¡¨æ ¼

        Args:
            text: æ–‡æ¡£å†…å®¹

        Returns:
            tables: è¡¨æ ¼åˆ—è¡¨
        """
        tables = []
        lines = text.split('\n')

        in_table = False
        table_lines = []
        table_start = 0

        for line_num, line in enumerate(lines, 1):
            # æ£€æµ‹è¡¨æ ¼å¼€å§‹
            if any(indicator in line for indicator in self.table_indicators):
                if not in_table:
                    in_table = True
                    table_start = line_num
                    table_lines = [line]
                else:
                    table_lines.append(line)
            elif in_table:
                # æ£€æµ‹è¡¨æ ¼ç»“æŸï¼ˆè¿ç»­2è¡Œæ— è¡¨æ ¼å­—ç¬¦ï¼‰
                if table_lines and len(table_lines) >= 2:
                    # ä¿å­˜è¡¨æ ¼
                    tables.append({
                        'content': '\n'.join(table_lines),
                        'start_line': table_start,
                        'end_line': line_num - 1,
                        'rows': len([l for l in table_lines if l.strip()])
                    })

                in_table = False
                table_lines = []

        logger.info(f"è¯†åˆ«åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
        return tables

    def extract_table_of_contents(self, text: str) -> List[Dict]:
        """
        æå–æ–‡æ¡£ç›®å½•

        Args:
            text: æ–‡æ¡£å…¨æ–‡

        Returns:
            toc: ç›®å½•åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡ç›®åŒ…å«æ ‡é¢˜ã€å±‚çº§ã€æ˜¯å¦éœ€è¦æå–ç­‰ä¿¡æ¯
        """
        toc = []
        lines = text.split('\n')

        # 1. æ‰¾åˆ°"ç›®å½•"å¼€å§‹ä½ç½®
        toc_start = -1
        for i, line in enumerate(lines):
            stripped = line.strip()
            if re.search(r'^ç›®\s*å½•|^CONTENTS|^Table of Contents', stripped, re.I):
                toc_start = i
                logger.info(f"æ‰¾åˆ°ç›®å½•èµ·å§‹ä½ç½®ï¼šç¬¬{i+1}è¡Œ")
                break

        if toc_start == -1:
            logger.warning("æœªæ‰¾åˆ°ç›®å½•ï¼Œå°†ä½¿ç”¨æ™®é€šç« èŠ‚è¯†åˆ«")
            return []

        # 2. æå–ç›®å½•é¡¹ï¼ˆç›´åˆ°é‡åˆ°æ­£æ–‡ï¼‰
        toc_end = -1
        for i in range(toc_start + 1, min(toc_start + 200, len(lines))):  # é™åˆ¶æœç´¢èŒƒå›´
            line = lines[i].strip()

            # ç›®å½•ç»“æŸæ ‡å¿—ï¼ˆé‡åˆ°ç¬¬ä¸€éƒ¨åˆ†/ç¬¬ä¸€ç« ç­‰æ­£æ–‡æ ‡é¢˜ï¼‰
            if self._is_content_start(line):
                toc_end = i
                logger.info(f"ç›®å½•ç»“æŸä½ç½®ï¼šç¬¬{i+1}è¡Œ")
                break

            # è§£æç›®å½•é¡¹
            # åŒ¹é…æ ¼å¼ï¼šç¬¬ä¸€éƒ¨åˆ† xxxã€ç¬¬1éƒ¨åˆ† xxxã€ä¸€ã€xxxç­‰
            patterns = [
                r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+éƒ¨åˆ†)\s*(.+?)(?:\s*\.{2,}.*)?$',
                r'(ç¬¬\d+éƒ¨åˆ†)\s*(.+?)(?:\s*\.{2,}.*)?$',
                r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )\s*(.+?)(?:\s*\.{2,}.*)?$',
                r'(ç¬¬\d+ç« )\s*(.+?)(?:\s*\.{2,}.*)?$',
                r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)ã€\s*(.+?)(?:\s*\.{2,}.*)?$',
                r'(\d+)ã€\s*(.+?)(?:\s*\.{2,}.*)?$',
            ]

            for pattern in patterns:
                match = re.match(pattern, line)
                if match:
                    section_num = match.group(1)
                    section_title = match.group(2).strip()
                    full_title = f"{section_num} {section_title}"

                    # åˆ¤æ–­æ˜¯å¦éœ€è¦æå–
                    is_relevant = self._is_relevant_section(full_title)

                    toc.append({
                        'title': full_title,
                        'section_num': section_num,
                        'section_name': section_title,
                        'level': 1,
                        'is_relevant': is_relevant,
                        'toc_line': i,
                        'start_position': None,
                        'end_position': None
                    })

                    status = "âœ… éœ€è¦æå–" if is_relevant else "âŒ è·³è¿‡"
                    logger.info(f"  ç›®å½•é¡¹: {full_title} {status}")
                    break

        logger.info(f"ä»ç›®å½•ä¸­è¯†åˆ«åˆ° {len(toc)} ä¸ªç« èŠ‚")
        return toc

    def _is_content_start(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ­£æ–‡å¼€å§‹ï¼ˆç”¨äºç›®å½•ç»“æŸåˆ¤æ–­ï¼‰"""
        # æ­£æ–‡é€šå¸¸ä»¥"ç¬¬Xéƒ¨åˆ†"ã€"ç¬¬Xç« "ç­‰å¼€å¤´ï¼Œä¸”åé¢æœ‰å®è´¨å†…å®¹
        patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+éƒ¨åˆ†\s+\S{2,}',
            r'^ç¬¬\d+éƒ¨åˆ†\s+\S{2,}',
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« \s+\S{2,}',
            r'^ç¬¬\d+ç« \s+\S{2,}',
        ]
        return any(re.match(p, line) for p in patterns)

    def _is_relevant_section(self, section_title: str) -> bool:
        """
        åˆ¤æ–­ç« èŠ‚æ˜¯å¦éœ€è¦æå–

        Args:
            section_title: ç« èŠ‚æ ‡é¢˜

        Returns:
            bool: Trueè¡¨ç¤ºéœ€è¦æå–ï¼ŒFalseè¡¨ç¤ºè·³è¿‡
        """
        title_lower = section_title.lower()

        # 1. å…ˆæ£€æŸ¥è·³è¿‡è§„åˆ™ï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
        for keyword in self.SKIP_SECTION_KEYWORDS:
            if keyword in section_title:
                return False

        # 2. å†æ£€æŸ¥éœ€è¦æå–çš„è§„åˆ™
        for keyword in self.RELEVANT_SECTION_KEYWORDS:
            if keyword in section_title:
                return True

        # 3. é»˜è®¤ä¿ç•™ï¼ˆè®©åç»­filteråˆ¤æ–­ï¼‰
        return True

    def locate_sections_in_content(self, text: str, toc: List[Dict]) -> List[Dict]:
        """
        åœ¨æ­£æ–‡ä¸­å®šä½æ¯ä¸ªç« èŠ‚çš„èµ·å§‹ä½ç½®

        Args:
            text: æ–‡æ¡£å…¨æ–‡
            toc: ç›®å½•åˆ—è¡¨

        Returns:
            toc: æ›´æ–°åçš„ç›®å½•åˆ—è¡¨ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰
        """
        lines = text.split('\n')

        for section in toc:
            # åœ¨å…¨æ–‡ä¸­æœç´¢ç« èŠ‚æ ‡é¢˜ï¼ˆä»ç›®å½•ä½ç½®ä¹‹åå¼€å§‹æœç´¢ï¼‰
            search_start = section.get('toc_line', 0) + 1

            for i in range(search_start, len(lines)):
                line = lines[i].strip()

                # åŒ¹é…ç« èŠ‚æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                # å®Œå…¨åŒ¹é…
                if section['title'] == line:
                    section['start_position'] = i
                    logger.debug(f"å®šä½ç« èŠ‚ '{section['title']}' åœ¨ç¬¬{i+1}è¡Œ")
                    break

                # åŒ…å«ç« èŠ‚ç¼–å·å’Œåç§°
                if section['section_num'] in line and section['section_name'] in line:
                    section['start_position'] = i
                    logger.debug(f"å®šä½ç« èŠ‚ '{section['title']}' åœ¨ç¬¬{i+1}è¡Œ")
                    break

        # è®¡ç®—æ¯ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ç« èŠ‚çš„å¼€å§‹-1ï¼‰
        for i, section in enumerate(toc):
            if section['start_position'] is None:
                logger.warning(f"æœªèƒ½å®šä½ç« èŠ‚: {section['title']}")
                continue

            if i < len(toc) - 1:
                # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ‰ä½ç½®ä¿¡æ¯çš„ç« èŠ‚
                next_position = None
                for next_section in toc[i+1:]:
                    if next_section['start_position'] is not None:
                        next_position = next_section['start_position']
                        break

                if next_position:
                    section['end_position'] = next_position - 1
                else:
                    section['end_position'] = len(lines) - 1
            else:
                section['end_position'] = len(lines) - 1

        # ç»Ÿè®¡å®šä½æˆåŠŸçš„ç« èŠ‚
        located_count = sum(1 for s in toc if s['start_position'] is not None)
        logger.info(f"æˆåŠŸå®šä½ {located_count}/{len(toc)} ä¸ªç« èŠ‚")

        return toc

    def chunk_document(self, text: str, metadata: Dict = None) -> List[DocumentChunk]:
        """
        æ™ºèƒ½åˆ†å—æ–‡æ¡£

        Args:
            text: æ–‡æ¡£å…¨æ–‡
            metadata: æ–‡æ¡£å…ƒæ•°æ®

        Returns:
            chunks: åˆ†å—åˆ—è¡¨
        """
        logger.info("å¼€å§‹æ–‡æ¡£åˆ†å—å¤„ç†...")

        # 1. å°è¯•æå–ç›®å½•ï¼ˆåŸºäºç›®å½•çš„ç« èŠ‚è¿‡æ»¤ï¼‰
        toc = self.extract_table_of_contents(text)

        if toc:
            # ä½¿ç”¨åŸºäºç›®å½•çš„åˆ†å—é€»è¾‘
            logger.info("ä½¿ç”¨åŸºäºç›®å½•çš„æ™ºèƒ½åˆ†å—...")
            return self._chunk_document_with_toc(text, toc, metadata)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„ç« èŠ‚è¯†åˆ«åˆ†å—é€»è¾‘
            logger.info("ä½¿ç”¨ä¼ ç»Ÿç« èŠ‚è¯†åˆ«åˆ†å—...")
            return self._chunk_document_traditional(text, metadata)

    def _chunk_document_with_toc(self, text: str, toc: List[Dict], metadata: Dict = None) -> List[DocumentChunk]:
        """
        åŸºäºç›®å½•çš„æ™ºèƒ½åˆ†å—

        Args:
            text: æ–‡æ¡£å…¨æ–‡
            toc: ç›®å½•åˆ—è¡¨
            metadata: æ–‡æ¡£å…ƒæ•°æ®

        Returns:
            chunks: åˆ†å—åˆ—è¡¨
        """
        # 1. å®šä½ç« èŠ‚åœ¨æ­£æ–‡ä¸­çš„ä½ç½®
        toc = self.locate_sections_in_content(text, toc)

        # 2. æå–è¡¨æ ¼
        tables = self.extract_tables(text)

        # 3. åˆ†å—å¤„ç†
        chunks = []
        chunk_index = 0
        lines = text.split('\n')

        for section in toc:
            # è·³è¿‡ä¸ç›¸å…³çš„ç« èŠ‚
            if not section['is_relevant']:
                logger.info(f"â­ï¸  è·³è¿‡æ— å…³ç« èŠ‚: {section['title']}")
                continue

            # è·³è¿‡æ— æ³•å®šä½çš„ç« èŠ‚
            if section['start_position'] is None:
                logger.warning(f"âš ï¸  æ— æ³•å®šä½ç« èŠ‚: {section['title']}")
                continue

            logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚: {section['title']}")

            # æå–è¯¥ç« èŠ‚çš„å†…å®¹
            start = section['start_position']
            end = section['end_position']
            section_content = '\n'.join(lines[start:end + 1])

            # ä¸ºç« èŠ‚æ ‡é¢˜åˆ›å»ºç‹¬ç«‹åˆ†å—
            chunk_metadata = {
                'section_title': section['title'],
                'section_level': section['level'],
                'start_line': start,
                'end_line': start,
                'token_count': self.count_tokens(section['title']),
                'is_from_toc': True
            }
            if metadata:
                chunk_metadata.update(metadata)

            chunks.append(DocumentChunk(
                chunk_index=chunk_index,
                chunk_type='title',
                content=section['title'],
                metadata=chunk_metadata
            ))
            chunk_index += 1

            # å¤„ç†ç« èŠ‚å†…å®¹
            if section_content.strip():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
                section_tables = [t for t in tables
                                 if start <= t['start_line'] <= end]

                if section_tables:
                    # åˆ†æ®µå¤„ç†ï¼ˆè¡¨æ ¼å‰ã€è¡¨æ ¼ã€è¡¨æ ¼åï¼‰
                    content_chunks = self._chunk_content_with_tables(
                        section_content, section_tables, section['title'], section['level']
                    )
                else:
                    # æ™®é€šå†…å®¹åˆ†å—
                    content_chunks = self._chunk_content(
                        section_content, section['title'], section['level']
                    )

                for chunk in content_chunks:
                    chunk.chunk_index = chunk_index
                    chunk.metadata['is_from_toc'] = True
                    if metadata:
                        chunk.metadata.update(metadata)
                    chunks.append(chunk)
                    chunk_index += 1

        logger.info(f"âœ… åŸºäºç›®å½•åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
        return chunks

    def _chunk_document_traditional(self, text: str, metadata: Dict = None) -> List[DocumentChunk]:
        """
        ä¼ ç»Ÿçš„ç« èŠ‚è¯†åˆ«åˆ†å—ï¼ˆä¸ä½¿ç”¨ç›®å½•ï¼‰

        Args:
            text: æ–‡æ¡£å…¨æ–‡
            metadata: æ–‡æ¡£å…ƒæ•°æ®

        Returns:
            chunks: åˆ†å—åˆ—è¡¨
        """
        # 1. æå–ç« èŠ‚ç»“æ„
        sections = self.extract_sections(text)

        # 2. æå–è¡¨æ ¼
        tables = self.extract_tables(text)

        # 3. ç”Ÿæˆåˆ†å—
        chunks = []
        chunk_index = 0

        for section in sections:
            section_title = section['title']
            section_content = section['content']
            section_level = section['level']

            # ä¸ºæ ‡é¢˜åˆ›å»ºç‹¬ç«‹åˆ†å—
            if section_title:
                chunk_metadata = {
                    'section_title': section_title,
                    'section_level': section_level,
                    'start_line': section['start_line'],
                    'end_line': section['start_line'],
                    'token_count': self.count_tokens(section_title)
                }
                if metadata:
                    chunk_metadata.update(metadata)

                chunks.append(DocumentChunk(
                    chunk_index=chunk_index,
                    chunk_type='title',
                    content=section_title,
                    metadata=chunk_metadata
                ))
                chunk_index += 1

            # å¤„ç†ç« èŠ‚å†…å®¹
            if section_content.strip():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
                section_tables = [t for t in tables
                                 if section['start_line'] <= t['start_line'] <= section['end_line']]

                if section_tables:
                    # åˆ†æ®µå¤„ç†ï¼ˆè¡¨æ ¼å‰ã€è¡¨æ ¼ã€è¡¨æ ¼åï¼‰
                    content_chunks = self._chunk_content_with_tables(
                        section_content, section_tables, section_title, section_level
                    )
                else:
                    # æ™®é€šå†…å®¹åˆ†å—
                    content_chunks = self._chunk_content(
                        section_content, section_title, section_level
                    )

                for chunk in content_chunks:
                    chunk.chunk_index = chunk_index
                    if metadata:
                        chunk.metadata.update(metadata)
                    chunks.append(chunk)
                    chunk_index += 1

        logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
        return chunks

    def _chunk_content(self, content: str, section_title: str, section_level: int) -> List[DocumentChunk]:
        """
        å¯¹æ™®é€šå†…å®¹è¿›è¡Œåˆ†å—

        Args:
            content: å†…å®¹æ–‡æœ¬
            section_title: æ‰€å±ç« èŠ‚æ ‡é¢˜
            section_level: ç« èŠ‚å±‚çº§

        Returns:
            chunks: åˆ†å—åˆ—è¡¨
        """
        chunks = []

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

        current_chunk_content = []
        current_tokens = 0

        for para in paragraphs:
            para_tokens = self.count_tokens(para)

            # å¦‚æœå•ä¸ªæ®µè½è¶…è¿‡æœ€å¤§å¤§å°ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if para_tokens > self.max_chunk_size:
                # å…ˆä¿å­˜å½“å‰ç´¯ç§¯çš„å†…å®¹
                if current_chunk_content:
                    chunks.append(self._create_chunk(
                        '\n\n'.join(current_chunk_content),
                        'paragraph',
                        section_title,
                        section_level
                    ))
                    current_chunk_content = []
                    current_tokens = 0

                # åˆ†å‰²é•¿æ®µè½
                para_chunks = self._split_long_paragraph(para, section_title, section_level)
                chunks.extend(para_chunks)

            elif current_tokens + para_tokens > self.max_chunk_size:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append(self._create_chunk(
                    '\n\n'.join(current_chunk_content),
                    'paragraph',
                    section_title,
                    section_level
                ))

                # æ·»åŠ é‡å å†…å®¹ï¼ˆæœ€åä¸€ä¸ªæ®µè½ï¼‰
                if current_chunk_content:
                    overlap_content = current_chunk_content[-1]
                    current_chunk_content = [overlap_content, para]
                    current_tokens = self.count_tokens(overlap_content) + para_tokens
                else:
                    current_chunk_content = [para]
                    current_tokens = para_tokens
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                current_chunk_content.append(para)
                current_tokens += para_tokens

        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk_content:
            chunks.append(self._create_chunk(
                '\n\n'.join(current_chunk_content),
                'paragraph',
                section_title,
                section_level
            ))

        return chunks

    def _split_long_paragraph(self, paragraph: str, section_title: str, section_level: int) -> List[DocumentChunk]:
        """åˆ†å‰²è¶…é•¿æ®µè½"""
        chunks = []

        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\n])', paragraph)
        sentences = [''.join(sentences[i:i+2]) for i in range(0, len(sentences)-1, 2)]

        current_content = []
        current_tokens = 0

        for sent in sentences:
            sent_tokens = self.count_tokens(sent)

            if current_tokens + sent_tokens > self.max_chunk_size:
                if current_content:
                    chunks.append(self._create_chunk(
                        ''.join(current_content),
                        'paragraph',
                        section_title,
                        section_level
                    ))

                current_content = [sent]
                current_tokens = sent_tokens
            else:
                current_content.append(sent)
                current_tokens += sent_tokens

        if current_content:
            chunks.append(self._create_chunk(
                ''.join(current_content),
                'paragraph',
                section_title,
                section_level
            ))

        return chunks

    def _chunk_content_with_tables(self, content: str, tables: List[Dict],
                                   section_title: str, section_level: int) -> List[DocumentChunk]:
        """å¤„ç†åŒ…å«è¡¨æ ¼çš„å†…å®¹"""
        chunks = []

        # TODO: å®ç°è¡¨æ ¼å’Œæ–‡æœ¬æ··åˆçš„åˆ†å—é€»è¾‘
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå°†è¡¨æ ¼å•ç‹¬æå–ä¸ºchunkï¼Œå…¶ä½™å†…å®¹æ­£å¸¸åˆ†å—
        for table in tables:
            chunks.append(self._create_chunk(
                table['content'],
                'table',
                section_title,
                section_level,
                {'table_rows': table['rows']}
            ))

        # å¤„ç†éè¡¨æ ¼å†…å®¹
        # ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®è¡¨æ ¼ä½ç½®åˆ‡åˆ†æ–‡æœ¬ï¼‰
        text_content = content
        for table in tables:
            text_content = text_content.replace(table['content'], '')

        if text_content.strip():
            text_chunks = self._chunk_content(text_content, section_title, section_level)
            chunks.extend(text_chunks)

        return chunks

    def _create_chunk(self, content: str, chunk_type: str, section_title: str,
                     section_level: int, extra_metadata: Dict = None) -> DocumentChunk:
        """åˆ›å»ºåˆ†å—å¯¹è±¡"""
        metadata = {
            'section_title': section_title,
            'section_level': section_level,
            'token_count': self.count_tokens(content),
            'char_count': len(content)
        }

        if extra_metadata:
            metadata.update(extra_metadata)

        return DocumentChunk(
            chunk_index=0,  # å°†åœ¨å¤–éƒ¨è®¾ç½®
            chunk_type=chunk_type,
            content=content,
            metadata=metadata
        )


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    sample_text = """
ç¬¬ä¸€ç«  é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªæ™ºèƒ½æ ‡ä¹¦å¤„ç†ç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. æ–‡æ¡£æ™ºèƒ½åˆ†å—
2. AIç­›é€‰
3. è¦æ±‚æå–

ç¬¬äºŒç«  æŠ€æœ¯è¦æ±‚

2.1 ç¡¬ä»¶è¦æ±‚
æŠ•æ ‡æ–¹åº”æä¾›ä»¥ä¸‹ç¡¬ä»¶è®¾å¤‡ï¼š
- æœåŠ¡å™¨ï¼šä¸å°‘äº2å°
- å­˜å‚¨ï¼šä¸å°‘äº10TB

2.2 è½¯ä»¶è¦æ±‚
ï¼ˆ1ï¼‰æ“ä½œç³»ç»Ÿï¼šLinuxæˆ–Windows Server
ï¼ˆ2ï¼‰æ•°æ®åº“ï¼šMySQL 8.0åŠä»¥ä¸Š
ï¼ˆ3ï¼‰ç¼–ç¨‹è¯­è¨€ï¼šPython 3.8åŠä»¥ä¸Š
"""

    chunker = DocumentChunker(max_chunk_size=200)
    chunks = chunker.chunk_document(sample_text)

    for chunk in chunks:
        print(f"\nã€åˆ†å— {chunk.chunk_index}ã€‘ç±»å‹: {chunk.chunk_type}")
        print(f"å†…å®¹: {chunk.content[:100]}...")
        print(f"å…ƒæ•°æ®: {chunk.metadata}")
