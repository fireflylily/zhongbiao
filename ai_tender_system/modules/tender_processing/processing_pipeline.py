#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ä¹¦æ™ºèƒ½å¤„ç†æµç¨‹åè°ƒå™¨
åŠŸèƒ½ï¼š
- æ•´åˆä¸‰æ­¥æµç¨‹ï¼šåˆ†å— -> ç­›é€‰ -> æå–
- è¿›åº¦è¿½è¸ª
- å¼‚æ­¥å¤„ç†æ”¯æŒ
- é”™è¯¯æ¢å¤æœºåˆ¶
"""

import uuid
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Callable
from pathlib import Path
from dataclasses import dataclass

from common import get_module_logger, get_config
from common.database import get_knowledge_base_db

from .chunker import DocumentChunker
from .filter import TenderFilter
from .requirement_extractor import RequirementExtractor

logger = get_module_logger("processing_pipeline")


@dataclass
class ProcessingProgress:
    """å¤„ç†è¿›åº¦æ•°æ®ç±»"""
    step: str  # chunking/filtering/extraction
    status: str  # pending/processing/completed/failed
    total_items: int = 0
    processed_items: int = 0
    success_items: int = 0
    failed_items: int = 0
    progress_percentage: float = 0.0
    estimated_time_remaining: int = 0  # ç§’

    def to_dict(self) -> Dict:
        return {
            'step': self.step,
            'status': self.status,
            'total_items': self.total_items,
            'processed_items': self.processed_items,
            'success_items': self.success_items,
            'failed_items': self.failed_items,
            'progress_percentage': self.progress_percentage,
            'estimated_time_remaining': self.estimated_time_remaining
        }


class TenderProcessingPipeline:
    """æ ‡ä¹¦æ™ºèƒ½å¤„ç†æµç¨‹åè°ƒå™¨"""

    def __init__(self, project_id: int, document_text: str,
                 filter_model: str = 'gpt-4o-mini',
                 extract_model: str = 'yuanjing-deepseek-v3',
                 progress_callback: Optional[Callable] = None):
        """
        åˆå§‹åŒ–å¤„ç†æµç¨‹

        Args:
            project_id: é¡¹ç›®ID
            document_text: æ–‡æ¡£å…¨æ–‡
            filter_model: ç­›é€‰æ¨¡å‹
            extract_model: æå–æ¨¡å‹
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(progress: ProcessingProgress)
        """
        self.project_id = project_id
        self.document_text = document_text
        self.filter_model = filter_model
        self.extract_model = extract_model
        self.progress_callback = progress_callback

        # ç”Ÿæˆä»»åŠ¡ID
        self.task_id = f"task_{project_id}_{uuid.uuid4().hex[:8]}"

        # åˆå§‹åŒ–ç»„ä»¶
        self.chunker = DocumentChunker(max_chunk_size=800, overlap_size=100)
        self.filter = TenderFilter(model_name=filter_model, max_workers=5)
        self.extractor = RequirementExtractor(model_name=extract_model, max_workers=3)

        # æ•°æ®åº“
        self.db = get_knowledge_base_db()

        # å¤„ç†ç»“æœ
        self.chunks = []
        self.filter_results = []
        self.requirements = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_cost = 0.0
        self.total_api_calls = 0
        self.start_time = None
        self.end_time = None

        logger.info(f"åˆå§‹åŒ–å¤„ç†æµç¨‹ - ä»»åŠ¡ID: {self.task_id}, é¡¹ç›®ID: {project_id}")

    def _update_progress(self, step: str, status: str, processed: int = 0, total: int = 0):
        """
        æ›´æ–°è¿›åº¦

        Args:
            step: å½“å‰æ­¥éª¤
            status: çŠ¶æ€
            processed: å·²å¤„ç†æ•°é‡
            total: æ€»æ•°é‡
        """
        progress = ProcessingProgress(
            step=step,
            status=status,
            total_items=total,
            processed_items=processed,
            success_items=processed,
            progress_percentage=processed / total * 100 if total > 0 else 0
        )

        # ä¼°ç®—å‰©ä½™æ—¶é—´
        if processed > 0 and self.start_time:
            elapsed = time.time() - self.start_time
            avg_time_per_item = elapsed / processed
            remaining_items = total - processed
            progress.estimated_time_remaining = int(avg_time_per_item * remaining_items)

        # è°ƒç”¨å›è°ƒ
        if self.progress_callback:
            self.progress_callback(progress)

        # æ›´æ–°æ•°æ®åº“
        self._update_task_in_db(step, status, progress.progress_percentage)

        logger.info(f"è¿›åº¦æ›´æ–° - {step}: {processed}/{total} ({progress.progress_percentage:.1f}%)")

    def _update_task_in_db(self, current_step: str, status: str, progress: float):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€åˆ°æ•°æ®åº“"""
        try:
            self.db.update_processing_task(
                task_id=self.task_id,
                current_step=current_step,
                overall_status=status,
                progress_percentage=progress
            )
        except Exception as e:
            logger.warning(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")

    def _save_chunks_to_db(self, chunks: List) -> bool:
        """ä¿å­˜åˆ†å—åˆ°æ•°æ®åº“"""
        try:
            chunks_data = []
            for chunk in chunks:
                chunk_dict = chunk.to_dict()
                chunk_dict['project_id'] = self.project_id
                chunks_data.append(chunk_dict)

            success = self.db.batch_create_tender_chunks(chunks_data)
            if success:
                logger.info(f"æˆåŠŸä¿å­˜ {len(chunks_data)} ä¸ªåˆ†å—åˆ°æ•°æ®åº“")
            return success
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†å—å¤±è´¥: {e}")
            return False

    def _update_filter_results_in_db(self, filter_results: List) -> bool:
        """æ›´æ–°ç­›é€‰ç»“æœåˆ°æ•°æ®åº“"""
        try:
            # è·å–æ•°æ®åº“ä¸­çš„åˆ†å—ï¼ˆå¸¦chunk_idï¼‰
            db_chunks = self.db.get_tender_chunks(self.project_id)

            # å»ºç«‹ç´¢å¼•æ˜ å°„
            chunk_id_map = {chunk['chunk_index']: chunk['chunk_id'] for chunk in db_chunks}

            # æ›´æ–°ç­›é€‰ç»“æœ
            for result in filter_results:
                chunk_index = result.chunk_id
                chunk_id = chunk_id_map.get(chunk_index)

                if chunk_id:
                    self.db.update_chunk_filter_result(
                        chunk_id=chunk_id,
                        is_valuable=result.is_valuable,
                        confidence=result.confidence,
                        model=self.filter_model
                    )

            logger.info(f"æˆåŠŸæ›´æ–° {len(filter_results)} ä¸ªåˆ†å—çš„ç­›é€‰ç»“æœ")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return False

    def _save_requirements_to_db(self, requirements: List) -> bool:
        """ä¿å­˜æå–çš„è¦æ±‚åˆ°æ•°æ®åº“"""
        try:
            # è·å–æ•°æ®åº“ä¸­çš„åˆ†å—ï¼ˆè·å–chunk_idæ˜ å°„ï¼‰
            db_chunks = self.db.get_tender_chunks(self.project_id, valuable_only=True)
            chunk_id_map = {chunk['chunk_index']: chunk['chunk_id'] for chunk in db_chunks}

            requirements_data = []
            for req in requirements:
                req_dict = req.to_dict()
                req_dict['project_id'] = self.project_id

                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è¿½è¸ªæ¯ä¸ªè¦æ±‚æ¥è‡ªå“ªä¸ªchunk
                # å¯ä»¥åœ¨æå–æ—¶è®°å½•chunk_id
                req_dict['chunk_id'] = None

                req_dict['extraction_model'] = self.extract_model
                requirements_data.append(req_dict)

            success = self.db.batch_create_tender_requirements(requirements_data)
            if success:
                logger.info(f"æˆåŠŸä¿å­˜ {len(requirements_data)} ä¸ªè¦æ±‚åˆ°æ•°æ®åº“")
            return success
        except Exception as e:
            logger.error(f"ä¿å­˜è¦æ±‚å¤±è´¥: {e}")
            return False

    def step1_chunking(self) -> bool:
        """
        æ­¥éª¤1ï¼šæ–‡æ¡£åˆ†å—

        Returns:
            success: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("æ­¥éª¤1ï¼šæ–‡æ¡£åˆ†å—")
        logger.info("=" * 60)

        try:
            self._update_progress('chunking', 'processing', 0, 1)

            # æ‰§è¡Œåˆ†å—
            self.chunks = self.chunker.chunk_document(
                text=self.document_text,
                metadata={'project_id': self.project_id}
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            if not self._save_chunks_to_db(self.chunks):
                raise Exception("ä¿å­˜åˆ†å—åˆ°æ•°æ®åº“å¤±è´¥")

            # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
            self.db.update_processing_task(
                task_id=self.task_id,
                total_chunks=len(self.chunks)
            )

            self._update_progress('chunking', 'completed', 1, 1)

            logger.info(f"âœ… åˆ†å—å®Œæˆ - å…±ç”Ÿæˆ {len(self.chunks)} ä¸ªåˆ†å—")
            return True

        except Exception as e:
            logger.error(f"âŒ åˆ†å—å¤±è´¥: {e}")
            self._update_progress('chunking', 'failed', 0, 1)
            return False

    def step2_filtering(self) -> bool:
        """
        æ­¥éª¤2ï¼šAIç­›é€‰

        Returns:
            success: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("æ­¥éª¤2ï¼šAIå¿«é€Ÿç­›é€‰")
        logger.info("=" * 60)

        try:
            # å‡†å¤‡ç­›é€‰æ•°æ®
            chunks_for_filter = [chunk.to_dict() for chunk in self.chunks]

            # è¿›åº¦å›è°ƒ
            def filter_progress(processed, total):
                self._update_progress('filtering', 'processing', processed, total)

            self._update_progress('filtering', 'processing', 0, len(chunks_for_filter))

            # æ‰§è¡Œç­›é€‰
            self.filter_results = self.filter.filter_chunks_parallel(
                chunks=chunks_for_filter,
                progress_callback=filter_progress
            )

            # æ›´æ–°æ•°æ®åº“
            if not self._update_filter_results_in_db(self.filter_results):
                raise Exception("æ›´æ–°ç­›é€‰ç»“æœåˆ°æ•°æ®åº“å¤±è´¥")

            # ç»Ÿè®¡
            valuable_count = sum(1 for r in self.filter_results if r.is_valuable)
            filter_rate = (len(self.filter_results) - valuable_count) / len(self.filter_results) * 100

            # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
            self.db.update_processing_task(
                task_id=self.task_id,
                valuable_chunks=valuable_count
            )

            # ç´¯è®¡æˆæœ¬
            filter_stats = self.filter.get_statistics()
            self.total_cost += filter_stats['total_cost']
            self.total_api_calls += filter_stats['total_api_calls']

            self._update_progress('filtering', 'completed', len(self.filter_results), len(self.filter_results))

            logger.info(f"âœ… ç­›é€‰å®Œæˆ")
            logger.info(f"   ä¿ç•™: {valuable_count} ({100-filter_rate:.1f}%)")
            logger.info(f"   è¿‡æ»¤: {len(self.filter_results) - valuable_count} ({filter_rate:.1f}%)")
            logger.info(f"   æˆæœ¬: ${filter_stats['total_cost']:.4f}")

            return True

        except Exception as e:
            logger.error(f"âŒ ç­›é€‰å¤±è´¥: {e}")
            self._update_progress('filtering', 'failed', 0, len(self.chunks))
            return False

    def step3_extraction(self) -> bool:
        """
        æ­¥éª¤3ï¼šç²¾å‡†æå–

        Returns:
            success: æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("æ­¥éª¤3ï¼šç²¾å‡†è¦æ±‚æå–")
        logger.info("=" * 60)

        try:
            # è·å–é«˜ä»·å€¼åˆ†å—
            valuable_chunks = [
                chunk.to_dict()
                for i, chunk in enumerate(self.chunks)
                if i < len(self.filter_results) and self.filter_results[i].is_valuable
            ]

            if not valuable_chunks:
                logger.warning("æ²¡æœ‰é«˜ä»·å€¼åˆ†å—ï¼Œè·³è¿‡æå–æ­¥éª¤")
                self._update_progress('extraction', 'completed', 0, 0)
                return True

            # è¿›åº¦å›è°ƒ
            def extract_progress(processed, total):
                self._update_progress('extraction', 'processing', processed, total)

            self._update_progress('extraction', 'processing', 0, len(valuable_chunks))

            # æ‰§è¡Œæå–
            self.requirements = self.extractor.extract_chunks_parallel(
                chunks=valuable_chunks,
                progress_callback=extract_progress
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            if not self._save_requirements_to_db(self.requirements):
                raise Exception("ä¿å­˜è¦æ±‚åˆ°æ•°æ®åº“å¤±è´¥")

            # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
            self.db.update_processing_task(
                task_id=self.task_id,
                total_requirements=len(self.requirements)
            )

            # ç´¯è®¡æˆæœ¬
            extract_stats = self.extractor.get_statistics()
            self.total_cost += extract_stats['total_cost']
            self.total_api_calls += extract_stats['total_api_calls']

            self._update_progress('extraction', 'completed', len(valuable_chunks), len(valuable_chunks))

            logger.info(f"âœ… æå–å®Œæˆ")
            logger.info(f"   å¤„ç†åˆ†å—: {len(valuable_chunks)}")
            logger.info(f"   æå–è¦æ±‚: {len(self.requirements)}")
            logger.info(f"   æˆæœ¬: ${extract_stats['total_cost']:.4f}")

            return True

        except Exception as e:
            logger.error(f"âŒ æå–å¤±è´¥: {e}")
            self._update_progress('extraction', 'failed', 0, len(self.chunks))
            return False

    def run(self) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„ä¸‰æ­¥å¤„ç†æµç¨‹

        Returns:
            result: å¤„ç†ç»“æœ
        """
        logger.info("ğŸš€ å¯åŠ¨æ ‡ä¹¦æ™ºèƒ½å¤„ç†æµç¨‹")
        logger.info(f"   ä»»åŠ¡ID: {self.task_id}")
        logger.info(f"   é¡¹ç›®ID: {self.project_id}")
        logger.info(f"   æ–‡æ¡£é•¿åº¦: {len(self.document_text)} å­—ç¬¦")

        self.start_time = time.time()

        # åˆ›å»ºä»»åŠ¡è®°å½•
        try:
            self.db.create_processing_task(
                project_id=self.project_id,
                task_id=self.task_id,
                pipeline_config={
                    'filter_model': self.filter_model,
                    'extract_model': self.extract_model
                }
            )
        except Exception as e:
            logger.warning(f"åˆ›å»ºä»»åŠ¡è®°å½•å¤±è´¥: {e}")

        # æ‰§è¡Œä¸‰æ­¥æµç¨‹
        steps = [
            ('chunking', self.step1_chunking),
            ('filtering', self.step2_filtering),
            ('extraction', self.step3_extraction)
        ]

        overall_success = True

        for step_name, step_func in steps:
            success = step_func()
            if not success:
                overall_success = False
                logger.error(f"æµç¨‹åœ¨æ­¥éª¤ {step_name} å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
                break

        self.end_time = time.time()
        total_time = self.end_time - self.start_time

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        final_status = 'completed' if overall_success else 'failed'
        self.db.update_processing_task(
            task_id=self.task_id,
            overall_status=final_status,
            progress_percentage=100.0 if overall_success else 0.0
        )

        # ç”Ÿæˆç»“æœæŠ¥å‘Š
        result = {
            'success': overall_success,
            'task_id': self.task_id,
            'project_id': self.project_id,
            'statistics': {
                'total_chunks': len(self.chunks),
                'valuable_chunks': sum(1 for r in self.filter_results if r.is_valuable),
                'filtered_chunks': sum(1 for r in self.filter_results if not r.is_valuable),
                'total_requirements': len(self.requirements),
                'mandatory_requirements': sum(1 for r in self.requirements if r.constraint_type == 'mandatory'),
                'optional_requirements': sum(1 for r in self.requirements if r.constraint_type == 'optional'),
                'scoring_requirements': sum(1 for r in self.requirements if r.constraint_type == 'scoring'),
            },
            'cost': {
                'total_cost': self.total_cost,
                'total_api_calls': self.total_api_calls,
                'filter_cost': self.filter.total_cost,
                'extraction_cost': self.extractor.total_cost
            },
            'performance': {
                'total_time': total_time,
                'total_time_formatted': f"{int(total_time // 60)}åˆ†{int(total_time % 60)}ç§’"
            }
        }

        # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        logger.info("=" * 60)
        logger.info("ğŸ“Š å¤„ç†å®Œæˆ - æœ€ç»ˆæŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"çŠ¶æ€: {'âœ… æˆåŠŸ' if overall_success else 'âŒ å¤±è´¥'}")
        logger.info(f"æ€»è€—æ—¶: {result['performance']['total_time_formatted']}")
        logger.info(f"æ€»åˆ†å—: {result['statistics']['total_chunks']}")
        logger.info(f"ä¿ç•™åˆ†å—: {result['statistics']['valuable_chunks']}")
        logger.info(f"æå–è¦æ±‚: {result['statistics']['total_requirements']}")
        logger.info(f"  - å¼ºåˆ¶æ€§: {result['statistics']['mandatory_requirements']}")
        logger.info(f"  - å¯é€‰: {result['statistics']['optional_requirements']}")
        logger.info(f"  - åŠ åˆ†é¡¹: {result['statistics']['scoring_requirements']}")
        logger.info(f"æ€»æˆæœ¬: ${self.total_cost:.4f}")
        logger.info(f"APIè°ƒç”¨: {self.total_api_calls} æ¬¡")
        logger.info("=" * 60)

        return result

    def run_step(self, step: int) -> Dict:
        """
        è¿è¡ŒæŒ‡å®šæ­¥éª¤ï¼ˆç”¨äºåˆ†æ­¥äº¤äº’å¼å¤„ç†ï¼‰

        Args:
            step: æ­¥éª¤ç¼–å· (1=åˆ†å—, 2=ç­›é€‰, 3=æå–)

        Returns:
            result: æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        logger.info(f"ğŸ”¹ æ‰§è¡Œæ­¥éª¤ {step}")

        if not hasattr(self, 'start_time') or self.start_time is None:
            self.start_time = time.time()

        # å¦‚æœæ˜¯ç¬¬ä¸€æ­¥ï¼Œåˆ›å»ºä»»åŠ¡è®°å½•
        if step == 1:
            try:
                self.db.create_processing_task(
                    project_id=self.project_id,
                    task_id=self.task_id,
                    pipeline_config={
                        'filter_model': self.filter_model,
                        'extract_model': self.extract_model
                    }
                )
            except Exception as e:
                logger.warning(f"åˆ›å»ºä»»åŠ¡è®°å½•å¤±è´¥: {e}")

        # æ‰§è¡Œå¯¹åº”æ­¥éª¤
        success = False
        step_name = ''

        if step == 1:
            step_name = 'chunking'
            success = self.step1_chunking()
        elif step == 2:
            step_name = 'filtering'
            success = self.step2_filtering()
        elif step == 3:
            step_name = 'extraction'
            success = self.step3_extraction()
        else:
            logger.error(f"æ— æ•ˆçš„æ­¥éª¤ç¼–å·: {step}")
            return {'success': False, 'error': f'æ— æ•ˆçš„æ­¥éª¤ç¼–å·: {step}'}

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'success': success,
            'task_id': self.task_id,
            'project_id': self.project_id,
            'step': step,
            'step_name': step_name
        }

        # æ ¹æ®æ­¥éª¤è¿”å›ç›¸åº”æ•°æ®
        if step == 1:
            # åˆ†å—å®Œæˆï¼Œè¿”å›åˆ†å—ç»Ÿè®¡
            result['statistics'] = {
                'total_chunks': len(self.chunks),
                'chunks_preview': [
                    {
                        'chunk_id': c.chunk_index,
                        'chunk_type': c.chunk_type,
                        'content_preview': c.content[:100] + '...' if len(c.content) > 100 else c.content,
                        'section_title': c.metadata.get('section_title', ''),
                        'token_count': c.metadata.get('token_count', 0)
                    }
                    for c in self.chunks[:20]  # åªè¿”å›å‰20ä¸ªåˆ†å—é¢„è§ˆ
                ]
            }
        elif step == 2:
            # ç­›é€‰å®Œæˆï¼Œè¿”å›ç­›é€‰ç»Ÿè®¡
            result['statistics'] = {
                'total_chunks': len(self.chunks),
                'valuable_chunks': sum(1 for r in self.filter_results if r.is_valuable),
                'filtered_chunks': sum(1 for r in self.filter_results if not r.is_valuable),
                'filter_rate': f"{(sum(1 for r in self.filter_results if not r.is_valuable) / len(self.filter_results) * 100):.1f}%" if self.filter_results else "0%"
            }
        elif step == 3:
            # æå–å®Œæˆï¼Œè¿”å›è¦æ±‚ç»Ÿè®¡
            result['statistics'] = {
                'total_requirements': len(self.requirements),
                'mandatory_requirements': sum(1 for r in self.requirements if r.constraint_type == 'mandatory'),
                'optional_requirements': sum(1 for r in self.requirements if r.constraint_type == 'optional'),
                'scoring_requirements': sum(1 for r in self.requirements if r.constraint_type == 'scoring'),
            }

        # è®¡ç®—æˆæœ¬å’Œæ—¶é—´
        current_time = time.time()
        elapsed_time = current_time - self.start_time

        result['cost'] = {
            'total_cost': self.total_cost,
            'total_api_calls': self.total_api_calls,
        }

        result['performance'] = {
            'elapsed_time': elapsed_time,
            'elapsed_time_formatted': f"{int(elapsed_time // 60)}åˆ†{int(elapsed_time % 60)}ç§’"
        }

        logger.info(f"æ­¥éª¤ {step} {'âœ… å®Œæˆ' if success else 'âŒ å¤±è´¥'}")

        return result


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    sample_document = """
ç¬¬ä¸€ç«  é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨å»ºè®¾ä¸€ä¸ªæ™ºèƒ½æ ‡ä¹¦å¤„ç†ç³»ç»Ÿï¼Œå®ç°æ ‡ä¹¦çš„æ™ºèƒ½åˆ†æå’Œè¦æ±‚æå–ã€‚

ç¬¬äºŒç«  æŠ•æ ‡äººèµ„æ ¼è¦æ±‚

2.1 åŸºæœ¬èµ„æ ¼
æŠ•æ ‡æ–¹å¿…é¡»å…·æœ‰å»ºç­‘å·¥ç¨‹æ–½å·¥æ€»æ‰¿åŒ…ä¸€çº§åŠä»¥ä¸Šèµ„è´¨ï¼Œå¹¶æä¾›æœ‰æ•ˆçš„èµ„è´¨è¯ä¹¦å¤å°ä»¶ã€‚

2.2 ä¸šç»©è¦æ±‚
æŠ•æ ‡æ–¹åº”å…·æœ‰3å¹´ä»¥ä¸Šç±»ä¼¼é¡¹ç›®å®æ–½ç»éªŒã€‚æä¾›3ä¸ªä»¥ä¸ŠæˆåŠŸæ¡ˆä¾‹å¯è·å¾—åŠ åˆ†ã€‚

ç¬¬ä¸‰ç«  æŠ€æœ¯è¦æ±‚

3.1 ç³»ç»Ÿæ¶æ„
ç³»ç»Ÿåº”é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•ã€‚å»ºè®®ä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²ã€‚

3.2 æ€§èƒ½æŒ‡æ ‡
ç³»ç»Ÿå¹¶å‘ç”¨æˆ·æ•°ä¸å¾—å°‘äº1000äººï¼Œå“åº”æ—¶é—´åº”åœ¨3ç§’ä»¥å†…ã€‚

ç¬¬å››ç«  å•†åŠ¡æ¡æ¬¾

4.1 ä»·æ ¼è¦æ±‚
æŠ•æ ‡æ€»ä»·ä¸å¾—è¶…è¿‡é¢„ç®—ä¸Šé™500ä¸‡å…ƒã€‚

4.2 ä»˜æ¬¾æ–¹å¼
æŒ‰ç…§é‡Œç¨‹ç¢‘ä»˜æ¬¾ï¼ŒéªŒæ”¶åˆæ ¼å30æ—¥å†…æ”¯ä»˜ã€‚
"""

    def progress_callback(progress):
        print(f"[{progress.step}] {progress.status} - {progress.progress_percentage:.1f}%")

    # åˆ›å»ºæµç¨‹å®ä¾‹
    pipeline = TenderProcessingPipeline(
        project_id=1,
        document_text=sample_document,
        filter_model='gpt-4o-mini',
        extract_model='yuanjing-deepseek-v3',
        progress_callback=progress_callback
    )

    # è¿è¡Œæµç¨‹
    result = pipeline.run()

    print("\n" + "=" * 60)
    print("å¤„ç†ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
