#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ç»“æ„è§£æå™¨ - ç”¨äº HITL 1 (äººå·¥ç« èŠ‚é€‰æ‹©)
åŠŸèƒ½ï¼š
- è§£æ Word æ–‡æ¡£çš„ç›®å½•ç»“æ„
- è¯†åˆ«ç« èŠ‚å±‚çº§ï¼ˆH1/H2/H3ï¼‰
- åŸºäºç™½åå•è‡ªåŠ¨æ¨èç« èŠ‚
- æä¾›ç« èŠ‚é¢„è§ˆæ–‡æœ¬
"""

import re
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from docx import Document
from docx.oxml import CT_Tbl, CT_P
from difflib import SequenceMatcher

from common import get_module_logger
from common.utils import resolve_file_path

logger = get_module_logger("structure_parser")


@dataclass
class ChapterNode:
    """ç« èŠ‚èŠ‚ç‚¹æ•°æ®ç±»"""
    id: str                      # å”¯ä¸€IDï¼Œå¦‚ "ch_1_2_3"
    level: int                   # å±‚çº§ï¼š1=H1, 2=H2, 3=H3
    title: str                   # ç« èŠ‚æ ‡é¢˜
    para_start_idx: int          # èµ·å§‹æ®µè½ç´¢å¼•
    para_end_idx: int            # ç»“æŸæ®µè½ç´¢å¼•ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
    word_count: int              # å­—æ•°ç»Ÿè®¡
    preview_text: str            # é¢„è§ˆæ–‡æœ¬ï¼ˆå‰5è¡Œï¼‰
    auto_selected: bool          # æ˜¯å¦è‡ªåŠ¨é€‰ä¸­ï¼ˆç™½åå•åŒ¹é…ï¼‰
    skip_recommended: bool       # æ˜¯å¦æ¨èè·³è¿‡ï¼ˆé»‘åå•åŒ¹é…ï¼‰
    content_tags: List[str] = None  # å†…å®¹æ ‡ç­¾ï¼ˆåŸºäºå†…å®¹å…³é”®è¯æ£€æµ‹ï¼‰
    content_sample: str = None   # å†…å®¹æ ·æœ¬ï¼ˆç”¨äºåˆåŒè¯†åˆ«ï¼Œ1500-2000å­—ï¼‰
    children: List['ChapterNode'] = None  # å­ç« èŠ‚åˆ—è¡¨

    def __post_init__(self):
        if self.children is None:
            self.children = []
        if self.content_tags is None:
            self.content_tags = []

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰"""
        data = asdict(self)
        data['children'] = [child.to_dict() for child in self.children]
        return data


class DocumentStructureParser:
    """æ–‡æ¡£ç»“æ„è§£æå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–è§£æå™¨"""
        self.logger = get_module_logger("structure_parser")

        # ========================================
        # æ–°å¢ï¼šç¼–å·æ¨¡å¼ï¼ˆç”¨äºè¯†åˆ«ç« èŠ‚é”šç‚¹ï¼‰ï¼ˆæ”¹è¿›4ï¼šæ‰©å±•ç¼–å·æ¨¡å¼ï¼‰
        # ========================================
        self.NUMBERING_PATTERNS = [
            # ä¸­æ–‡éƒ¨åˆ†/ç« èŠ‚ç¼–å·
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+éƒ¨åˆ†\s*',
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+ç« \s*',
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+èŠ‚\s*',
            # æ•°å­—ç¼–å·
            r'^\d+\.\s*',           # 1.
            r'^\d+\.\d+\s*',        # 1.1
            r'^\d+\.\d+\.\d+\s*',   # 1.1.1
            r'^\d+\.\d+\.\d+\.\d+\s*',  # 1.1.1.1ï¼ˆå››çº§ç¼–å·ï¼Œæ”¹è¿›4æ–°å¢ï¼‰
            # ä¸­æ–‡åºå·
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s*',
            r'^ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰\s*',
            r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s*',
            # å­—æ¯ç¼–å·
            r'^[A-Z]\.\s*',
            r'^[a-z]\.\s*',
            r'^\([A-Za-z]\)\s*',
            # ç½—é©¬æ•°å­—
            r'^[IVX]+\.\s*',
            r'^[ivx]+\.\s*',
            # é™„ä»¶/é™„è¡¨ç¼–å·ï¼ˆæ”¹è¿›4æ–°å¢ï¼‰
            r'^é™„ä»¶[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*',  # é™„ä»¶1: æˆ– é™„ä»¶ä¸€ï¼š
            r'^é™„è¡¨[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*',  # é™„è¡¨1: æˆ– é™„è¡¨ä¸€ï¼š
            r'^é™„å½•[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*',  # é™„å½•1: æˆ– é™„å½•ä¸€ï¼š
        ]

        # ç™½åå•ï¼šè‡ªåŠ¨é€‰ä¸­çš„å…³é”®è¯ï¼ˆæ”¹è¿›2ï¼šæ‰©å±•å…³é”®è¯åº“ï¼‰
        self.WHITELIST_KEYWORDS = [
            # æŠ•æ ‡è¦æ±‚ç±»
            "æŠ•æ ‡é¡»çŸ¥", "ä¾›åº”å•†é¡»çŸ¥", "æŠ•æ ‡äººé¡»çŸ¥", "èµ„æ ¼è¦æ±‚", "èµ„è´¨è¦æ±‚",
            "å“åº”äººé¡»çŸ¥",  # æ–°å¢ï¼šç«äº‰æ€§è°ˆåˆ¤å¸¸ç”¨
            "æŠ•æ ‡é‚€è¯·", "è°ˆåˆ¤é‚€è¯·", "é‡‡è´­é‚€è¯·", "æ‹›æ ‡å…¬å‘Š", "é¡¹ç›®æ¦‚å†µ",
            "ç£‹å•†é‚€è¯·",  # æ–°å¢ï¼šç«äº‰æ€§ç£‹å•†å¸¸ç”¨
            "å•ä¸€æ¥æº", "ç«äº‰æ€§è°ˆåˆ¤", "è¯¢ä»·å…¬å‘Š",
            # æŠ€æœ¯è¦æ±‚ç±»
            "æŠ€æœ¯è¦æ±‚", "æŠ€æœ¯éœ€æ±‚", "éœ€æ±‚ä¹¦", "æŠ€æœ¯è§„æ ¼", "æŠ€æœ¯å‚æ•°", "æ€§èƒ½æŒ‡æ ‡", "é¡¹ç›®éœ€æ±‚",
            "éœ€æ±‚è¯´æ˜", "æŠ€æœ¯æ ‡å‡†", "åŠŸèƒ½è¦æ±‚", "æŠ€æœ¯è§„èŒƒ", "æŠ€æœ¯æ–¹æ¡ˆ",
            "é‡‡è´­éœ€æ±‚", "æœåŠ¡è¦æ±‚", "æœåŠ¡å†…å®¹", "æœåŠ¡èŒƒå›´",  # æ–°å¢ï¼šæœåŠ¡ç±»é¡¹ç›®å¸¸ç”¨
            # å•†åŠ¡è¦æ±‚ç±»
            "å•†åŠ¡è¦æ±‚", "å•†åŠ¡æ¡æ¬¾", "ä»˜æ¬¾æ–¹å¼", "äº¤ä»˜è¦æ±‚", "è´¨ä¿è¦æ±‚",
            "ä»·æ ¼è¦æ±‚", "æŠ¥ä»·è¦æ±‚",
            "åˆåŒä¸»è¦æ¡æ¬¾", "ä»˜æ¬¾æ¡æ¬¾", "ç»“ç®—æ–¹å¼",  # æ–°å¢ï¼šåˆåŒç›¸å…³ï¼ˆä½†åœ¨ç™½åå•ä¸­ï¼‰
            # è¯„åˆ†æ ‡å‡†ç±»
            "è¯„åˆ†æ ‡å‡†", "è¯„æ ‡åŠæ³•", "è¯„åˆ†ç»†åˆ™", "æ‰“åˆ†æ ‡å‡†", "ç»¼åˆè¯„åˆ†",
            "è¯„å®¡æ ‡å‡†", "è¯„å®¡åŠæ³•",
            "è¯„ä»·æ–¹æ³•", "æ‰“åˆ†ç»†åˆ™",  # æ–°å¢ï¼šè¯„åˆ†ç›¸å…³å˜ä½“
        ]

        # é»‘åå•ï¼šæ¨èè·³è¿‡çš„å…³é”®è¯ï¼ˆä¼˜å…ˆçº§é«˜äºç™½åå•ï¼‰
        self.BLACKLIST_KEYWORDS = [
            # åˆåŒç±»ï¼ˆåŒ…å«"åˆåŒ"ä½†éè¦æ±‚ç±»çš„æ ‡é¢˜ï¼‰
            "åˆåŒæ¡æ¬¾", "åˆåŒæ–‡æœ¬", "åˆåŒèŒƒæœ¬", "åˆåŒæ ¼å¼", "åˆåŒåè®®",
            "é€šç”¨æ¡æ¬¾", "ä¸“ç”¨æ¡æ¬¾", "åˆåŒä¸»è¦æ¡æ¬¾", "åˆåŒè‰ç¨¿", "æ‹Ÿç­¾åˆåŒ",
            "æœåŠ¡åˆåŒ", "é‡‡è´­åˆåŒ", "ä¹°å–åˆåŒ", "é”€å”®åˆåŒ", "æ–½å·¥åˆåŒ",
            "åˆ†åŒ…åˆåŒ", "åŠ³åŠ¡åˆåŒ", "ç§ŸèµåˆåŒ", "å§”æ‰˜åˆåŒ", "ä»£ç†åˆåŒ",
            # åˆåŒå…ƒä¿¡æ¯
            "åˆåŒç¼–å·", "åˆåŒåŒæ–¹", "ç”²æ–¹", "ä¹™æ–¹", "ä¸™æ–¹",
            "ç­¾è®¢åœ°ç‚¹", "ç­¾è®¢æ—¥æœŸ", "æœ‰æ•ˆæœŸ", "åˆåŒæœŸé™",
            # é¡¹ç›®å’Œå…¬å¸ä¿¡æ¯
            "é¡¹ç›®åç§°", "é¡¹ç›®ç¼–å·", "å…¬å¸åç§°", "å…¬å¸ç®€ä»‹", "ä¼ä¸šä¿¡æ¯",
            "é‡‡è´­äººä¿¡æ¯", "ä¾›åº”å•†ä¿¡æ¯", "æŠ•æ ‡äººä¿¡æ¯",
            # ç›®å½•ç»“æ„
            "ç›®å½•", "ç´¢å¼•", "ç« èŠ‚ç›®å½•", "å†…å®¹ç›®å½•",
            # æ ¼å¼ç±»
            "æŠ•æ ‡æ–‡ä»¶æ ¼å¼", "æ–‡ä»¶æ ¼å¼", "æ ¼å¼è¦æ±‚", "ç¼–åˆ¶è¦æ±‚", "å°è£…è¦æ±‚",
            "å“åº”æ–‡ä»¶æ ¼å¼", "èµ„æ–™æ¸…å•", "åŒ…è£…è¦æ±‚", "å¯†å°è¦æ±‚",
            # æ³•å¾‹å£°æ˜ç±»
            "æ³•å¾‹å£°æ˜", "å…è´£å£°æ˜", "æŠ•æ ‡æ‰¿è¯º", "å»‰æ”¿æ‰¿è¯º", "ä¿å¯†åè®®",
            "è¯šä¿¡æ‰¿è¯º", "å£°æ˜å‡½", "æˆæƒä¹¦", "å§”æ‰˜ä¹¦",
            # é™„ä»¶ç±»
            "é™„ä»¶", "é™„è¡¨", "é™„å½•", "æ ·è¡¨", "æ¨¡æ¿", "æ ¼å¼èŒƒæœ¬", "ç©ºç™½è¡¨æ ¼",
            # è¯´æ˜æ€§æ–‡å­—
            "å¡«å†™è¯´æ˜", "å¡«è¡¨è¯´æ˜", "ä½¿ç”¨è¯´æ˜", "æ³¨æ„äº‹é¡¹", "ç‰¹åˆ«è¯´æ˜",
            "å¤‡æ³¨", "å‚è€ƒæ ·æœ¬", "ç¤ºä¾‹", "ä»…ä¾›å‚è€ƒ",
        ]

        # æ ‡é¢˜æ ·å¼åç§°æ˜ å°„ï¼ˆä¸­è‹±æ–‡ï¼‰
        self.HEADING_STYLES = {
            1: ['Heading 1', 'æ ‡é¢˜ 1', 'heading 1', '1çº§æ ‡é¢˜'],
            2: ['Heading 2', 'æ ‡é¢˜ 2', 'heading 2', '2çº§æ ‡é¢˜'],
            3: ['Heading 3', 'æ ‡é¢˜ 3', 'heading 3', '3çº§æ ‡é¢˜'],
        }

    def parse_document_structure(
        self,
        doc_path: str,
        methods: Optional[List[str]] = None,
        fallback: bool = True
    ) -> Dict:
        """
        è§£ææ–‡æ¡£ç»“æ„ - æ€»è°ƒç”¨å™¨

        æ”¯æŒæŒ‡å®šè§£ææ–¹æ³•æˆ–ä½¿ç”¨é»˜è®¤æ™ºèƒ½ç­–ç•¥

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„
            methods: å¯é€‰ï¼ŒæŒ‡å®šè¦ä½¿ç”¨çš„è§£ææ–¹æ³•åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•
                    å¯é€‰å€¼: ['toc_exact', 'semantic_anchors', 'style', 'hybrid',
                            'azure', 'outline_level', 'gemini']
                    é»˜è®¤Noneè¡¨ç¤ºä½¿ç”¨æ™ºèƒ½ç­–ç•¥ï¼ˆæ ¹æ®æ–‡æ¡£ç‰¹å¾è‡ªåŠ¨é€‰æ‹©ï¼‰
            fallback: å¯é€‰ï¼Œæ˜¯å¦å¯ç”¨å›é€€æœºåˆ¶ï¼ˆå½“å‰æ–¹æ³•å¤±è´¥æ—¶å°è¯•ä¸‹ä¸€ä¸ªï¼‰
                     é»˜è®¤True

        Returns:
            {
                "success": True/False,
                "chapters": [ChapterNode.to_dict(), ...],
                "statistics": {
                    "total_chapters": 10,
                    "auto_selected": 5,
                    "skip_recommended": 3,
                    "total_words": 15000
                },
                "method": "ä½¿ç”¨çš„è§£ææ–¹æ³•åç§°",
                "error": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"
            }
        """
        # æ–¹æ³•æ˜ å°„è¡¨
        method_map = {
            'toc_exact': self.parse_by_toc_exact,
            'semantic_anchors': self.parse_by_semantic_anchors,
            'style': self.parse_by_style,
            'hybrid': self.parse_by_hybrid,
            'azure': self.parse_by_azure,
            'outline_level': self.parse_by_outline_level,
            'gemini': self.parse_by_gemini
        }

        try:
            self.logger.info(f"å¼€å§‹è§£ææ–‡æ¡£ç»“æ„: {doc_path}")

            # â­ï¸ æ–‡ä»¶æ ¼å¼æ£€æµ‹ï¼šä¸æ”¯æŒæ—§ç‰ˆ.docæ ¼å¼
            if doc_path.lower().endswith('.doc'):
                error_message = (
                    "æš‚ä¸æ”¯æŒæ—§ç‰ˆ .doc æ ¼å¼æ–‡æ¡£çš„ç« èŠ‚è§£æã€‚\n\n"
                    "è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n"
                    "1. ä½¿ç”¨ Microsoft Word æˆ– WPS Office æ‰“å¼€è¯¥æ–‡ä»¶\n"
                    '2. ç‚¹å‡»"æ–‡ä»¶" â†’ "å¦å­˜ä¸º"\n'
                    '3. åœ¨"ä¿å­˜ç±»å‹"ä¸­é€‰æ‹© "Word æ–‡æ¡£ (*.docx)"\n'
                    "4. ä¿å­˜åé‡æ–°ä¸Šä¼  .docx æ–‡ä»¶\n\n"
                    "æç¤ºï¼š.docx æ ¼å¼å…¼å®¹æ€§æ›´å¥½ï¼Œæ¨èä½¿ç”¨ã€‚"
                )
                self.logger.error(f".doc æ–‡ä»¶ä¸æ”¯æŒ: {doc_path}")
                raise ValueError(error_message)

            # åœºæ™¯1: ç”¨æˆ·æŒ‡å®šäº†å…·ä½“æ–¹æ³•
            if methods is not None:
                self.logger.info(f"ä½¿ç”¨æŒ‡å®šæ–¹æ³•: {methods}, fallback={fallback}")

                for method_name in methods:
                    if method_name not in method_map:
                        self.logger.warning(f"æœªçŸ¥æ–¹æ³•: {method_name}ï¼Œè·³è¿‡")
                        continue

                    self.logger.info(f"å°è¯•æ–¹æ³•: {method_name}")
                    result = method_map[method_name](doc_path)

                    # åˆ¤æ–­æˆåŠŸæ ‡å‡†
                    if result.get('success'):
                        # æ£€æŸ¥æ˜¯å¦è¯†åˆ«åˆ°è¶³å¤Ÿçš„ç« èŠ‚
                        chapters = result.get('chapters', [])
                        if len(chapters) >= 1:  # è‡³å°‘è¯†åˆ«åˆ°1ä¸ªç« èŠ‚
                            self.logger.info(f"æ–¹æ³• {method_name} æˆåŠŸï¼Œè¯†åˆ«åˆ° {len(chapters)} ä¸ªç« èŠ‚")
                            return result
                        else:
                            self.logger.warning(f"æ–¹æ³• {method_name} æœªè¯†åˆ«åˆ°ç« èŠ‚")

                    # å¦‚æœä¸å¯ç”¨fallbackï¼Œç›´æ¥è¿”å›ç»“æœï¼ˆå³ä½¿å¤±è´¥ï¼‰
                    if not fallback:
                        self.logger.info(f"fallback=Falseï¼Œç›´æ¥è¿”å› {method_name} çš„ç»“æœ")
                        return result

                    # å¦åˆ™ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ–¹æ³•
                    self.logger.warning(f"æ–¹æ³• {method_name} å¤±è´¥æˆ–æ•ˆæœä¸ä½³ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–¹æ³•")

                # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
                return {
                    "success": False,
                    "chapters": [],
                    "statistics": {},
                    "error": f"æ‰€æœ‰æŒ‡å®šæ–¹æ³•({methods})éƒ½å¤±è´¥",
                    "method": "none"
                }

            # åœºæ™¯2: é»˜è®¤æ™ºèƒ½ç­–ç•¥ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            self.logger.info("ä½¿ç”¨é»˜è®¤æ™ºèƒ½ç­–ç•¥")

            # ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æï¼ˆå…¼å®¹æœ¬åœ°å’Œç”Ÿäº§ç¯å¢ƒï¼‰
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            self.logger.info(f"æ–‡æ¡£è·¯å¾„è§£ææˆåŠŸ: {doc_path} -> {doc_path_abs}")

            # æ‰“å¼€æ–‡æ¡£
            doc = Document(str(doc_path_abs))

            # 1. å°è¯•æ£€æµ‹ç›®å½•
            toc_idx = self._find_toc_section(doc)

            if toc_idx is not None:
                # æœ‰ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ç²¾ç¡®åŒ¹é…è§£æï¼ˆæ–¹æ³•1ï¼‰
                self.logger.info("æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨ç²¾ç¡®åŒ¹é…è§£ææ–¹æ¡ˆ")
                toc_items, toc_end_idx = self._parse_toc_items(doc, toc_idx)

                if toc_items and len(toc_items) > 0:
                    # æå–ç›®å½•æ ‡é¢˜åˆ—è¡¨ï¼ˆä½œä¸ºè¯­ä¹‰ç›®æ ‡ï¼‰
                    toc_targets = [item['title'] for item in toc_items]

                    # ä¼˜å…ˆä½¿ç”¨æ–¹æ³•1ï¼šç²¾ç¡®åŒ¹é…ï¼ˆé€Ÿåº¦å¿«ã€å‡†ç¡®ç‡é«˜ï¼‰
                    chapters = self._locate_chapters_by_toc(doc, toc_items, toc_end_idx)

                    # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼ˆè¯†åˆ«çš„ç« èŠ‚å¤ªå°‘ï¼‰ï¼Œå›é€€åˆ°è¯­ä¹‰é”šç‚¹æ–¹æ³•
                    if len(chapters) < len(toc_items) * 0.5:  # è‡³å°‘è¯†åˆ«50%çš„ç›®å½•é¡¹
                        self.logger.warning(f"ç²¾ç¡®åŒ¹é…æ•ˆæœä¸ä½³ï¼ˆè¯†åˆ«{len(chapters)}/{len(toc_items)}ï¼‰ï¼Œå›é€€åˆ°è¯­ä¹‰é”šç‚¹è§£ææ–¹æ¡ˆ")
                        chapters = self._parse_chapters_by_semantic_anchors(doc, toc_targets, toc_end_idx)
                else:
                    # ç›®å½•è§£æå¤±è´¥ï¼Œå›é€€åˆ°æ ‡é¢˜æ ·å¼è¯†åˆ«
                    self.logger.warning("ç›®å½•è§£æå¤±è´¥ï¼Œå›é€€åˆ°æ ‡é¢˜æ ·å¼è¯†åˆ«æ–¹æ¡ˆ")
                    chapters = self._parse_chapters_from_doc(doc)
                    chapters = self._locate_chapter_content(doc, chapters)
            else:
                # æ— ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨æ–¹æ³•äº”ï¼ˆå¤§çº²çº§åˆ«è¯†åˆ«ï¼‰
                self.logger.info("æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨æ–¹æ³•äº”ï¼ˆå¤§çº²çº§åˆ«è¯†åˆ«ï¼‰")
                chapters = self._parse_chapters_by_outline_level(doc)

                # å¦‚æœè¯†åˆ«ç« èŠ‚å¤ªå°‘ï¼Œå›é€€åˆ°æ ‡é¢˜æ ·å¼è¯†åˆ«
                if len(chapters) < 3:
                    self.logger.warning(f"æ–¹æ³•äº”è¯†åˆ«æ•ˆæœä¸ä½³ï¼ˆåªæ‰¾åˆ°{len(chapters)}ä¸ªç« èŠ‚ï¼‰ï¼Œå›é€€åˆ°æ ‡é¢˜æ ·å¼è¯†åˆ«")
                    chapters = self._parse_chapters_from_doc(doc)

                chapters = self._locate_chapter_content(doc, chapters)

            # 2. æ„å»ºå±‚çº§æ ‘
            chapter_tree = self._build_chapter_tree(chapters)

            # ä¼ æ’­é»‘åå•çŠ¶æ€(çˆ¶ç« èŠ‚è¢«è·³è¿‡æ—¶,å­ç« èŠ‚ä¹Ÿåº”è·³è¿‡)
            chapter_tree = self._propagate_skip_status(chapter_tree)

            # ç»Ÿè®¡ä¿¡æ¯
            stats = self._calculate_statistics(chapter_tree)

            self.logger.info(f"ç»“æ„è§£æå®Œæˆ: æ‰¾åˆ° {stats['total_chapters']} ä¸ªç« èŠ‚")

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "default"
            }

        except Exception as e:
            self.logger.error(f"æ–‡æ¡£ç»“æ„è§£æå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "chapters": [],
                "statistics": {},
                "error": str(e),
                "method": "error"
            }

    # ============================================
    # ç‹¬ç«‹è§£ææ–¹æ³• - å¯å•ç‹¬è°ƒç”¨æˆ–ç»„åˆä½¿ç”¨
    # ============================================

    def parse_by_toc_exact(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•0: ç²¾ç¡®åŒ¹é…(åŸºäºç›®å½•)

        ç›´æ¥ä½¿ç”¨ç›®å½•é¡¹ç²¾ç¡®åŒ¹é…æ­£æ–‡ä½ç½®,é€Ÿåº¦å¿«ã€å‡†ç¡®ç‡é«˜

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "toc_exact"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            # æ£€æµ‹ç›®å½•
            toc_idx = self._find_toc_section(doc)
            if toc_idx is None:
                return {
                    "success": False,
                    "error": "æœªæ£€æµ‹åˆ°ç›®å½•,æ— æ³•ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ–¹æ³•",
                    "chapters": [],
                    "statistics": {},
                    "method": "toc_exact"
                }

            toc_items, toc_end_idx = self._parse_toc_items(doc, toc_idx)
            if not toc_items:
                return {
                    "success": False,
                    "error": "ç›®å½•è§£æå¤±è´¥",
                    "chapters": [],
                    "statistics": {},
                    "method": "toc_exact"
                }

            # ä½¿ç”¨ç²¾ç¡®åŒ¹é…
            chapters = self._locate_chapters_by_toc(doc, toc_items, toc_end_idx)
            chapter_tree = self._build_chapter_tree(chapters)
            chapter_tree = self._propagate_skip_status(chapter_tree)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "toc_exact"
            }

        except Exception as e:
            self.logger.error(f"ç²¾ç¡®åŒ¹é…è§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "toc_exact"
            }

    def parse_by_style(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•2: æ ·å¼è¯†åˆ«

        åŸºäºWordæ ‡é¢˜æ ·å¼è¯†åˆ«ç« èŠ‚

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "style"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            chapters = self._parse_chapters_from_doc(doc)
            chapters = self._locate_chapter_content(doc, chapters)
            chapter_tree = self._build_chapter_tree(chapters)
            chapter_tree = self._propagate_skip_status(chapter_tree)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "style"
            }

        except Exception as e:
            self.logger.error(f"æ ·å¼è¯†åˆ«è§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "style"
            }

    def parse_by_outline_level(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•5: Wordå¤§çº²çº§åˆ«è¯†åˆ«

        ä½¿ç”¨Wordæ–‡æ¡£å¤§çº²çº§åˆ«(Outline Level)è¯†åˆ«ç« èŠ‚

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "outline_level"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            chapters = self._parse_chapters_by_outline_level(doc)
            chapters = self._locate_chapter_content(doc, chapters)
            chapter_tree = self._build_chapter_tree(chapters)
            chapter_tree = self._propagate_skip_status(chapter_tree)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "outline_level"
            }

        except Exception as e:
            self.logger.error(f"å¤§çº²çº§åˆ«è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "outline_level"
            }

    def parse_by_semantic_anchors(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•1: è¯­ä¹‰é”šç‚¹è§£æ

        åŸºäºç›®å½•ä¿¡æ¯ï¼Œä½¿ç”¨è¯­ä¹‰åŒ¹é…æ–¹æ³•å®šä½ç« èŠ‚ä½ç½®

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "semantic_anchors"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            # æ£€æµ‹ç›®å½•
            toc_idx = self._find_toc_section(doc)
            if toc_idx is None:
                return {
                    "success": False,
                    "error": "æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œæ— æ³•ä½¿ç”¨è¯­ä¹‰é”šç‚¹è§£æ",
                    "chapters": [],
                    "statistics": {},
                    "method": "semantic_anchors"
                }

            # è§£æç›®å½•
            toc_items, toc_end_idx = self._parse_toc_items(doc, toc_idx)
            if not toc_items:
                return {
                    "success": False,
                    "error": "ç›®å½•è§£æå¤±è´¥",
                    "chapters": [],
                    "statistics": {},
                    "method": "semantic_anchors"
                }

            # æå–ç›®æ ‡æ ‡é¢˜
            toc_targets = [item['title'] for item in toc_items]

            # ä½¿ç”¨è¯­ä¹‰é”šç‚¹è§£æ
            chapters = self._parse_chapters_by_semantic_anchors(doc, toc_targets, toc_end_idx)

            # ä¸ºæ¯ä¸ªç« èŠ‚è¯†åˆ«å­ç« èŠ‚
            for i, chapter in enumerate(chapters):
                subsections = self._parse_subsections_in_range(
                    doc,
                    chapter.para_start_idx,
                    chapter.para_end_idx,
                    chapter.level,
                    f"sem_{i}"
                )
                if subsections:
                    chapter.children = subsections

            # æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self._build_chapter_tree(chapters)
            chapter_tree = self._propagate_skip_status(chapter_tree)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "semantic_anchors"
            }

        except Exception as e:
            self.logger.error(f"è¯­ä¹‰é”šç‚¹è§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "semantic_anchors"
            }

    def parse_by_hybrid(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•3: æ··åˆå¯å‘å¼è¯†åˆ«

        ç»¼åˆå¤šç§ç‰¹å¾ï¼ˆç¼–å·ã€æ ·å¼ã€ç¼©è¿›ã€é•¿åº¦ï¼‰åˆ¤æ–­æ ‡é¢˜

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "hybrid"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))
            chapters = []

            # éå†æ‰€æœ‰æ®µè½ï¼Œä½¿ç”¨å¤šç»´åº¦è¯„åˆ†
            for i, para in enumerate(doc.paragraphs):
                text = para.text.strip()

                # åŸºç¡€è¿‡æ»¤
                if not text or len(text) > 150 or len(text) < 2:
                    continue

                # è®¡ç®—å¤šç»´åº¦å¾—åˆ†
                score = 0

                # ç‰¹å¾1: ç¼–å·æ¨¡å¼è¯†åˆ« (30åˆ†)
                numbering_patterns = [
                    (r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', 30),
                    (r'^\d+\.\s+\S', 25),
                    (r'^\d+\.\d+\s+\S', 20),
                    (r'^\d+\.\d+\.\d+\s+\S', 15),
                    (r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', 20),
                    (r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)', 15),
                ]

                for pattern, points in numbering_patterns:
                    if re.match(pattern, text):
                        score += points
                        break

                # ç‰¹å¾2: å­—ä½“å¤§å°å’ŒåŠ ç²— (25åˆ†)
                if para.runs:
                    sizes = []
                    bold_count = 0
                    total_runs = len(para.runs)

                    for run in para.runs:
                        if run.font.size:
                            sizes.append(run.font.size.pt)
                        if run.bold:
                            bold_count += 1

                    # åŠ ç²—æ¯”ä¾‹
                    if bold_count >= total_runs * 0.5:
                        score += 10

                    # å­—ä½“å¤§å°
                    if sizes:
                        avg_size = sum(sizes) / len(sizes)
                        if avg_size >= 16:
                            score += 15
                        elif avg_size >= 13:
                            score += 10
                        elif avg_size >= 10:
                            score += 5

                # ç‰¹å¾3: æ®µè½ç¼©è¿› (20åˆ†)
                try:
                    if para.paragraph_format.left_indent:
                        indent_pt = para.paragraph_format.left_indent.pt
                        if indent_pt == 0:
                            score += 20
                        elif indent_pt <= 10:
                            score += 10
                        elif indent_pt <= 20:
                            score += 5
                except (AttributeError, TypeError):
                    score += 10

                # ç‰¹å¾4: å†…å®¹é•¿åº¦ (15åˆ†)
                text_len = len(text)
                if text_len <= 30:
                    score += 15
                elif text_len <= 50:
                    score += 10
                elif text_len <= 80:
                    score += 5

                # ç‰¹å¾5: ä½ç½®ç‰¹å¾ (10åˆ†)
                if i < len(doc.paragraphs) * 0.1:
                    score += 10
                elif i < len(doc.paragraphs) * 0.3:
                    score += 5

                # åˆ¤æ–­é˜ˆå€¼: 60åˆ†ä»¥ä¸Šè®¤ä¸ºæ˜¯æ ‡é¢˜
                if score >= 60:
                    level = self._determine_level_by_numbering(text)

                    chapter = ChapterNode(
                        id=f"hybrid_{i}",
                        level=level,
                        title=text,
                        para_start_idx=i,
                        para_end_idx=i,
                        word_count=0,
                        preview_text="",
                        auto_selected=False,
                        skip_recommended=False,
                        content_tags=[f'score_{score}']
                    )
                    chapters.append(chapter)

            if not chapters:
                return {
                    "success": False,
                    "error": "æœªè¯†åˆ«åˆ°ç« èŠ‚",
                    "chapters": [],
                    "statistics": {},
                    "method": "hybrid"
                }

            # å®šä½ç« èŠ‚å†…å®¹
            chapters = self._locate_chapter_content(doc, chapters)
            chapter_tree = self._build_chapter_tree(chapters)
            chapter_tree = self._propagate_skip_status(chapter_tree)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "hybrid"
            }

        except Exception as e:
            self.logger.error(f"æ··åˆå¯å‘å¼è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "hybrid"
            }

    def parse_by_azure(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•4: Azure Form Recognizerè§£æ

        ä½¿ç”¨Azure AIæœåŠ¡è¯†åˆ«æ–‡æ¡£ç»“æ„

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "azure"
            }
        """
        try:
            # æ£€æŸ¥Azureè§£æå™¨æ˜¯å¦å¯ç”¨
            try:
                from modules.tender_processing.azure_parser import AzureDocumentParser, is_azure_available

                if not is_azure_available():
                    return {
                        "success": False,
                        "error": "Azure Form Recognizeræœªé…ç½®æˆ–SDKæœªå®‰è£…",
                        "chapters": [],
                        "statistics": {},
                        "method": "azure"
                    }

                doc_path_abs = resolve_file_path(doc_path)
                if not doc_path_abs:
                    raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

                # ä½¿ç”¨Azureè§£æå™¨
                azure_parser = AzureDocumentParser()
                result = azure_parser.parse_document_structure(str(doc_path_abs))
                return result

            except ImportError as e:
                return {
                    "success": False,
                    "error": f"Azureè§£æå™¨ä¸å¯ç”¨: {str(e)}",
                    "chapters": [],
                    "statistics": {},
                    "method": "azure"
                }

        except Exception as e:
            self.logger.error(f"Azureè§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "azure"
            }

    def parse_by_gemini(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•6: Gemini AIè§£æ

        ä½¿ç”¨Google Gemini AIæ¨¡å‹è¯†åˆ«æ–‡æ¡£ç»“æ„

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "gemini"
            }
        """
        try:
            # æ£€æŸ¥Geminiè§£æå™¨æ˜¯å¦å¯ç”¨
            try:
                from modules.tender_processing.parsers.gemini_parser import GeminiParser

                gemini_parser = GeminiParser()

                if not gemini_parser.is_available():
                    return {
                        "success": False,
                        "error": "Gemini APIå¯†é’¥æœªé…ç½®",
                        "chapters": [],
                        "statistics": {},
                        "method": "gemini"
                    }

                doc_path_abs = resolve_file_path(doc_path)
                if not doc_path_abs:
                    raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

                # ä½¿ç”¨Geminiè§£æå™¨
                result = gemini_parser.parse_structure(str(doc_path_abs))

                # ç¡®ä¿chaptersæ˜¯dictæ ¼å¼
                if result.get('success') and result.get('chapters'):
                    chapters = result['chapters']
                    if chapters and hasattr(chapters[0], 'to_dict'):
                        result['chapters'] = [ch.to_dict() if hasattr(ch, 'to_dict') else ch for ch in chapters]

                return result

            except ImportError as e:
                return {
                    "success": False,
                    "error": f"Geminiè§£æå™¨ä¸å¯ç”¨: {str(e)} (pip install google-generativeai)",
                    "chapters": [],
                    "statistics": {},
                    "method": "gemini"
                }

        except Exception as e:
            self.logger.error(f"Geminiè§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "gemini"
            }

    def _is_title_page_content(self, para_idx: int, para_text: str, total_paras: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜é¡µå†…å®¹ (ä¼˜åŒ–5: è¿‡æ»¤æ ‡é¢˜é¡µ)

        è§„åˆ™:
        1. ä½äºå‰10ä¸ªæ®µè½
        2. ä¸åŒ…å«ç« èŠ‚ç¼–å·
        3. å­—ä½“å¾ˆå¤§ä½†æ²¡æœ‰"ç¬¬Xç« "ç­‰å…³é”®è¯
        4. åŒ…å«"å…¬å¸"ã€"é¡¹ç›®"ç­‰çº¯åç§°ç‰¹å¾

        Args:
            para_idx: æ®µè½ç´¢å¼•
            para_text: æ®µè½æ–‡æœ¬
            total_paras: æ–‡æ¡£æ€»æ®µè½æ•°

        Returns:
            True è¡¨ç¤ºåº”è¯¥è¢«è¿‡æ»¤
        """
        # åªæ£€æŸ¥å‰10æ®µ
        if para_idx >= 10:
            return False

        # åŒ…å«ç« èŠ‚ç¼–å·åˆ™ä¸æ˜¯æ ‡é¢˜é¡µ
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]', para_text):
            return False

        if re.match(r'^\d+\.\s', para_text):
            return False

        # çº¯å…¬å¸åç§°æ¨¡å¼
        if re.match(r'^.{2,50}(æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|é›†å›¢æœ‰é™å…¬å¸|é›†å›¢)$', para_text):
            self.logger.debug(f"æ ‡é¢˜é¡µè¿‡æ»¤: è¯†åˆ«ä¸ºå…¬å¸åç§° '{para_text}'")
            return True

        # çº¯é¡¹ç›®åç§°æ¨¡å¼ (ä¸åŒ…å«"é¡¹ç›®éœ€æ±‚"ã€"é¡¹ç›®è¦æ±‚"ç­‰)
        if re.match(r'^.{5,50}é¡¹ç›®$', para_text) and 'éœ€æ±‚' not in para_text and 'è¦æ±‚' not in para_text and 'æ¦‚å†µ' not in para_text:
            self.logger.debug(f"æ ‡é¢˜é¡µè¿‡æ»¤: è¯†åˆ«ä¸ºé¡¹ç›®åç§° '{para_text}'")
            return True

        # çº¯å¹´ä»½é¡¹ç›®æ¨¡å¼ (å¦‚ "2025å¹´XXé¡¹ç›®")
        if re.match(r'^\d{4}å¹´.{5,50}é¡¹ç›®$', para_text) and 'éœ€æ±‚' not in para_text:
            self.logger.debug(f"æ ‡é¢˜é¡µè¿‡æ»¤: è¯†åˆ«ä¸ºå¹´ä»½é¡¹ç›®åç§° '{para_text}'")
            return True

        # çº¯æ–‡æ¡£åç§°æ¨¡å¼ (å¦‚ "é‡‡è´­éœ€æ±‚æ–‡ä»¶"ä½†æ²¡æœ‰ç¼–å·)
        if re.match(r'^.{2,15}(æ–‡ä»¶|æ–‡æ¡£|ææ–™|èµ„æ–™)$', para_text) and para_idx < 5:
            # è¿›ä¸€æ­¥åˆ¤æ–­: å¦‚æœåé¢ç´§è·Ÿ"ç¬¬ä¸€ç« "ç­‰ï¼Œåˆ™æ˜¯æ ‡é¢˜é¡µ
            if para_idx + 1 < total_paras:
                next_para_match = False
                for i in range(para_idx + 1, min(para_idx + 3, total_paras)):
                    # æ£€æŸ¥åç»­æ®µè½æ˜¯å¦åŒ…å«ç« èŠ‚æ ‡è®°
                    # è¿™é‡Œåªæ£€æŸ¥æ–‡æœ¬ï¼Œä¸éœ€è¦è¯»å–å®é™…æ®µè½å¯¹è±¡
                    pass  # æš‚æ—¶ä¿ç•™ç®€åŒ–é€»è¾‘
            self.logger.debug(f"æ ‡é¢˜é¡µè¿‡æ»¤: è¯†åˆ«ä¸ºæ–‡æ¡£åç§° '{para_text}'")
            return True

        return False

    def _is_valid_chapter_title(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯åˆæ³•çš„ç« èŠ‚æ ‡é¢˜ (è¿‡æ»¤åˆ—è¡¨é¡¹ã€è¯´æ˜æ€§å†…å®¹ç­‰)

        Args:
            text: æ®µè½æ–‡æœ¬

        Returns:
            True è¡¨ç¤ºå¯èƒ½æ˜¯ç« èŠ‚æ ‡é¢˜ï¼ŒFalse è¡¨ç¤ºåº”è¯¥è¢«è¿‡æ»¤
        """
        if not text or len(text.strip()) == 0:
            return False

        # 1. è¿‡æ»¤çº¯ç¼–å·ï¼ˆå¦‚ "1"ã€"1.1"ï¼‰
        if re.match(r'^[\d\.]+$', text):
            return False

        # 2. è¿‡æ»¤çº¯åˆ—è¡¨é¡¹æ ‡è®°ï¼ˆå¦‚ "1."ã€"2."ã€"(1)"ï¼‰
        if re.match(r'^[\d]+\.$', text):  # "1." "2."
            return False
        if re.match(r'^\([\d]+\)$', text):  # "(1)" "(2)"
            return False
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€$', text):  # "ä¸€ã€" "äºŒã€"
            return False

        # 3. è¿‡æ»¤è¯´æ˜æ€§å†…å®¹çš„ç‰¹å¾å…³é”®è¯
        note_keywords = ['æ³¨ï¼š', 'æ³¨:', 'è¯´æ˜ï¼š', 'è¯´æ˜:', 'å¤‡æ³¨ï¼š', 'å¤‡æ³¨:']
        if any(text.startswith(kw) for kw in note_keywords):
            # "æ³¨ï¼š"æœ¬èº«å¯èƒ½ä¸æ˜¯ç« èŠ‚ï¼Œä½†å¦‚æœåé¢æœ‰å†…å®¹å¯èƒ½æ˜¯
            # å¦‚æœåªæœ‰"æ³¨ï¼š"ä¸¤ä¸ªå­—ï¼Œè‚¯å®šä¸æ˜¯ç« èŠ‚æ ‡é¢˜
            if len(text) <= 4:
                return False

        # 4. è¿‡æ»¤è¶…é•¿å†…å®¹ï¼ˆç« èŠ‚æ ‡é¢˜ä¸€èˆ¬ä¸è¶…è¿‡50ä¸ªå­—ç¬¦ï¼‰
        # ä½†å…è®¸åŒ…å«é•¿æ¡æ¬¾ç¼–å·çš„ç« èŠ‚ï¼ˆå¦‚ "7.3 åŒæ–¹å‡åº”å½“..."ï¼‰
        if len(text) > 100:  # è¶…è¿‡100å­—ç¬¦è‚¯å®šä¸æ˜¯æ ‡é¢˜
            return False

        # 5. è¿‡æ»¤çº¯åˆ—è¡¨é¡¹æ ¼å¼ï¼ˆå¼€å¤´æ˜¯ç¼–å·+ç®€çŸ­å†…å®¹ï¼‰
        # ä¾‹å¦‚: "1.ä»¥ä¸Šå“åº”æ–‡ä»¶çš„æ„æˆä¸ºå¿…é¡»åŒ…å«çš„å†…å®¹..."
        if re.match(r'^[\d]+\.[\u4e00-\u9fa5]{2,}', text):
            # å¦‚æœåŒ¹é… "æ•°å­—.ä¸­æ–‡"ï¼Œä¸”æ²¡æœ‰æ˜ç¡®çš„ç« èŠ‚ç‰¹å¾è¯ï¼Œåˆ™å¯èƒ½æ˜¯åˆ—è¡¨é¡¹
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç« èŠ‚ç‰¹å¾è¯
            chapter_markers = ['ç« ', 'èŠ‚', 'éƒ¨åˆ†', 'ç¬¬', 'æ¡æ¬¾', 'è¦æ±‚', 'æ ‡å‡†', 'æ–¹æ³•', 'åŸåˆ™']
            if not any(marker in text for marker in chapter_markers):
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå¦‚æœå†…å®¹å¾ˆé•¿ï¼ˆ>20å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯è¯´æ˜æ€§åˆ—è¡¨é¡¹
                if len(text) > 30:
                    return False

        # 6. ç‰¹æ®Šè¿‡æ»¤ï¼šæ’é™¤çº¯æ¡æ¬¾ç¼–å·ï¼ˆå¦‚ "1.1.1"ã€"2.3.4.5"ï¼‰
        if re.match(r'^[\d]+\.[\d]+\.[\d]+(\.[\d]+)*$', text):
            return False

        # 7. ç‰¹æ®Šè¿‡æ»¤ï¼šæ’é™¤ä»¥æ¡æ¬¾ç¼–å·å¼€å¤´ä¸”è¿‡é•¿çš„å†…å®¹
        # ä¾‹å¦‚: "7.3 åŒæ–¹å‡åº”å½“ä¸¥æ ¼æŒ‰ç…§ç›¸å…³æ³•å¾‹æ³•è§„..."ï¼ˆè¿™ä¸æ˜¯ç« èŠ‚æ ‡é¢˜ï¼‰
        if re.match(r'^[\d]+\.[\d]+\s+', text) and len(text) > 50:
            return False

        return True

    def _parse_chapters_from_doc(self, doc: Document) -> List[ChapterNode]:
        """
        ä» Word æ–‡æ¡£ä¸­è§£æç« èŠ‚ (ä¼˜åŒ–5: æ·»åŠ æ ‡é¢˜é¡µè¿‡æ»¤)

        Args:
            doc: python-docx Document å¯¹è±¡

        Returns:
            ç« èŠ‚åˆ—è¡¨ï¼ˆæ‰å¹³ç»“æ„ï¼Œæœªæ„å»ºæ ‘ï¼‰
        """
        chapters = []
        chapter_counter = 0

        for para_idx, paragraph in enumerate(doc.paragraphs):
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜
            level = self._get_heading_level(paragraph)

            if level > 0:
                title = paragraph.text.strip()

                # è·³è¿‡ç©ºæ ‡é¢˜
                if not title:
                    continue

                # æ–°å¢: è¿‡æ»¤æ ‡é¢˜é¡µå†…å®¹
                if self._is_title_page_content(para_idx, title, len(doc.paragraphs)):
                    self.logger.debug(f"è·³è¿‡æ ‡é¢˜é¡µå†…å®¹: æ®µè½{para_idx} '{title}'")
                    continue

                # åˆ¤æ–­æ˜¯å¦åŒ¹é…ç™½/é»‘åå•
                auto_selected = self._matches_whitelist(title)
                skip_recommended = self._matches_blacklist(title)

                # å¦‚æœåœ¨é»‘åå•ä¸­ï¼Œåˆ™ä¸è‡ªåŠ¨é€‰ä¸­
                if skip_recommended:
                    auto_selected = False

                chapter = ChapterNode(
                    id=f"ch_{chapter_counter}",
                    level=level,
                    title=title,
                    para_start_idx=para_idx,
                    para_end_idx=None,  # ç¨åè®¡ç®—
                    word_count=0,       # ç¨åè®¡ç®—
                    preview_text="",    # ç¨åæå–
                    auto_selected=auto_selected,
                    skip_recommended=skip_recommended
                )

                chapters.append(chapter)
                chapter_counter += 1

                self.logger.debug(
                    f"æ‰¾åˆ°ç« èŠ‚ [{level}çº§]: {title} "
                    f"{'âœ…è‡ªåŠ¨é€‰ä¸­' if auto_selected else 'âŒè·³è¿‡' if skip_recommended else 'âšªé»˜è®¤'}"
                )

        return chapters

    def _parse_chapters_by_outline_level(self, doc: Document) -> List[ChapterNode]:
        """
        æ–¹æ³•äº”ï¼šåŸºäºWordå¤§çº²çº§åˆ«ï¼ˆoutlineLevelï¼‰è¯†åˆ«ç« èŠ‚

        ä½¿ç”¨å¾®è½¯å®˜æ–¹çš„å¤§çº²çº§åˆ«APIï¼Œæ”¯æŒ0-8çº§æ ‡é¢˜è¯†åˆ«ï¼Œ
        åŒ…å«3å±‚ç²¾ç»†è¿‡æ»¤è§„åˆ™å’ŒHeadingæ ·å¼å¤‡ç”¨æ£€æµ‹ã€‚

        Args:
            doc: python-docx Document å¯¹è±¡

        Returns:
            ç« èŠ‚åˆ—è¡¨ï¼ˆæ‰å¹³ç»“æ„ï¼Œæœªæ„å»ºæ ‘ï¼‰
        """
        import re

        chapters = []
        chapter_counter = 0

        self.logger.info("ä½¿ç”¨æ–¹æ³•äº”ï¼šå¤§çº²çº§åˆ«è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼‰")

        # ç›´æ¥ä»Wordæ–‡æ¡£æå–æ ‡é¢˜ - ä½¿ç”¨å¾®è½¯å®˜æ–¹çš„å¤§çº²çº§åˆ«API
        for para_idx, paragraph in enumerate(doc.paragraphs):
            is_heading = False
            level = 0
            detection_method = ""

            # â­ ä¼˜å…ˆçº§1: æ£€æŸ¥å¤§çº²çº§åˆ« (Outline Level) - å¾®è½¯å®˜æ–¹è¯­ä¹‰æ ‡è®°
            # è¿™æ˜¯Wordå¯¼èˆªçª—æ ¼å’Œå¤§çº²è§†å›¾ä½¿ç”¨çš„ç»“æ„ï¼Œå‡†ç¡®åº¦æœ€é«˜
            try:
                pPr = paragraph._element.pPr
                if pPr is not None:
                    outlineLvl = pPr.outlineLvl
                    if outlineLvl is not None:
                        outline_level_val = int(outlineLvl.val)
                        # Wordå¤§çº²çº§åˆ«: 0-8è¡¨ç¤ºæ ‡é¢˜(0=ä¸€çº§), 9è¡¨ç¤ºæ­£æ–‡
                        if outline_level_val <= 8:
                            # ğŸ”§ æ·»åŠ è¿‡æ»¤è§„åˆ™ï¼Œæ’é™¤å™ªéŸ³å†…å®¹
                            text = paragraph.text.strip()
                            should_skip = False

                            # è¿‡æ»¤1: è·³è¿‡æ–‡æ¡£å‰30æ®µçš„å°é¢/å…ƒæ•°æ®ï¼ˆLevel 0ï¼‰
                            if para_idx < 30 and outline_level_val == 0:
                                metadata_keywords = ['é¡¹ç›®ç¼–å·', 'æ‹›æ ‡äºº', 'ä»£ç†æœºæ„', 'è”ç³»äºº', 'è”ç³»æ–¹å¼',
                                                    'åœ°å€', 'ç”µè¯', 'ä¼ çœŸ', 'é‚®ç¼–', 'ç½‘å€', 'http']
                                if any(kw in text for kw in metadata_keywords):
                                    should_skip = True
                                    self.logger.debug(f"è¿‡æ»¤å°é¢: æ®µè½{para_idx} '{text[:30]}'")

                            # è¿‡æ»¤2: è·³è¿‡Level 3-4çš„é•¿æ¡æ¬¾å†…å®¹
                            if not should_skip and outline_level_val >= 3:
                                # å½¢å¦‚ "1.1 è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„è¯´æ˜æ–‡å­—..." çš„æ˜¯æ¡æ¬¾ï¼Œä¸æ˜¯æ ‡é¢˜
                                if re.match(r'^\d+\.\d+\s+.{15,}', text):
                                    should_skip = True
                                    self.logger.debug(f"è¿‡æ»¤æ¡æ¬¾: æ®µè½{para_idx} '{text[:30]}'")

                            # è¿‡æ»¤3: æ ‡é¢˜é•¿åº¦é™åˆ¶ï¼ˆè¶…è¿‡50å­—çš„é€šå¸¸ä¸æ˜¯æ ‡é¢˜ï¼‰
                            if not should_skip and len(text) > 50:
                                # é™¤éæœ‰æ˜ç¡®çš„ç« èŠ‚ç¼–å·
                                if not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', text):
                                    should_skip = True
                                    self.logger.debug(f"è¿‡æ»¤é•¿æ–‡æœ¬: æ®µè½{para_idx} '{text[:30]}'")

                            if not should_skip:
                                is_heading = True
                                level = outline_level_val + 1  # è½¬æ¢: 0â†’1çº§, 1â†’2çº§, ...
                                detection_method = f"å¤§çº²çº§åˆ«{outline_level_val}"
            except (AttributeError, TypeError, ValueError):
                pass  # æ²¡æœ‰å¤§çº²çº§åˆ«ï¼Œç»§ç»­å…¶ä»–æ–¹æ³•

            # ä¼˜å…ˆçº§2: æ£€æŸ¥æ ‡å‡†Headingæ ·å¼ (å¤‡ç”¨æ–¹æ¡ˆ)
            if not is_heading:
                style_name = paragraph.style.name if paragraph.style else ""

                # åªæ¥å—æ ‡å‡†çš„Headingæ ·å¼ï¼ˆç²¾ç¡®åŒ¹é…ï¼Œé¿å…è¯¯è¯†åˆ«ï¼‰
                if style_name.startswith('Heading '):  # 'Heading 1', 'Heading 2'
                    match = re.search(r'Heading (\d+)', style_name)
                    if match:
                        is_heading = True
                        level = int(match.group(1))
                        detection_method = f"æ ·å¼{style_name}"
                elif style_name.startswith('æ ‡é¢˜ '):  # 'æ ‡é¢˜ 1', 'æ ‡é¢˜ 2'
                    match = re.search(r'æ ‡é¢˜ (\d+)', style_name)
                    if match:
                        is_heading = True
                        level = int(match.group(1))
                        detection_method = f"æ ·å¼{style_name}"

            if is_heading and paragraph.text.strip():
                title = paragraph.text.strip()

                # åˆ¤æ–­æ˜¯å¦åŒ¹é…ç™½/é»‘åå•
                auto_selected = self._matches_whitelist(title)
                skip_recommended = self._matches_blacklist(title)

                if skip_recommended:
                    auto_selected = False

                chapter = ChapterNode(
                    id=f"docx_{chapter_counter}",
                    level=level if level > 0 else 1,
                    title=title,
                    para_start_idx=para_idx,
                    para_end_idx=None,  # ç¨åè®¡ç®—
                    word_count=0,       # ç¨åè®¡ç®—
                    preview_text="",    # ç¨åæå–
                    auto_selected=auto_selected,
                    skip_recommended=skip_recommended,
                    content_tags=['docx_native', detection_method]
                )

                chapters.append(chapter)
                chapter_counter += 1

                self.logger.debug(
                    f"è¯†åˆ«æ ‡é¢˜: æ®µè½{para_idx} [{detection_method}] '{title[:50]}' "
                    f"{'âœ…è‡ªåŠ¨é€‰ä¸­' if auto_selected else 'âŒè·³è¿‡' if skip_recommended else 'âšªé»˜è®¤'}"
                )

        self.logger.info(f"âœ… æ–¹æ³•äº”è¯†åˆ«å®Œæˆï¼šæ‰¾åˆ° {len(chapters)} ä¸ªæ ‡é¢˜")
        return chapters

    def _get_heading_level(self, paragraph) -> int:
        """
        è·å–æ®µè½çš„æ ‡é¢˜å±‚çº§ (ä¼˜åŒ–6: æ·»åŠ å‰ç½®è¿‡æ»¤å’Œå¤§çº²çº§åˆ«æ£€æŸ¥)

        Args:
            paragraph: python-docx Paragraph å¯¹è±¡

        Returns:
            0: ä¸æ˜¯æ ‡é¢˜
            1-3: æ ‡é¢˜å±‚çº§
        """
        text = paragraph.text.strip()

        # âš ï¸ æ–¹æ³•0ï¼šå‰ç½®è¿‡æ»¤ - æ’é™¤æ˜æ˜¾ä¸æ˜¯ç« èŠ‚çš„å†…å®¹
        if not self._is_valid_chapter_title(text):
            return 0

        # â­ æ–¹æ³•1ï¼šå¤§çº²çº§åˆ«åˆ¤æ–­ï¼ˆæœ€å¯é ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
        try:
            pPr = paragraph._element.pPr
            if pPr is not None:
                outlineLvl = pPr.outlineLvl
                if outlineLvl is not None:
                    level = int(outlineLvl.val)
                    if level <= 2:  # 0=ä¸€çº§, 1=äºŒçº§, 2=ä¸‰çº§
                        self.logger.debug(f"âœ“ å¤§çº²çº§åˆ«è¯†åˆ«: Level {level} â†’ '{text[:30]}'")
                        return level + 1  # è½¬æ¢ä¸º1-3
        except (AttributeError, TypeError):
            pass  # æ²¡æœ‰å¤§çº²çº§åˆ«ï¼Œç»§ç»­å…¶ä»–æ–¹æ³•

        # æ–¹æ³•2ï¼šé€šè¿‡æ ·å¼ååˆ¤æ–­ (ä¼˜å…ˆï¼Œæœ€å¯é )
        if paragraph.style and paragraph.style.name:
            style_name = paragraph.style.name
            for level, style_names in self.HEADING_STYLES.items():
                if any(sn.lower() in style_name.lower() for sn in style_names):
                    return level

        # æ–¹æ³•3ï¼šé€šè¿‡ XML æ ·å¼å±æ€§åˆ¤æ–­ï¼ˆæ›´å‡†ç¡®ï¼‰
        try:
            pPr = paragraph._element.pPr
            if pPr is not None:
                pStyle = pPr.pStyle
                if pStyle is not None:
                    style_val = pStyle.val
                    if 'heading1' in style_val.lower() or style_val.lower() == '1':
                        return 1
                    elif 'heading2' in style_val.lower() or style_val.lower() == '2':
                        return 2
                    elif 'heading3' in style_val.lower() or style_val.lower() == '3':
                        return 3
        except (AttributeError, TypeError):
            pass  # XMLå±æ€§ä¸å¯ç”¨æ—¶ä½¿ç”¨å…¶ä»–æ–¹æ³•

        # æ–¹æ³•4ï¼šé€šè¿‡æ–‡æœ¬æ ¼å¼å¯å‘å¼åˆ¤æ–­ (ä¼˜åŒ–: æ”¶é›†æ‰€æœ‰runçš„ä¿¡æ¯)
        if paragraph.runs:
            # æ”¶é›†æ‰€æœ‰runçš„å­—ä½“ä¿¡æ¯
            sizes = []
            bold_count = 0

            for run in paragraph.runs:
                if run.font.size:
                    sizes.append(run.font.size.pt)
                if run.bold:
                    bold_count += 1

            # è‡³å°‘ä¸€åŠçš„runæ˜¯åŠ ç²—ï¼Œæ‰è®¤ä¸ºæ˜¯æ ‡é¢˜
            if sizes and bold_count >= len(paragraph.runs) / 2:
                avg_size = sum(sizes) / len(sizes)

                # è°ƒæ•´é˜ˆå€¼: æ›´çµæ´»çš„åˆ¤æ–­ (é™ä½é˜ˆå€¼ä»¥æé«˜è¯†åˆ«ç‡)
                if avg_size >= 16:  # 16pt+ â†’ Level 1 (åŸ18pt)
                    return 1
                elif avg_size >= 13:  # 13-15pt â†’ Level 2 (åŸ15pt)
                    return 2
                elif avg_size >= 10:  # 10-12pt â†’ Level 3 (åŸ12pt)
                    return 3

        # æ–¹æ³•5ï¼šé€šè¿‡ç¼–å·æ¨¡å¼åˆ¤æ–­ (å¢å¼ºfallbackæœºåˆ¶)
        # ä¸€çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', text):
            return 1
        if re.match(r'^\d+\.\s+\S', text) and not re.match(r'^\d+\.\d+', text):  # 1. xxx (ä¸åŒ…å«1.1)
            return 1

        # äºŒçº§æ ‡é¢˜æ¨¡å¼ (å¦‚æœæ–‡æœ¬è¾ƒçŸ­ä¸”æœ‰ç¼–å·)
        if re.match(r'^\d+\.\d+\s+\S', text) and not re.match(r'^\d+\.\d+\.\d+', text):
            if len(text.strip()) <= 100:  # é•¿åº¦é™åˆ¶
                return 2

        # ä¸‰çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^\d+\.\d+\.\d+\s+\S', text):
            if len(text.strip()) <= 100:
                return 3

        return 0

    def _clean_title_v2(self, title: str, aggressive=False) -> str:
        """
        åˆ†é˜¶æ®µæ¸…ç†æ ‡é¢˜æ–‡æœ¬ (ä¼˜åŒ–3: æ¸©å’Œ/æ¿€è¿›ä¸¤ç§æ¨¡å¼)

        Args:
            title: åŸå§‹æ ‡é¢˜
            aggressive: æ˜¯å¦ä½¿ç”¨æ¿€è¿›æ¸…ç†æ¨¡å¼

        Returns:
            æ¸…ç†åçš„æ ‡é¢˜
        """
        if not aggressive:
            # æ¸©å’Œæ¸…ç†: åªåˆ é™¤æ˜æ˜¾çš„ç¼–å·å’Œç©ºæ ¼
            cleaned = re.sub(r'^\d+\.\s*', '', title)  # åˆ é™¤ "1. "
            cleaned = re.sub(r'^\d+\.\d+\s*', '', cleaned)  # åˆ é™¤ "1.1 "
            cleaned = re.sub(r'^\d+\.\d+\.\d+\s*', '', cleaned)  # åˆ é™¤ "1.1.1 "
            cleaned = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*', '', cleaned)  # åˆ é™¤ "ç¬¬ä¸€ç«  "
            cleaned = re.sub(r'\s+', '', cleaned)  # åˆ é™¤ç©ºæ ¼
            return cleaned
        else:
            # æ¿€è¿›æ¸…ç†: åˆ é™¤æ‰€æœ‰ç¼–å·ã€ç¬¦å·å’Œç©ºæ ¼
            cleaned = title
            cleaned = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s*', '', cleaned)
            cleaned = re.sub(r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)\s*', '', cleaned)
            cleaned = re.sub(r'^\d+[-\.]\d*\s*', '', cleaned)
            cleaned = re.sub(r'^[A-Za-z]+\.\s*', '', cleaned)
            cleaned = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*', '', cleaned)
            cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', cleaned)  # åªä¿ç•™ä¸­è‹±æ–‡æ•°å­—
            return cleaned

    def fuzzy_match_title_v2(self, title1: str, title2: str, threshold=0.7) -> float:
        """
        æ¨¡ç³ŠåŒ¹é…æ ‡é¢˜ï¼Œæ”¯æŒå¤šçº§æ¸…ç†å°è¯• (ä¼˜åŒ–3: åˆ†é˜¶æ®µåŒ¹é…)

        Args:
            title1: æ ‡é¢˜1
            title2: æ ‡é¢˜2
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            ç›¸ä¼¼åº¦å¾—åˆ† (0.0-1.0)
        """
        # Level 1: åŸå§‹æ¯”è¾ƒ
        if title1 == title2:
            return 1.0

        # Level 2: æ¸©å’Œæ¸…ç†åæ¯”è¾ƒ
        clean1 = self._clean_title_v2(title1, aggressive=False)
        clean2 = self._clean_title_v2(title2, aggressive=False)

        if clean1 == clean2:
            return 0.95

        if clean1 in clean2 or clean2 in clean1:
            return 0.90

        # Level 3: æ¿€è¿›æ¸…ç†åæ¯”è¾ƒ
        aggr1 = self._clean_title_v2(title1, aggressive=True)
        aggr2 = self._clean_title_v2(title2, aggressive=True)

        if aggr1 == aggr2:
            return 0.85

        if aggr1 in aggr2 or aggr2 in aggr1:
            shorter = aggr1 if len(aggr1) <= len(aggr2) else aggr2
            longer = aggr2 if len(aggr1) <= len(aggr2) else aggr1
            return len(shorter) / len(longer) * 0.80  # åŒ…å«åŒ¹é…ï¼Œæ ¹æ®é•¿åº¦æ¯”ä¾‹æ‰“åˆ†

        # Level 4: SequenceMatcherç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, aggr1, aggr2).ratio()

        return similarity

    def _matches_whitelist(self, title: str) -> bool:
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ¹é…ç™½åå•"""
        return any(keyword in title for keyword in self.WHITELIST_KEYWORDS)

    def _matches_blacklist(self, title: str) -> bool:
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ¹é…é»‘åå•"""
        # 1. å…³é”®è¯åŒ¹é…
        if any(keyword in title for keyword in self.BLACKLIST_KEYWORDS):
            return True

        # 2. ç‰¹æ®Šæ¨¡å¼åŒ¹é…
        special_patterns = [
            # åŒ¹é…çº¯å…¬å¸åç§°ç« èŠ‚ï¼ˆå¦‚ "ä¸­å›½å…‰å¤§é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸"ã€"XXXå…¬å¸"ï¼‰
            r'^.{2,30}(æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|é›†å›¢æœ‰é™å…¬å¸|é›†å›¢)$',
            # åŒ¹é…ç”²ä¹™ä¸™æ–¹å¼€å¤´çš„ç« èŠ‚
            r'^(ç”²æ–¹|ä¹™æ–¹|ä¸™æ–¹)[:ï¼š]',
            # åŒ¹é…çº¯é¡¹ç›®åç§°ç« èŠ‚ï¼ˆå¦‚ "XXXé¡¹ç›®"ï¼Œä½†ä¸åŒ…æ‹¬ "é¡¹ç›®éœ€æ±‚"ã€"é¡¹ç›®æ¦‚å†µ" ç­‰ï¼‰
            r'^.{1,20}é¡¹ç›®$',  # ä»¥"é¡¹ç›®"ç»“å°¾ï¼Œå‰é¢æ˜¯é¡¹ç›®åç§°
            # åŒ¹é…åˆåŒç¼–å·æ ¼å¼ç« èŠ‚
            r'.*ç¼–å·[:ï¼š].{0,50}$',  # åŒ…å« "ç¼–å·:" æˆ– "ç¼–å·ï¼š"
        ]

        for pattern in special_patterns:
            if re.match(pattern, title.strip()):
                return True

        # 3. åŒ¹é…ç©ºç™½æˆ–æçŸ­ç« èŠ‚ï¼ˆ< 3ä¸ªå­—ç¬¦ï¼Œå¯èƒ½æ˜¯æ ¼å¼é”™è¯¯ï¼‰
        if len(title.strip()) < 3:
            return True

        return False

    def _calculate_contract_density(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬çš„åˆåŒå¯†åº¦ï¼ˆåˆåŒç‰¹å¾è¯å‡ºç°é¢‘ç‡ï¼‰

        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬å†…å®¹

        Returns:
            åˆåŒå¯†åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
        """
        if not text or len(text) < 100:  # æ–‡æœ¬å¤ªçŸ­ï¼Œæ— æ³•åˆ¤æ–­
            return 0.0

        # å®šä¹‰åˆåŒç‰¹å¾è¯åŠå…¶æƒé‡
        contract_keywords = {
            # å¼ºåˆåŒç‰¹å¾ï¼ˆæƒé‡ x3ï¼‰
            'ç”²æ–¹': 3, 'ä¹™æ–¹': 3, 'ä¸™æ–¹': 3,
            # ä¸­ç­‰åˆåŒç‰¹å¾ï¼ˆæƒé‡ x2ï¼‰
            'è¿çº¦é‡‘': 2, 'è¿çº¦è´£ä»»': 2, 'å±¥çº¦ä¿è¯é‡‘': 2, 'æœ¬åˆåŒ': 2,
            'åˆåŒç”Ÿæ•ˆ': 2, 'åˆåŒç»ˆæ­¢': 2, 'åˆåŒè§£é™¤': 2,
            # å¼±åˆåŒç‰¹å¾ï¼ˆæƒé‡ x1ï¼‰
            'ä»˜æ¬¾': 1, 'éªŒæ”¶': 1, 'ä¿å¯†': 1, 'äº‰è®®': 1,
            'ä»²è£': 1, 'ç®¡è¾–': 1, 'åŒæ–¹': 1, 'ç­¾è®¢': 1
        }

        # è®¡ç®—åŠ æƒå‡ºç°æ¬¡æ•°
        total_weight = 0
        for keyword, weight in contract_keywords.items():
            count = text.count(keyword)
            total_weight += count * weight

        # è®¡ç®—å¯†åº¦ï¼ˆæ¯1000å­—ç¬¦çš„åŠ æƒå…³é”®è¯å‡ºç°æ¬¡æ•°ï¼‰
        text_length = len(text)
        density = (total_weight / text_length) * 1000

        # å½’ä¸€åŒ–åˆ°0-1ä¹‹é—´ï¼ˆå‡è®¾å¯†åº¦>50å°±æ˜¯100%çš„åˆåŒï¼‰
        normalized_density = min(density / 50.0, 1.0)

        return normalized_density

    def _is_contract_chapter(self, title: str, content_sample: str = None) -> tuple:
        """
        åˆ¤æ–­ç« èŠ‚æ˜¯å¦ä¸ºåˆåŒç« èŠ‚ï¼ˆç»“åˆæ ‡é¢˜å’Œå†…å®¹ç‰¹å¾ï¼‰

        Args:
            title: ç« èŠ‚æ ‡é¢˜
            content_sample: ç« èŠ‚å†…å®¹æ ·æœ¬ï¼ˆå¯é€‰ï¼‰

        Returns:
            (is_contract, density, reason): æ˜¯å¦ä¸ºåˆåŒç« èŠ‚ã€åˆåŒå¯†åº¦ã€åˆ¤æ–­ç†ç”±
        """
        # 1. åŸºäºæ ‡é¢˜çš„å¼ºåˆåŒæ ‡è¯†ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        strong_contract_keywords = [
            "åˆåŒæ¡æ¬¾", "åˆåŒæ–‡æœ¬", "åˆåŒèŒƒæœ¬", "åˆåŒæ ¼å¼", "åˆåŒåè®®",
            "é€šç”¨æ¡æ¬¾", "ä¸“ç”¨æ¡æ¬¾", "åˆåŒä¸»è¦æ¡æ¬¾", "åˆåŒè‰ç¨¿", "æ‹Ÿç­¾åˆåŒ",
            "æœåŠ¡åˆåŒ", "é‡‡è´­åˆåŒ", "ä¹°å–åˆåŒ", "é”€å”®åˆåŒ", "æ–½å·¥åˆåŒ",
            "åˆ†åŒ…åˆåŒ", "åŠ³åŠ¡åˆåŒ", "ç§ŸèµåˆåŒ", "å§”æ‰˜åˆåŒ", "ä»£ç†åˆåŒ",
        ]

        for keyword in strong_contract_keywords:
            if keyword in title:
                return (True, 1.0, f"æ ‡é¢˜å¼ºåŒ¹é…: '{keyword}'")

        # 2. åŸºäºå†…å®¹çš„åˆåŒå¯†åº¦æ£€æµ‹ï¼ˆæ ¸å¿ƒæ–°å¢åŠŸèƒ½ï¼‰
        if content_sample:
            density = self._calculate_contract_density(content_sample)

            # é˜ˆå€¼ï¼šå¯†åº¦ > 5% è®¤ä¸ºæ˜¯åˆåŒç« èŠ‚
            if density > 0.05:
                return (True, density, f"å†…å®¹å¯†åº¦: {density:.1%}")

        # 3. å¼±åˆåŒæ ‡è¯†ï¼ˆæ ‡é¢˜æ¨¡ç³Šä½†å¯èƒ½æ˜¯åˆåŒï¼‰
        weak_contract_patterns = [
            r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|é™„ä»¶\d*)[^\u4e00-\u9fa5]*(åè®®|æ¡æ¬¾|æƒåˆ©|ä¹‰åŠ¡)',
            r'åŒæ–¹.*æƒåˆ©.*ä¹‰åŠ¡',
            r'ç”².*ä¹™.*æ–¹',
        ]

        import re
        for pattern in weak_contract_patterns:
            if re.search(pattern, title):
                # å¦‚æœæœ‰å†…å®¹æ ·æœ¬ï¼Œè¿›ä¸€æ­¥éªŒè¯
                if content_sample:
                    density = self._calculate_contract_density(content_sample)
                    if density > 0.03:  # é™ä½é˜ˆå€¼åˆ°3%
                        return (True, density, f"æ ‡é¢˜å¼±åŒ¹é…+å†…å®¹éªŒè¯: {density:.1%}")

        return (False, 0.0, "éåˆåŒç« èŠ‚")

    def _extract_content_sample(self, doc: Document, para_start_idx: int, para_end_idx: int, sample_size: int = 2000) -> str:
        """
        æå–ç« èŠ‚å†…å®¹æ ·æœ¬ï¼ˆç”¨äºåˆåŒè¯†åˆ«ï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_start_idx: èµ·å§‹æ®µè½ç´¢å¼•
            para_end_idx: ç»“æŸæ®µè½ç´¢å¼•
            sample_size: æ ·æœ¬å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰

        Returns:
            å†…å®¹æ ·æœ¬å­—ç¬¦ä¸²
        """
        if para_end_idx is None or para_end_idx <= para_start_idx:
            return ""

        # æå–ç« èŠ‚å†…å®¹ï¼ˆè·³è¿‡æ ‡é¢˜æœ¬èº«ï¼‰
        content_paras = doc.paragraphs[para_start_idx + 1 : para_end_idx + 1]

        # åˆå¹¶æ–‡æœ¬
        sample_text = ""
        for para in content_paras:
            text = para.text.strip()
            if text:
                sample_text += text + "\n"
                if len(sample_text) >= sample_size:
                    break

        # æˆªå–æŒ‡å®šé•¿åº¦
        return sample_text[:sample_size]

    def _calculate_paragraph_contract_score(self, doc: Document, para_idx: int, window_size: int = 20) -> float:
        """
        è®¡ç®—æ®µè½å‘¨å›´çš„åˆåŒå¯†åº¦åˆ†æ•°ï¼ˆæ»‘åŠ¨çª—å£ï¼‰

        ç”¨äºåœ¨è¯†åˆ«ç« èŠ‚æ—¶å¿«é€Ÿåˆ¤æ–­è¯¥æ®µè½æ˜¯å¦ä½äºåˆåŒåŒºåŸŸå†…

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_idx: æ®µè½ç´¢å¼•
            window_size: çª—å£å¤§å°ï¼ˆæ£€æŸ¥å‰åNä¸ªæ®µè½ï¼Œé»˜è®¤20ï¼‰

        Returns:
            åˆåŒå¯†åº¦åˆ†æ•° (0.0-1.0)
        """
        # è®¡ç®—çª—å£èŒƒå›´ï¼ˆå‰åå„ä¸€åŠï¼‰
        half_window = window_size // 2
        start_idx = max(0, para_idx - half_window)
        end_idx = min(len(doc.paragraphs), para_idx + half_window + 1)

        # æå–çª—å£å†…çš„æ–‡æœ¬
        window_text = ""
        for i in range(start_idx, end_idx):
            para_text = doc.paragraphs[i].text.strip()
            if para_text:
                window_text += para_text + "\n"

        # ä½¿ç”¨ç°æœ‰çš„åˆåŒå¯†åº¦è®¡ç®—æ–¹æ³•
        density = self._calculate_contract_density(window_text)

        return density

    def _detect_contract_cluster_in_chapter(self, doc: Document, start_idx: int, end_idx: int) -> Optional[Dict]:
        """
        æ£€æµ‹ç« èŠ‚å†…çš„åˆåŒæ®µè½èšé›†åŒºï¼ˆç²¾ç¡®å®šä½èµ·å§‹ä½ç½®ï¼‰

        ç­–ç•¥ï¼š
        1. ç”¨50æ®µæ»‘åŠ¨çª—å£æ‰«ææ•´ä¸ªç« èŠ‚
        2. æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆåŒå¯†åº¦>20%çš„åŒºåŸŸ
        3. ä»è¯¥åŒºåŸŸå‘å‰ç²¾ç¡®å®šä½èšé›†åŒºèµ·ç‚¹ï¼ˆæ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å«åˆåŒç‰¹å¾çš„æ®µè½ï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            start_idx: ç« èŠ‚èµ·å§‹æ®µè½ç´¢å¼•
            end_idx: ç« èŠ‚ç»“æŸæ®µè½ç´¢å¼•

        Returns:
            å¦‚æœå‘ç°èšé›†åŒºï¼š{'start': int, 'end': int, 'density': float}
            å¦åˆ™è¿”å› None
        """
        if end_idx - start_idx < 50:
            # ç« èŠ‚å¤ªçŸ­ï¼Œä¸éœ€è¦æ£€æµ‹
            return None

        window_size = 50  # çª—å£å¤§å°ï¼š50ä¸ªæ®µè½
        density_threshold = 0.2  # å¯†åº¦é˜ˆå€¼ï¼š20%
        step_size = 10  # æ»‘åŠ¨æ­¥é•¿ï¼šæ¯æ¬¡ç§»åŠ¨10æ®µ

        # æ»‘åŠ¨çª—å£æ‰«æ
        for i in range(start_idx, end_idx - window_size, step_size):
            window_end = min(i + window_size, end_idx)

            # æå–çª—å£å†…æ–‡æœ¬
            window_text = ""
            for j in range(i, window_end):
                if j < len(doc.paragraphs):
                    para_text = doc.paragraphs[j].text.strip()
                    if para_text:
                        window_text += para_text + "\n"

            # è®¡ç®—åˆåŒå¯†åº¦
            density = self._calculate_contract_density(window_text)

            if density > density_threshold:
                # æ‰¾åˆ°é«˜å¯†åº¦åŒºåŸŸï¼Œå‘å‰ç²¾ç¡®å®šä½èµ·ç‚¹
                cluster_start = i

                # å‘å‰æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«å¼ºåˆåŒç‰¹å¾çš„æ®µè½
                strong_contract_keywords = ['ç”²æ–¹', 'ä¹™æ–¹', 'æœ¬åˆåŒ', 'åˆåŒçš„ç»„æˆ', 'åˆåŒç»„æˆ']

                for j in range(i, start_idx - 1, -1):  # å‘å‰æŸ¥æ‰¾
                    if j < len(doc.paragraphs):
                        para_text = doc.paragraphs[j].text.strip()
                        if any(kw in para_text for kw in strong_contract_keywords):
                            cluster_start = j
                        else:
                            # æ‰¾åˆ°ä¸å«åˆåŒç‰¹å¾çš„æ®µè½ï¼Œåœæ­¢
                            break

                # ç¡®å®šèšé›†åŒºç»“æŸä½ç½®ï¼ˆå‘åæ‰«æï¼Œæ‰¾åˆ°å¯†åº¦é™ä½çš„ä½ç½®ï¼‰
                cluster_end = end_idx
                for j in range(window_end, end_idx, 10):
                    check_end = min(j + 50, end_idx)
                    check_text = "\n".join(
                        doc.paragraphs[k].text.strip()
                        for k in range(j, check_end)
                        if k < len(doc.paragraphs) and doc.paragraphs[k].text.strip()
                    )
                    check_density = self._calculate_contract_density(check_text)

                    if check_density < density_threshold:
                        cluster_end = j - 1
                        break

                self.logger.info(
                    f"æ£€æµ‹åˆ°åˆåŒèšé›†åŒº: æ®µè½{cluster_start}-{cluster_end} "
                    f"(ç« èŠ‚èŒƒå›´:{start_idx}-{end_idx}, åˆåŒå¯†åº¦:{density:.1%})"
                )

                return {
                    'start': cluster_start,
                    'end': cluster_end,
                    'density': density
                }

        return None

    def _calculate_dynamic_threshold(self, toc_items_count: int, doc_paragraph_count: int) -> float:
        """
        æ ¹æ®æ–‡æ¡£ç‰¹å¾åŠ¨æ€è®¡ç®—ç›¸ä¼¼åº¦é˜ˆå€¼

        Args:
            toc_items_count: ç›®å½•é¡¹æ•°é‡
            doc_paragraph_count: æ–‡æ¡£æ€»æ®µè½æ•°

        Returns:
            ç›¸ä¼¼åº¦é˜ˆå€¼ (0.60-0.80)
        """
        # åŸºç¡€é˜ˆå€¼æ ¹æ®ç›®å½•é¡¹æ•°é‡å†³å®šï¼ˆé™ä½5%ä»¥æé«˜å®¹é”™ç‡ï¼‰
        if toc_items_count < 10:
            base_threshold = 0.70  # å°‘é‡ç« èŠ‚ï¼Œé™ä½é˜ˆå€¼ä»0.75åˆ°0.70
        elif toc_items_count < 20:
            base_threshold = 0.65  # ä¸­ç­‰ç« èŠ‚æ•°ï¼Œé™ä½é˜ˆå€¼ä»0.70åˆ°0.65
        else:
            base_threshold = 0.60  # å¤§é‡ç« èŠ‚ï¼Œé™ä½é˜ˆå€¼ä»0.65åˆ°0.60

        # æ ¹æ®æ–‡æ¡£å¤æ‚åº¦å¾®è°ƒ (æ®µè½æ•°è¶Šå¤šï¼Œæ–‡æ¡£è¶Šå¤æ‚ï¼Œå¯é€‚å½“æ”¾å®½)
        doc_complexity = min(1.0, doc_paragraph_count / 1000)  # æ ‡å‡†åŒ–åˆ°0-1
        adjusted_threshold = base_threshold + (doc_complexity * 0.05)

        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸Šé™ä»0.80é™ä½åˆ°0.75ï¼‰
        final_threshold = min(0.75, max(0.55, adjusted_threshold))

        self.logger.info(f"åŠ¨æ€é˜ˆå€¼è®¡ç®—: ç›®å½•é¡¹={toc_items_count}, æ®µè½æ•°={doc_paragraph_count}, "
                        f"åŸºç¡€é˜ˆå€¼={base_threshold}, è°ƒæ•´å={final_threshold:.2f}")

        return final_threshold

    def _find_toc_section(self, doc: Document) -> Optional[int]:
        """
        æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç›®å½•éƒ¨åˆ† (ä¼˜åŒ–ç‰ˆ: æ‰©å±•å…³é”®è¯ + SDTæ”¯æŒ)

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡

        Returns:
            ç›®å½•èµ·å§‹æ®µè½ç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        # ä¼˜åŒ–2: æ‰©å±•ç›®å½•å…³é”®è¯åˆ—è¡¨
        TOC_KEYWORDS = [
            # ä¸­æ–‡
            "ç›®å½•", "ç›®  å½•", "ç´¢å¼•", "ç« èŠ‚ç›®å½•", "å†…å®¹ç›®å½•",
            # è‹±æ–‡
            "contents", "table of contents", "catalogue", "index"
        ]

        # ç¬¬é›¶è½®: æ£€æµ‹SDTå®¹å™¨ä¸­çš„TOCåŸŸï¼ˆWordè‡ªåŠ¨ç›®å½•ï¼‰
        try:
            body = doc.element.body
            ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}

            # æŸ¥æ‰¾æ‰€æœ‰SDTå…ƒç´ 
            sdt_elements = body.findall('.//w:sdt', namespaces=ns)

            for sdt in sdt_elements:
                # æ£€æŸ¥æ˜¯å¦æ˜¯TOCç±»å‹çš„SDT
                docpart = sdt.find('.//w:docPartObj/w:docPartGallery', namespaces=ns)
                if docpart is not None:
                    gallery_val = docpart.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')
                    if gallery_val == 'Table of Contents':
                        # æ‰¾åˆ°TOC SDT,è·å–å…¶ä¸­ç¬¬ä¸€ä¸ªæ®µè½çš„ç´¢å¼•
                        sdt_paras = sdt.findall('.//w:p', namespaces=ns)
                        if sdt_paras:
                            # æ‰¾åˆ°SDTä¸­ç¬¬ä¸€ä¸ªæ®µè½åœ¨doc.paragraphsä¸­çš„ç´¢å¼•
                            first_sdt_para = sdt_paras[0]
                            for idx, para in enumerate(doc.paragraphs[:100]):
                                if para._element == first_sdt_para:
                                    self.logger.info(f"æ£€æµ‹åˆ°Word TOCåŸŸï¼ˆSDTå®¹å™¨ï¼‰ï¼Œç›®å½•èµ·å§‹äºæ®µè½ {idx}")
                                    return idx
        except Exception as e:
            self.logger.debug(f"SDTæ£€æµ‹å¤±è´¥ï¼ˆæ­£å¸¸æƒ…å†µï¼‰: {e}")
            pass

        # ç¬¬ä¸€è½®: æ£€æµ‹æ˜¾å¼ç›®å½•æ ‡é¢˜
        for i, para in enumerate(doc.paragraphs[:50]):  # åªæ£€æŸ¥å‰50æ®µ
            text = para.text.strip()

            # è·³è¿‡ç©ºæ®µè½
            if not text:
                continue

            # æ£€æµ‹ç›®å½•æ ‡é¢˜ (ä½¿ç”¨æ‰©å±•å…³é”®è¯åˆ—è¡¨)
            text_lower = text.lower()
            for keyword in TOC_KEYWORDS:
                if text_lower == keyword.lower() or text.replace(" ", "") == keyword.replace(" ", ""):
                    self.logger.info(f"æ£€æµ‹åˆ°ç›®å½•æ ‡é¢˜ (å…³é”®è¯: '{keyword}')ï¼Œä½äºæ®µè½ {i}: {text}")
                    return i

            # æ£€æµ‹TOCåŸŸï¼ˆWordè‡ªåŠ¨ç”Ÿæˆçš„ç›®å½•ï¼‰
            # é€šè¿‡æ£€æŸ¥æ®µè½çš„XMLæ¥è¯†åˆ«
            try:
                xml_str = para._element.xml.decode() if isinstance(para._element.xml, bytes) else str(para._element.xml)
                if 'TOC' in xml_str and 'fldChar' in xml_str:
                    self.logger.info(f"æ£€æµ‹åˆ°Word TOCåŸŸï¼Œä½äºæ®µè½ {i}")
                    return i
            except (AttributeError, UnicodeDecodeError, TypeError):
                pass  # æ— æ³•è·å–XMLæˆ–è§£æå¤±è´¥æ—¶è·³è¿‡è¯¥æ®µè½

        self.logger.info("æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œå°†ä½¿ç”¨æ ‡é¢˜æ ·å¼è¯†åˆ«æ–¹æ¡ˆ")
        return None

    def _detect_toc_level(self, para, title: str) -> int:
        """
        æ£€æµ‹ç›®å½•é¡¹çš„å±‚çº§

        Args:
            para: python-docx Paragraph å¯¹è±¡
            title: æ ‡é¢˜æ–‡æœ¬

        Returns:
            1-3: æ ‡é¢˜å±‚çº§
        """
        # æ–¹æ³•1ï¼šé€šè¿‡æ®µè½ç¼©è¿›åˆ¤æ–­
        try:
            if para.paragraph_format.left_indent:
                indent_pt = para.paragraph_format.left_indent.pt
                if indent_pt > 40:
                    return 3
                elif indent_pt > 20:
                    return 2
        except (AttributeError, TypeError):
            pass  # æ— æ³•è·å–ç¼©è¿›ä¿¡æ¯æ—¶ä½¿ç”¨å…¶ä»–æ–¹æ³•

        # æ–¹æ³•2ï¼šé€šè¿‡æ ‡é¢˜ç¼–å·æ ¼å¼åˆ¤æ–­
        # ä¸‰çº§ï¼š1.1.1, 1.1.1.1 ç­‰
        if re.match(r'^\d+\.\d+\.\d+', title):
            return 3
        # äºŒçº§ï¼š1.1, 1.2 ç­‰
        elif re.match(r'^\d+\.\d+[^\d]', title):
            return 2
        # ä¸€çº§ï¼šç¬¬Xéƒ¨åˆ†ã€1.ã€2.ã€ä¸€ã€äºŒã€ç­‰
        elif re.match(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)', title):
            return 1

        # é»˜è®¤1çº§
        return 1

    def _parse_toc_items(self, doc: Document, toc_start_idx: int) -> Tuple[List[Dict], int]:
        """
        è§£æç›®å½•é¡¹ï¼ˆæ”¹è¿›3ï¼šç¡®ä¿ toc_end_idx > toc_start_idxï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            toc_start_idx: ç›®å½•èµ·å§‹æ®µè½ç´¢å¼•

        Returns:
            (ç›®å½•é¡¹åˆ—è¡¨, ç›®å½•ç»“æŸç´¢å¼•)
            ç›®å½•é¡¹åˆ—è¡¨æ ¼å¼ï¼š[{'title': '...', 'page_num': 1, 'level': 1}, ...]
        """
        toc_items = []
        consecutive_non_toc = 0  # è¿ç»­éç›®å½•é¡¹è®¡æ•°
        toc_end_idx = toc_start_idx  # ç›®å½•ç»“æŸä½ç½®

        # â­ï¸ ç›®å½•é¡¹æ•°é‡é™åˆ¶ï¼ˆé¿å…æ‰«æè¿‡è¿œï¼‰
        MAX_TOC_ITEMS = 100

        for i in range(toc_start_idx + 1, min(toc_start_idx + 100, len(doc.paragraphs))):
            para = doc.paragraphs[i]
            text = para.text.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not text:
                continue

            # å°è¯•åŒ¹é…ç›®å½•é¡¹æ ¼å¼
            # æ ¼å¼1: "æ ‡é¢˜æ–‡æœ¬    é¡µç " (å¤šä¸ªç©ºæ ¼)
            match = re.match(r'^(.+?)\s{2,}(\d+)$', text)
            if not match:
                # æ ¼å¼2: "æ ‡é¢˜æ–‡æœ¬....é¡µç " (ç‚¹å·å¡«å……)
                match = re.match(r'^(.+?)\.{2,}(\d+)$', text)
            if not match:
                # æ ¼å¼3: "æ ‡é¢˜æ–‡æœ¬\té¡µç " (åˆ¶è¡¨ç¬¦)
                match = re.match(r'^(.+?)\t+(\d+)$', text)

            if match:
                title = match.group(1).strip()
                page_num = int(match.group(2))

                # â­ï¸ é‡å¤æ£€æµ‹ï¼šå¦‚æœä¸ç¬¬ä¸€é¡¹é‡å¤ï¼Œè¯´æ˜æ‰«æåˆ°æ­£æ–‡äº†
                # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ä»£æ›¿ä¸¥æ ¼ç›¸ç­‰ï¼Œä»¥åº”å¯¹æ–‡æœ¬ç•¥æœ‰å·®å¼‚çš„æƒ…å†µ
                if toc_items:
                    first_title = toc_items[0]['title']
                    similarity = self.fuzzy_match_title_v2(title, first_title)
                    if similarity >= 0.70:  # 70%ä»¥ä¸Šç›¸ä¼¼åº¦è®¤ä¸ºæ˜¯é‡å¤
                        self.logger.info(f"æ£€æµ‹åˆ°é‡å¤ç›®å½•é¡¹ï¼ˆä¸ç¬¬1é¡¹ç›¸ä¼¼åº¦{similarity:.0%}ï¼‰ï¼Œç›®å½•è§£æç»“æŸ")
                        self.logger.debug(f"  ç¬¬1é¡¹: '{first_title}' vs å½“å‰: '{title}'")
                        break

                # â­ï¸ æ•°é‡é™åˆ¶æ£€æŸ¥
                if len(toc_items) >= MAX_TOC_ITEMS:
                    self.logger.info(f"ç›®å½•é¡¹å·²è¾¾ä¸Šé™({MAX_TOC_ITEMS})ï¼Œåœæ­¢è§£æ")
                    break

                # æ£€æµ‹å±‚çº§
                level = self._detect_toc_level(para, title)

                toc_items.append({
                    'title': title,
                    'page_num': page_num,
                    'level': level
                })

                self.logger.debug(f"ç›®å½•é¡¹ [{level}çº§]: {title} -> ç¬¬{page_num}é¡µ")

                # æ›´æ–°ç›®å½•ç»“æŸä½ç½®
                toc_end_idx = i
                # é‡ç½®è®¡æ•°
                consecutive_non_toc = 0
            else:
                # æ–°å¢ï¼šå°è¯•è¯†åˆ«æ— é¡µç çš„ç®€å•ç›®å½•é¡¹
                # ç‰¹å¾1ï¼šæœ‰ç»Ÿä¸€ç¼©è¿›ï¼ˆç›®å½•é¡¹é€šå¸¸ç¼©è¿›å¯¹é½ï¼‰
                has_indent = (para.paragraph_format.left_indent and
                              para.paragraph_format.left_indent > 200000)

                # ç‰¹å¾2ï¼šåŒ¹é…ç« èŠ‚æ¨¡å¼
                is_chapter_pattern = (
                    re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', text) or
                    re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', text) or
                    text in ['ç«äº‰æ€§ç£‹å•†å…¬å‘Š', 'æ‹›æ ‡å…¬å‘Š', 'é‡‡è´­å…¬å‘Š', 'è°ˆåˆ¤å…¬å‘Š', 'è¯¢ä»·å…¬å‘Š']
                )

                # å¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œä½œä¸ºæ— é¡µç ç›®å½•é¡¹
                if (has_indent or is_chapter_pattern) and len(text) < 50 and not text.startswith('é¡¹ç›®'):
                    title = text.strip()

                    # â­ï¸ é‡å¤æ£€æµ‹ï¼šå¦‚æœä¸ç¬¬ä¸€é¡¹é‡å¤ï¼Œè¯´æ˜æ‰«æåˆ°æ­£æ–‡äº†
                    # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ä»£æ›¿ä¸¥æ ¼ç›¸ç­‰ï¼Œä»¥åº”å¯¹æ–‡æœ¬ç•¥æœ‰å·®å¼‚çš„æƒ…å†µ
                    if toc_items:
                        first_title = toc_items[0]['title']
                        similarity = self.fuzzy_match_title_v2(title, first_title)
                        if similarity >= 0.70:  # 70%ä»¥ä¸Šç›¸ä¼¼åº¦è®¤ä¸ºæ˜¯é‡å¤
                            self.logger.info(f"æ£€æµ‹åˆ°é‡å¤ç›®å½•é¡¹ï¼ˆä¸ç¬¬1é¡¹ç›¸ä¼¼åº¦{similarity:.0%}ï¼‰ï¼Œç›®å½•è§£æç»“æŸ")
                            self.logger.debug(f"  ç¬¬1é¡¹: '{first_title}' vs å½“å‰: '{title}'")
                            break

                    # â­ï¸ æ•°é‡é™åˆ¶æ£€æŸ¥
                    if len(toc_items) >= MAX_TOC_ITEMS:
                        self.logger.info(f"ç›®å½•é¡¹å·²è¾¾ä¸Šé™({MAX_TOC_ITEMS})ï¼Œåœæ­¢è§£æ")
                        break

                    page_num = -1  # æ ‡è®°ä¸ºæ— é¡µç 
                    level = self._detect_toc_level(para, title)

                    toc_items.append({
                        'title': title,
                        'page_num': page_num,
                        'level': level
                    })

                    self.logger.debug(f"ç›®å½•é¡¹(æ— é¡µç ) [{level}çº§]: {title}")
                    toc_end_idx = i
                    consecutive_non_toc = 0
                    continue  # æˆåŠŸè¯†åˆ«ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

                # éç›®å½•é¡¹
                consecutive_non_toc += 1
                # è¿ç»­5è¡Œä¸åŒ¹é…ï¼Œè®¤ä¸ºç›®å½•ç»“æŸ
                if consecutive_non_toc >= 5 and len(toc_items) > 0:
                    self.logger.info(f"ç›®å½•è§£æå®Œæˆï¼Œå…± {len(toc_items)} é¡¹ï¼Œç»“æŸäºæ®µè½ {toc_end_idx}")
                    break
                # æ–°å¢ï¼šå¦‚æœæ‰¾åˆ°äº†ç›®å½•é¡¹ä½†é‡åˆ°ç©ºè¡Œåçš„Headingæ ·å¼ï¼Œè®¤ä¸ºç›®å½•ç»“æŸ
                if (len(toc_items) > 0 and consecutive_non_toc >= 2 and
                    para.style and para.style.name.startswith('Heading')):
                    self.logger.info(f"æ£€æµ‹åˆ°Headingæ ·å¼ï¼Œç›®å½•ç»“æŸäºæ®µè½ {toc_end_idx}")
                    break

        # æ”¹è¿›3ï¼šç¡®ä¿ toc_end_idx ä¸¥æ ¼å¤§äº toc_start_idx
        if toc_end_idx == toc_start_idx:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›®å½•é¡¹ï¼Œè‡³å°‘å‘åç§»åŠ¨1ä¸ªæ®µè½
            toc_end_idx = toc_start_idx + 1
            self.logger.warning(f"æœªè§£æåˆ°ç›®å½•é¡¹ï¼Œå°†ç›®å½•ç»“æŸä½ç½®è®¾ä¸º {toc_end_idx}ï¼ˆé¿å…é€»è¾‘é”™è¯¯ï¼‰")

        return toc_items, toc_end_idx

    def _find_paragraph_by_title(self, doc: Document, title: str, start_idx: int = 0) -> Optional[int]:
        """
        åœ¨æ–‡æ¡£ä¸­æœç´¢ä¸æ ‡é¢˜åŒ¹é…çš„æ®µè½

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            title: è¦æœç´¢çš„æ ‡é¢˜æ–‡æœ¬
            start_idx: å¼€å§‹æœç´¢çš„æ®µè½ç´¢å¼•

        Returns:
            æ®µè½ç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        def aggressive_normalize(text: str) -> str:
            """æ¿€è¿›æ–‡æœ¬è§„èŒƒåŒ–ï¼šç§»é™¤æ‰€æœ‰åˆ†éš”ç¬¦ã€å‰ç¼€ã€ç©ºæ ¼"""
            # ç§»é™¤"é™„ä»¶-"ã€"é™„ä»¶:"ç­‰å‰ç¼€
            text = re.sub(r'^é™„ä»¶[-:ï¼š]?', '', text)
            # ç§»é™¤è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿ã€åˆ¶è¡¨ç¬¦
            text = re.sub(r'[-_\t]+', '', text)
            # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
            text = re.sub(r'\s+', '', text)
            return text

        def extract_core_keywords(text: str) -> str:
            """æå–æ ¸å¿ƒå…³é”®è¯ï¼šå»é™¤ç¼–å·å’Œå¸¸è§å‰ç¼€"""
            # ç§»é™¤ç¼–å·
            text = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', text)
            # ç§»é™¤"é™„ä»¶"å‰ç¼€
            text = re.sub(r'^é™„ä»¶[-:ï¼š]?', '', text)
            # ç§»é™¤åˆ†éš”ç¬¦
            text = re.sub(r'[-_\t]+', '', text)
            # ç§»é™¤ç©ºæ ¼
            text = re.sub(r'\s+', '', text)
            return text

        def calculate_similarity(str1: str, str2: str) -> float:
            """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºåŒ…å«å…³ç³»ï¼‰"""
            if not str1 or not str2:
                return 0.0

            shorter = str1 if len(str1) <= len(str2) else str2
            longer = str2 if len(str1) <= len(str2) else str1

            # æ£€æŸ¥shorteræ˜¯å¦è¢«longeråŒ…å«
            if shorter in longer:
                return len(shorter) / len(longer)

            # æ£€æŸ¥éƒ¨åˆ†é‡å 
            max_overlap = 0
            for i in range(len(shorter)):
                for j in range(i + 1, len(shorter) + 1):
                    substr = shorter[i:j]
                    if substr in longer and len(substr) > max_overlap:
                        max_overlap = len(substr)

            return max_overlap / max(len(str1), len(str2))

        # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤å¤šä½™ç©ºæ ¼ï¼‰
        clean_title = re.sub(r'\s+', '', title)

        # æ¿€è¿›è§„èŒƒåŒ–çš„æ ‡é¢˜
        aggressive_title = aggressive_normalize(title)

        # æå–æ ¸å¿ƒå…³é”®è¯
        core_keywords = extract_core_keywords(aggressive_title)

        self.logger.info(f"æœç´¢æ ‡é¢˜: '{title}' (æ¸…ç†å: '{clean_title}', æ ¸å¿ƒ: '{core_keywords}'), ä»æ®µè½ {start_idx} å¼€å§‹")

        # å€™é€‰åŒ¹é…åˆ—è¡¨ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        candidates = []

        for i in range(start_idx, len(doc.paragraphs)):
            para = doc.paragraphs[i]
            para_text = para.text.strip()

            if not para_text:
                continue

            # æ¸…ç†æ®µè½æ–‡æœ¬
            clean_para = re.sub(r'\s+', '', para_text)

            # æ¿€è¿›è§„èŒƒåŒ–çš„æ®µè½
            aggressive_para = aggressive_normalize(para_text)

            # æ®µè½æ ¸å¿ƒå…³é”®è¯
            para_keywords = extract_core_keywords(aggressive_para)

            # Level 1: å®Œå…¨åŒ¹é…æˆ–åŒ…å«åŒ¹é…
            if clean_title == clean_para or clean_title in clean_para:
                self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 1-å®Œå…¨): æ®µè½ {i}: '{para_text}'")
                return i

            # Level 2: æ¿€è¿›è§„èŒƒåŒ–åçš„å®Œå…¨åŒ¹é…
            if aggressive_title == aggressive_para or aggressive_title in aggressive_para:
                self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 2-è§„èŒƒåŒ–): æ®µè½ {i}: '{para_text}'")
                return i

            # æ£€æŸ¥æ ‡é¢˜å’Œæ®µè½æ˜¯å¦åŒ…å«"ç¬¬Xéƒ¨åˆ†"ï¼ˆç”¨äºLevel 3å’ŒLevel 4çº¦æŸï¼‰
            title_has_part_number = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', title))
            para_has_part_number = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', para_text))

            # Level 3: å»é™¤ç¼–å·åçš„åŒ¹é…
            # æ”¯æŒå¤šç§ç¼–å·æ ¼å¼ï¼šç¬¬Xéƒ¨åˆ†ã€ç¬¬Xç« ã€1.ã€1.1ã€ä¸€ã€ç­‰
            title_without_number = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', clean_title)
            para_without_number = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', clean_para)

            if title_without_number and para_without_number and title_without_number == para_without_number:
                # å¦‚æœTOCæ ‡é¢˜æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼Œåˆ™æ®µè½ä¹Ÿå¿…é¡»æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼ˆé¿å…åŒ¹é…åˆ°TOCå†…çš„ç¼–å·å†…å®¹ï¼‰
                if title_has_part_number and not para_has_part_number:
                    pass  # è·³è¿‡ï¼Œä¸åŒ¹é…
                else:
                    self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 3-å»ç¼–å·): æ®µè½ {i}: '{para_text}'")
                    return i

            # Level 4: æ ¸å¿ƒå…³é”®è¯åŒ¹é…ï¼ˆé•¿åº¦â‰¥4å­—ï¼‰
            # ç‰¹åˆ«æ£€æŸ¥ï¼šå¦‚æœåŸæ ‡é¢˜åŒ…å«"ç¬¬Xéƒ¨åˆ†",åˆ™æ®µè½ä¹Ÿå¿…é¡»åŒ…å«"ç¬¬Xéƒ¨åˆ†"

            if len(core_keywords) >= 4 and len(para_keywords) >= 4:
                # åŒå‘åŒ…å«æ£€æŸ¥
                if core_keywords in para_keywords or para_keywords in core_keywords:
                    # å¦‚æœæ ‡é¢˜æœ‰"ç¬¬Xéƒ¨åˆ†",åˆ™æ®µè½ä¹Ÿå¿…é¡»æœ‰,ä¸”æ®µè½åº”è¯¥æ˜¯çŸ­æ ‡é¢˜(â‰¤50å­—)
                    if title_has_part_number:
                        if para_has_part_number and len(para_text) <= 50:
                            self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 4-å…³é”®è¯+éƒ¨åˆ†ç¼–å·): æ®µè½ {i}: '{para_text}' (æ ¸å¿ƒè¯: '{para_keywords}')")
                            return i
                    else:
                        # æ ‡é¢˜æ²¡æœ‰"ç¬¬Xéƒ¨åˆ†",æ™®é€šå…³é”®è¯åŒ¹é…
                        self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 4-å…³é”®è¯): æ®µè½ {i}: '{para_text}' (æ ¸å¿ƒè¯: '{para_keywords}')")
                        return i

            # Level 4.5: éƒ¨åˆ†å­ä¸²åŒ¹é…ï¼ˆè§£å†³TOCä¸å®é™…æ–‡æœ¬éƒ¨åˆ†å·®å¼‚é—®é¢˜ï¼‰
            # ä¾‹å¦‚ï¼šTOC="å•ä¸€æ¥æºé‡‡è´­è°ˆåˆ¤é‚€è¯·" vs å®é™…="å•ä¸€æ¥æºé‡‡è´­é‚€è¯·" (å°‘"è°ˆåˆ¤")
            if len(core_keywords) >= 6 and title_has_part_number:
                # ä»é•¿åˆ°çŸ­å°è¯•æå–å­ä¸²
                for substr_len in range(len(core_keywords), 5, -1):
                    substr = core_keywords[:substr_len]
                    if substr in para_keywords and len(substr) >= 6:
                        # æ‰¾åˆ°å¤§éƒ¨åˆ†åŒ¹é…ï¼ŒéªŒè¯æ®µè½æ ¼å¼
                        if para_has_part_number and len(para_text) <= 50:
                            match_ratio = len(substr) / len(core_keywords)
                            self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 4.5-éƒ¨åˆ†å­ä¸²{match_ratio:.0%}): æ®µè½ {i}: '{para_text}' (åŒ¹é…: '{substr}')")
                            return i
                        break  # æ‰¾åˆ°ä½†æ ¼å¼ä¸å¯¹ï¼Œä¸ç»§ç»­å°è¯•æ›´çŸ­çš„

            # Level 5: ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆç›¸ä¼¼åº¦â‰¥80%ï¼Œæ›´ä¸¥æ ¼ï¼‰
            if len(core_keywords) >= 4:
                similarity = calculate_similarity(core_keywords, para_keywords)
                if similarity >= 0.8:  # æé«˜é˜ˆå€¼ä»70%åˆ°80%
                    self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 5-ç›¸ä¼¼åº¦{similarity:.0%}): æ®µè½ {i}: '{para_text}'")
                    return i

                # è®°å½•é«˜ç›¸ä¼¼åº¦å€™é€‰
                if similarity >= 0.6:  # å€™é€‰é˜ˆå€¼ä¹Ÿç›¸åº”æé«˜
                    candidates.append((i, para_text, similarity, core_keywords, para_keywords))

            # Level 6: å®½æ¾å…³é”®è¯åŒ¹é…ï¼ˆè‡³å°‘6å­—æ ‡é¢˜ï¼‰
            if len(title_without_number) >= 6:
                # æ£€æŸ¥æ®µè½æ˜¯å¦åŒ…å«æ ‡é¢˜å»é™¤ç¼–å·åçš„å¤§éƒ¨åˆ†å†…å®¹
                if title_without_number in clean_para:
                    self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 6-å®½æ¾): æ®µè½ {i}: '{para_text}'")
                    return i

            # é¢å¤–å°è¯•ï¼šå°†"ç¬¬Xéƒ¨åˆ†"è½¬æ¢ä¸º"X."è¿›è¡ŒåŒ¹é…
            # ä¾‹å¦‚ï¼š"ç¬¬ä¸€éƒ¨åˆ† å•ä¸€æ¥æºé‡‡è´­è°ˆåˆ¤é‚€è¯·" ä¹Ÿå¯ä»¥åŒ¹é… "1.å•ä¸€æ¥æºé‡‡è´­è°ˆåˆ¤é‚€è¯·"
            def convert_chinese_to_number(text):
                """å°†ç¬¬ä¸€/ç¬¬äºŒ/ç¬¬ä¸‰ç­‰è½¬æ¢ä¸º1/2/3"""
                mapping = {'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
                          'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10'}
                # åŒ¹é…"ç¬¬Xéƒ¨åˆ†"æ ¼å¼
                match = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)éƒ¨åˆ†(.*)$', text)
                if match:
                    num = mapping.get(match.group(1), match.group(1))
                    return f"{num}.{match.group(2)}"
                return text

            # Level 7: è½¬æ¢ç¼–å·ååŒ¹é…
            converted_title = convert_chinese_to_number(clean_title)
            if converted_title != clean_title and clean_para.startswith(converted_title[:3]):
                # è½¬æ¢åçš„æ ‡é¢˜å¼€å¤´ä¸æ®µè½åŒ¹é…
                converted_para_without_num = re.sub(r'^\d+\.', '', clean_para)
                converted_title_without_num = re.sub(r'^\d+\.', '', converted_title)
                if converted_title_without_num == converted_para_without_num:
                    self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 7-è½¬æ¢ç¼–å·): æ®µè½ {i}: '{para_text}'")
                    return i

            # æ”¶é›†ä½ç›¸ä¼¼åº¦å€™é€‰ï¼ˆç”¨äºè¯Šæ–­ï¼‰
            if i < start_idx + 100 and len(para_text) > 5 and len(para_text) < 100:
                # æ£€æŸ¥æ˜¯å¦éƒ¨åˆ†åŒ¹é…
                if title_without_number and para_without_number:
                    # å¦‚æœæ ‡é¢˜å»ç¼–å·åçš„å†…å®¹éƒ¨åˆ†å‡ºç°åœ¨æ®µè½ä¸­
                    if len(title_without_number) >= 3:
                        if title_without_number[:4] in para_without_number or para_without_number[:4] in title_without_number:
                            if not any(c[0] == i for c in candidates):  # é¿å…é‡å¤
                                candidates.append((i, para_text, 0.4, title_without_number, para_without_number))

        # æœªæ‰¾åˆ°ï¼Œè¾“å‡ºè¯Šæ–­ä¿¡æ¯
        self.logger.warning(f"æœªæ‰¾åˆ°æ ‡é¢˜åŒ¹é…: '{title}'")
        if candidates:
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            candidates.sort(key=lambda x: x[2] if isinstance(x[2], float) else 0.3, reverse=True)
            self.logger.info(f"  å¯èƒ½çš„å€™é€‰æ®µè½ (å‰{min(5, len(candidates))}ä¸ªï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº):")
            for idx, text, sim, title_key, para_key in candidates[:5]:
                sim_str = f"{sim:.0%}" if isinstance(sim, float) else "ä½"
                self.logger.info(f"    æ®µè½ {idx} (ç›¸ä¼¼åº¦{sim_str}): '{text[:50]}...' ")
                self.logger.info(f"      æ ‡é¢˜æ ¸å¿ƒ: '{title_key}' vs æ®µè½æ ¸å¿ƒ: '{para_key}'")

        return None

    def _detect_numbering_pattern(self, text: str) -> Optional[Tuple[str, int]]:
        """
        æ£€æµ‹æ®µè½çš„ç¼–å·æ¨¡å¼

        Args:
            text: æ®µè½æ–‡æœ¬

        Returns:
            (ç¼–å·å‰ç¼€, å±‚çº§) æˆ– None
            ä¾‹å¦‚: ("2.1.", 2), ("2.1.1.", 3), ("ä¸€ã€", 1)
        """
        patterns = [
            (r'^(\d+\.\d+\.\d+\.)\s*', 3),  # 2.1.1.
            (r'^(\d+\.\d+\.)\s*', 2),       # 2.1.
            (r'^(\d+\.)\s*', 1),            # 2.
            (r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', 1),  # ä¸€ã€
            (r'^(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\))\s*', 2),  # (ä¸€)
        ]

        for pattern, level in patterns:
            match = re.match(pattern, text)
            if match:
                return (match.group(1), level)
        return None

    def _is_bold_subtitle(self, paragraph) -> bool:
        """
        åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºåŠ ç²—çš„å­æ ‡é¢˜

        Args:
            paragraph: Wordæ®µè½å¯¹è±¡

        Returns:
            æ˜¯å¦ä¸ºåŠ ç²—å­æ ‡é¢˜
        """
        text = paragraph.text.strip()

        # æ’é™¤ç©ºæ®µè½
        if not text:
            return False

        # æ’é™¤è¿‡é•¿çš„æ®µè½(å­æ ‡é¢˜é€šå¸¸è¾ƒçŸ­)
        if len(text) > 50:
            return False

        # æ’é™¤ç¼–å·å¼€å¤´çš„æ®µè½(å·²é€šè¿‡ç¼–å·æ¨¡å¼è¯†åˆ«)
        if re.match(r'^\d+\.', text):
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰åŠ ç²—çš„run
        has_bold = False
        if paragraph.runs:
            # è‡³å°‘æœ‰ä¸€ä¸ªrunæ˜¯åŠ ç²—çš„,ä¸”åŠ ç²—å†…å®¹å æ¯”è¾ƒå¤§
            bold_chars = sum(len(r.text) for r in paragraph.runs if r.bold)
            total_chars = len(text)
            has_bold = bold_chars > total_chars * 0.5  # åŠ ç²—å†…å®¹è¶…è¿‡50%

        return has_bold

    def _parse_subsections_in_range(self, doc: Document, start_idx: int, end_idx: int,
                                     parent_level: int, parent_id: str) -> List[ChapterNode]:
        """
        åœ¨æŒ‡å®šæ®µè½èŒƒå›´å†…è¯†åˆ«å­ç« èŠ‚ (å¢å¼ºç‰ˆ)

        è¯†åˆ«ç­–ç•¥:
        1. æ ·å¼æ ‡é¢˜: Heading 1/2/3ç­‰æ ·å¼
        2. ç¼–å·æ¨¡å¼: 2.1., 2.1.1., ä¸€ã€ç­‰
        3. åŠ ç²—å­æ ‡é¢˜: åŠ ç²—ä¸”è¾ƒçŸ­çš„æ®µè½

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            start_idx: èµ·å§‹æ®µè½ç´¢å¼•
            end_idx: ç»“æŸæ®µè½ç´¢å¼•
            parent_level: çˆ¶ç« èŠ‚å±‚çº§
            parent_id: çˆ¶ç« èŠ‚ID

        Returns:
            å­ç« èŠ‚åˆ—è¡¨
        """
        subsections = []
        counter = 0

        # è®°å½•ä¸Šä¸€ä¸ªç¼–å·,ç”¨äºæ£€æµ‹ç¼–å·é‡ç½®
        last_numbering = None

        for para_idx in range(start_idx + 1, end_idx + 1):
            if para_idx >= len(doc.paragraphs):
                break

            paragraph = doc.paragraphs[para_idx]
            text = paragraph.text.strip()

            if not text:
                continue

            level = self._get_heading_level(paragraph)
            is_subsection = False
            recognition_type = ""

            # ç­–ç•¥1: æ ·å¼æ ‡é¢˜ (åŸæœ‰é€»è¾‘)
            if level > 0 and level > parent_level:
                is_subsection = True
                recognition_type = f"æ ·å¼{level}çº§"

            # ç­–ç•¥2: ç¼–å·æ¨¡å¼è¯†åˆ«
            elif not is_subsection:
                numbering_result = self._detect_numbering_pattern(text)
                if numbering_result:
                    numbering_prefix, numbering_level = numbering_result

                    # æ£€æµ‹ç¼–å·é‡ç½®(ä¾‹å¦‚ 2.1.6 -> 2.1.1 è¡¨ç¤ºæ–°çš„å­ç« èŠ‚ç»„)
                    if last_numbering and numbering_prefix < last_numbering:
                        self.logger.debug(f"  âš ï¸  æ£€æµ‹åˆ°ç¼–å·é‡ç½®: {last_numbering} -> {numbering_prefix}")

                    last_numbering = numbering_prefix

                    # ç¼–å·å±‚çº§åº”è¯¥æ¯”çˆ¶å±‚çº§æ·±
                    if numbering_level > parent_level:
                        is_subsection = True
                        recognition_type = f"ç¼–å·{numbering_prefix}"

            # ç­–ç•¥3: åŠ ç²—å­æ ‡é¢˜è¯†åˆ«
            elif not is_subsection and self._is_bold_subtitle(paragraph):
                is_subsection = True
                recognition_type = "åŠ ç²—å­æ ‡é¢˜"
                # åŠ ç²—å­æ ‡é¢˜è§†ä¸ºæ¯”çˆ¶å±‚çº§æ·±1çº§
                level = parent_level + 1

            # å¦‚æœè¯†åˆ«ä¸ºå­ç« èŠ‚,åˆ›å»ºèŠ‚ç‚¹
            if is_subsection:
                title = text

                # åˆ¤æ–­æ˜¯å¦åŒ¹é…ç™½/é»‘åå•
                auto_selected = self._matches_whitelist(title)
                skip_recommended = self._matches_blacklist(title)

                if skip_recommended:
                    auto_selected = False

                subsection = ChapterNode(
                    id=f"{parent_id}_{counter}",
                    level=level if level > 0 else parent_level + 1,
                    title=title,
                    para_start_idx=para_idx,
                    para_end_idx=None,  # ç¨åè®¡ç®—
                    word_count=0,       # ç¨åè®¡ç®—
                    preview_text="",    # ç¨åæå–
                    auto_selected=auto_selected,
                    skip_recommended=skip_recommended
                )

                subsections.append(subsection)
                counter += 1

                self.logger.debug(
                    f"  â””â”€ æ‰¾åˆ°å­ç« èŠ‚ [{recognition_type}]: {title} "
                    f"{'âœ…è‡ªåŠ¨é€‰ä¸­' if auto_selected else 'âŒè·³è¿‡' if skip_recommended else 'âšªé»˜è®¤'}"
                )

        # è®¡ç®—æ¯ä¸ªå­ç« èŠ‚çš„èŒƒå›´
        for i, subsection in enumerate(subsections):
            # ç¡®å®šå­ç« èŠ‚ç»“æŸä½ç½®
            if i + 1 < len(subsections):
                subsection.para_end_idx = subsections[i + 1].para_start_idx - 1
            else:
                subsection.para_end_idx = end_idx

            # æå–å­ç« èŠ‚å†…å®¹
            content_paras = doc.paragraphs[subsection.para_start_idx + 1 : subsection.para_end_idx + 1]
            content_text = '\n'.join(p.text for p in content_paras)
            subsection.word_count = len(content_text.replace(' ', '').replace('\n', ''))

            # æå–é¢„è§ˆæ–‡æœ¬
            preview_lines = []
            for p in content_paras[:5]:
                text = p.text.strip()
                if text:
                    preview_lines.append(text[:100] + ('...' if len(text) > 100 else ''))
                if len(preview_lines) >= 5:
                    break

            subsection.preview_text = '\n'.join(preview_lines) if preview_lines else "(æ— å†…å®¹)"

        return subsections

    def _locate_chapters_by_toc(self, doc: Document, toc_items: List[Dict], toc_end_idx: int) -> List[ChapterNode]:
        """
        æ ¹æ®ç›®å½•é¡¹å®šä½ç« èŠ‚åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            toc_items: ç›®å½•é¡¹åˆ—è¡¨
            toc_end_idx: ç›®å½•ç»“æŸçš„æ®µè½ç´¢å¼•

        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        chapters = []
        # ä»ç›®å½•ç»“æŸä½ç½®ä¹‹åå¼€å§‹æœç´¢ï¼Œé¿å…å°†ç›®å½•ä¸­çš„é¡¹è¯¯è¯†åˆ«ä¸ºç« èŠ‚æ ‡é¢˜
        last_found_idx = toc_end_idx + 1
        self.logger.info(f"ç›®å½•ç»“æŸäºæ®µè½ {toc_end_idx}ï¼Œä»æ®µè½ {last_found_idx} å¼€å§‹æœç´¢ç« èŠ‚æ­£æ–‡")

        for i, item in enumerate(toc_items):
            title = item['title']
            level = item['level']

            # ä»ä¸Šä¸€ä¸ªä½ç½®ä¹‹åå¼€å§‹æœç´¢ï¼ˆç« èŠ‚æŒ‰é¡ºåºå‡ºç°ï¼‰
            para_idx = self._find_paragraph_by_title(doc, title, last_found_idx)

            if para_idx is None:
                self.logger.warning(f"æœªæ‰¾åˆ°ç›®å½•é¡¹å¯¹åº”çš„ç« èŠ‚: {title}")
                continue

            # æ›´æ–°æœç´¢èµ·ç‚¹
            last_found_idx = para_idx + 1

            # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®
            para_end_idx = len(doc.paragraphs) - 1  # é»˜è®¤åˆ°æ–‡æ¡£æœ«å°¾

            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªç›®å½•é¡¹çš„ä½ç½®
            for j in range(i + 1, len(toc_items)):
                next_para_idx = self._find_paragraph_by_title(doc, toc_items[j]['title'], last_found_idx)
                if next_para_idx:
                    para_end_idx = next_para_idx - 1
                    break

            # æå–ç« èŠ‚å†…å®¹
            content_paras = doc.paragraphs[para_idx + 1 : para_end_idx + 1]
            content_text = '\n'.join(p.text for p in content_paras)
            word_count = len(content_text.replace(' ', '').replace('\n', ''))

            # æå–é¢„è§ˆæ–‡æœ¬
            preview_lines = []
            for p in content_paras[:5]:
                text = p.text.strip()
                if text:
                    preview_lines.append(text[:100] + ('...' if len(text) > 100 else ''))
                if len(preview_lines) >= 5:
                    break

            preview_text = '\n'.join(preview_lines) if preview_lines else "(æ— å†…å®¹)"

            # åˆ¤æ–­æ˜¯å¦åŒ¹é…ç™½/é»‘åå•
            auto_selected = self._matches_whitelist(title)
            skip_recommended = self._matches_blacklist(title)

            if skip_recommended:
                auto_selected = False

            chapter = ChapterNode(
                id=f"ch_{i}",
                level=level,
                title=title,
                para_start_idx=para_idx,
                para_end_idx=para_end_idx,
                word_count=word_count,
                preview_text=preview_text,
                auto_selected=auto_selected,
                skip_recommended=skip_recommended
            )

            # åœ¨ç« èŠ‚èŒƒå›´å†…è¯†åˆ«å­ç« èŠ‚
            self.logger.info(f"æ­£åœ¨è¯†åˆ« '{title}' çš„å­ç« èŠ‚ (æ®µè½èŒƒå›´: {para_idx}-{para_end_idx})")
            subsections = self._parse_subsections_in_range(
                doc, para_idx, para_end_idx, level, f"ch_{i}"
            )

            if subsections:
                chapter.children = subsections
                # æ³¨æ„ï¼šçˆ¶ç« èŠ‚çš„word_countå·²ç»åŒ…å«äº†å…¶æ®µè½èŒƒå›´å†…çš„æ‰€æœ‰å†…å®¹
                # æ— éœ€å†ç´¯åŠ å­ç« èŠ‚å­—æ•°ï¼Œå¦åˆ™ä¼šå¯¼è‡´é‡å¤è®¡ç®—
                self.logger.info(f"  â””â”€ è¯†åˆ«åˆ° {len(subsections)} ä¸ªå­ç« èŠ‚ï¼ˆçˆ¶ç« èŠ‚å­—æ•°: {chapter.word_count}ï¼‰")

            chapters.append(chapter)

            self.logger.debug(
                f"å®šä½ç« èŠ‚ [{level}çº§]: {title} "
                f"(æ®µè½ {para_idx}-{para_end_idx}, {word_count}å­—) "
                f"{'âœ…è‡ªåŠ¨é€‰ä¸­' if auto_selected else 'âŒè·³è¿‡' if skip_recommended else 'âšªé»˜è®¤'}"
            )

        return chapters

    def _locate_chapter_content(self, doc: Document, chapters: List[ChapterNode]) -> List[ChapterNode]:
        """
        å®šä½æ¯ä¸ªç« èŠ‚çš„å†…å®¹èŒƒå›´

        Args:
            doc: Word æ–‡æ¡£å¯¹è±¡
            chapters: ç« èŠ‚åˆ—è¡¨

        Returns:
            æ›´æ–°åçš„ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å« para_end_idxã€word_countã€preview_textï¼‰
        """
        # â­ï¸ å…³é”®ä¿®å¤ï¼šæŒ‰æ®µè½ç´¢å¼•æ’åºï¼Œç¡®ä¿ç« èŠ‚é¡ºåºä¸æ–‡æ¡£ç‰©ç†é¡ºåºä¸€è‡´
        # é˜²æ­¢ç´¢å¼•å€’ç½®é—®é¢˜ï¼ˆå¦‚ para_start_idx=542 > para_end_idx=62ï¼‰
        chapters_sorted = sorted(chapters, key=lambda ch: ch.para_start_idx)
        self.logger.info(f"ç« èŠ‚å·²æŒ‰æ®µè½ç´¢å¼•æ’åºï¼Œå…± {len(chapters_sorted)} ä¸ªç« èŠ‚")

        total_paras = len(doc.paragraphs)

        # ğŸ†• ç”¨äºæ”¶é›†éœ€è¦æ’å…¥çš„åˆåŒç« èŠ‚
        contract_chapters_to_insert = []

        for i, chapter in enumerate(chapters_sorted):
            # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜çš„å‰ä¸€ä¸ªæ®µè½ï¼‰
            next_start = total_paras  # é»˜è®¤åˆ°æ–‡æ¡£æœ«å°¾

            for j in range(i + 1, len(chapters_sorted)):
                if chapters_sorted[j].level <= chapter.level:
                    next_start = chapters_sorted[j].para_start_idx
                    break

            chapter.para_end_idx = next_start - 1

            # æå–ç« èŠ‚å†…å®¹ï¼ˆåŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼ï¼‰
            content_text, preview_text = self._extract_chapter_content_with_tables(
                doc, chapter.para_start_idx, chapter.para_end_idx
            )

            # è®¡ç®—å­—æ•°
            chapter.word_count = len(content_text.replace(' ', '').replace('\n', ''))
            chapter.preview_text = preview_text if preview_text else "(æ— å†…å®¹)"

            # ã€æ–°å¢ã€‘å¯¹äºlevel 1-2çš„ç« èŠ‚ï¼Œæå–å†…å®¹æ ·æœ¬å¹¶è¿›è¡ŒåˆåŒè¯†åˆ«
            if chapter.level <= 2:
                # æå–å†…å®¹æ ·æœ¬ç”¨äºåˆåŒè¯†åˆ«
                chapter.content_sample = self._extract_content_sample(
                    doc, chapter.para_start_idx, chapter.para_end_idx, sample_size=2000
                )

                # åŸºäºå†…å®¹è¿›è¡ŒåˆåŒè¯†åˆ«
                is_contract, density, reason = self._is_contract_chapter(
                    chapter.title, chapter.content_sample
                )

                if is_contract:
                    # æ›´æ–°skip_recommendedæ ‡è®°
                    if not chapter.skip_recommended:  # é¿å…é‡å¤æ ‡è®°
                        chapter.skip_recommended = True
                        chapter.auto_selected = False
                        self.logger.info(
                            f"  âœ“ åˆåŒç« èŠ‚è¯†åˆ«: '{chapter.title}' - {reason}"
                        )
                    else:
                        self.logger.debug(
                            f"  âœ“ åˆåŒç« èŠ‚å·²æ ‡è®°: '{chapter.title}' - {reason}"
                        )

            # ğŸ†• æ–°å¢ï¼šæ£€æµ‹ç« èŠ‚å†…æ˜¯å¦æœ‰åˆåŒèšé›†åŒºï¼ˆç”¨äºæ‹†åˆ†ç« èŠ‚ï¼‰
            contract_cluster = self._detect_contract_cluster_in_chapter(
                doc, chapter.para_start_idx, chapter.para_end_idx
            )

            if contract_cluster:
                cluster_start = contract_cluster['start']
                cluster_end = contract_cluster['end']
                density = contract_cluster['density']

                # ç¡®ä¿èšé›†åŒºèµ·ç‚¹åœ¨ç« èŠ‚å†…ä¸”æœ‰è¶³å¤Ÿçš„å‰ç½®å†…å®¹
                min_content_length = 1000  # å‰åŠéƒ¨åˆ†è‡³å°‘1000å­—

                if cluster_start > chapter.para_start_idx + 5:  # è‡³å°‘è·³è¿‡5ä¸ªæ®µè½
                    # è®¡ç®—å‰åŠéƒ¨åˆ†çš„å­—æ•°
                    front_content = "\n".join(
                        doc.paragraphs[j].text.strip()
                        for j in range(chapter.para_start_idx + 1, cluster_start)
                        if j < len(doc.paragraphs)
                    )
                    front_word_count = len(front_content.replace(' ', '').replace('\n', ''))

                    if front_word_count >= min_content_length:
                        self.logger.warning(
                            f"âš ï¸ ç« èŠ‚å°†è¢«æ‹†åˆ†: '{chapter.title}' "
                            f"â†’ æ­£å¸¸éƒ¨åˆ†({chapter.para_start_idx}-{cluster_start-1}, {front_word_count}å­—) "
                            f"+ åˆåŒéƒ¨åˆ†({cluster_start}-{cluster_end}, å¯†åº¦{density:.1%})"
                        )

                        # æˆªæ–­å½“å‰ç« èŠ‚ï¼ˆåªä¿ç•™åˆåŒä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                        original_end = chapter.para_end_idx
                        chapter.para_end_idx = cluster_start - 1

                        # é‡æ–°è®¡ç®—ç¼©çŸ­åçš„ç« èŠ‚å†…å®¹
                        content_text, preview_text = self._extract_chapter_content_with_tables(
                            doc, chapter.para_start_idx, chapter.para_end_idx
                        )
                        chapter.word_count = len(content_text.replace(' ', '').replace('\n', ''))
                        chapter.preview_text = preview_text

                        # ğŸ†• åˆ›å»ºåˆåŒç« èŠ‚ï¼ˆæ ‡è®°ä¸ºå¾…æ’å…¥ï¼‰
                        contract_chapter = ChapterNode(
                            id=f"ch_{i}_contract",  # ä¸´æ—¶IDï¼Œåç»­ä¼šé‡æ–°åˆ†é…
                            level=chapter.level,  # ä¸åŸç« èŠ‚åŒçº§
                            title="[æ£€æµ‹åˆ°çš„åˆåŒæ¡æ¬¾-éœ€äººå·¥ç¡®è®¤]",
                            para_start_idx=cluster_start,
                            para_end_idx=original_end,
                            word_count=0,
                            preview_text="",
                            auto_selected=False,
                            skip_recommended=True  # æ ‡è®°ä¸ºæ¨èè·³è¿‡
                        )

                        # è®¡ç®—åˆåŒç« èŠ‚å†…å®¹
                        contract_content, contract_preview = self._extract_chapter_content_with_tables(
                            doc, contract_chapter.para_start_idx, contract_chapter.para_end_idx
                        )
                        contract_chapter.word_count = len(contract_content.replace(' ', '').replace('\n', ''))
                        contract_chapter.preview_text = contract_preview

                        # æ·»åŠ åˆ°å¾…æ’å…¥åˆ—è¡¨ï¼ˆè®°å½•æ’å…¥ä½ç½®ï¼‰
                        contract_chapters_to_insert.append((i + 1, contract_chapter))

                        self.logger.info(
                            f"âœ‚ï¸ ç« èŠ‚æ‹†åˆ†å®Œæˆ: "
                            f"æ­£å¸¸éƒ¨åˆ†({chapter.para_start_idx}-{chapter.para_end_idx}, {chapter.word_count}å­—) "
                            f"+ åˆåŒéƒ¨åˆ†({contract_chapter.para_start_idx}-{contract_chapter.para_end_idx}, {contract_chapter.word_count}å­—)"
                        )
                    else:
                        self.logger.info(
                            f"è·³è¿‡æ‹†åˆ†: '{chapter.title}' å‰åŠéƒ¨åˆ†å†…å®¹ä¸è¶³({front_word_count}å­— < {min_content_length}å­—)"
                        )
                else:
                    self.logger.info(
                        f"è·³è¿‡æ‹†åˆ†: '{chapter.title}' åˆåŒèšé›†åŒºèµ·ç‚¹å¤ªé å‰(æ®µè½{cluster_start})"
                    )

        # ğŸ†• æ’å…¥æ‰€æœ‰æ£€æµ‹åˆ°çš„åˆåŒç« èŠ‚
        if contract_chapters_to_insert:
            # æŒ‰æ’å…¥ä½ç½®å€’åºæ’å…¥ï¼ˆé¿å…ç´¢å¼•åç§»ï¼‰
            for insert_pos, contract_chapter in reversed(contract_chapters_to_insert):
                chapters_sorted.insert(insert_pos, contract_chapter)

            self.logger.info(f"å·²æ’å…¥ {len(contract_chapters_to_insert)} ä¸ªåˆåŒç« èŠ‚")

            # ğŸ†• é‡æ–°åˆ†é…ç« èŠ‚IDï¼Œç¡®ä¿IDè¿ç»­
            for idx, ch in enumerate(chapters_sorted):
                ch.id = f"ch_{idx}"

            self.logger.info(f"ç« èŠ‚IDå·²é‡æ–°åˆ†é…ï¼Œå½“å‰å…± {len(chapters_sorted)} ä¸ªç« èŠ‚")

        return chapters_sorted

    def _extract_chapter_content_with_tables(self, doc: Document, para_start_idx: int, para_end_idx: int) -> tuple:
        """
        æå–ç« èŠ‚å†…å®¹,åŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_start_idx: èµ·å§‹æ®µè½ç´¢å¼•
            para_end_idx: ç»“æŸæ®µè½ç´¢å¼•

        Returns:
            (å®Œæ•´å†…å®¹æ–‡æœ¬, é¢„è§ˆæ–‡æœ¬)
        """
        # æ„å»ºæ®µè½ç´¢å¼•åˆ°bodyå…ƒç´ ç´¢å¼•çš„æ˜ å°„
        para_count = 0
        start_body_idx = None
        end_body_idx = None

        for body_idx, element in enumerate(doc.element.body):
            if isinstance(element, CT_P):
                if para_count == para_start_idx and start_body_idx is None:
                    start_body_idx = body_idx
                if para_count == para_end_idx:
                    end_body_idx = body_idx
                    break
                para_count += 1

        if start_body_idx is None:
            return "", ""

        if end_body_idx is None:
            end_body_idx = len(doc.element.body) - 1

        # æå–å†…å®¹(è·³è¿‡ç« èŠ‚æ ‡é¢˜,ä»start+1å¼€å§‹)
        content_parts = []
        preview_lines = []
        preview_limit = 5

        for body_idx in range(start_body_idx + 1, end_body_idx + 1):
            element = doc.element.body[body_idx]

            if isinstance(element, CT_P):
                # æ®µè½
                from docx.text.paragraph import Paragraph
                para = Paragraph(element, doc)
                text = para.text.strip()
                if text:
                    content_parts.append(text)
                    # æ·»åŠ åˆ°é¢„è§ˆ
                    if len(preview_lines) < preview_limit:
                        preview_lines.append(text[:100] + ('...' if len(text) > 100 else ''))

            elif isinstance(element, CT_Tbl):
                # è¡¨æ ¼
                from docx.table import Table
                table = Table(element, doc)

                # æå–è¡¨æ ¼æ–‡æœ¬
                table_text_parts = []
                table_preview_parts = []

                for row_idx, row in enumerate(table.rows):
                    row_data = []
                    for cell in row.cells:
                        cell_text = '\n'.join(p.text.strip() for p in cell.paragraphs if p.text.strip())
                        row_data.append(cell_text)

                    if any(cell.strip() for cell in row_data):  # éç©ºè¡Œ
                        row_text = ' | '.join(row_data)
                        table_text_parts.append(row_text)

                        # æ·»åŠ åˆ°é¢„è§ˆ(è¡¨æ ¼çš„å‰å‡ è¡Œ)
                        if len(preview_lines) < preview_limit and row_idx < 3:
                            table_preview_parts.append(row_text[:100] + ('...' if len(row_text) > 100 else ''))

                if table_text_parts:
                    # æ·»åŠ è¡¨æ ¼æ ‡è¯†
                    table_content = f"[è¡¨æ ¼]\n" + '\n'.join(table_text_parts)
                    content_parts.append(table_content)

                    # æ·»åŠ è¡¨æ ¼é¢„è§ˆ
                    if len(preview_lines) < preview_limit:
                        preview_lines.append("[è¡¨æ ¼]")
                        preview_lines.extend(table_preview_parts[:preview_limit - len(preview_lines)])

        full_content = '\n'.join(content_parts)
        preview_text = '\n'.join(preview_lines)

        return full_content, preview_text

    def _build_chapter_tree(self, chapters: List[ChapterNode]) -> List[ChapterNode]:
        """
        æ„å»ºç« èŠ‚å±‚çº§æ ‘

        Args:
            chapters: æ‰å¹³ç« èŠ‚åˆ—è¡¨

        Returns:
            æ ¹çº§ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å«å­ç« èŠ‚ï¼‰
        """
        if not chapters:
            return []

        # ä½¿ç”¨æ ˆæ¥æ„å»ºæ ‘
        root_chapters = []
        stack = []  # [(level, chapter), ...]

        for chapter in chapters:
            # å¼¹å‡ºæ‰€æœ‰å±‚çº§ >= å½“å‰ç« èŠ‚å±‚çº§çš„èŠ‚ç‚¹
            while stack and stack[-1][0] >= chapter.level:
                stack.pop()

            if not stack:
                # å½“å‰æ˜¯æ ¹çº§ç« èŠ‚
                root_chapters.append(chapter)
            else:
                # å½“å‰æ˜¯å­ç« èŠ‚ï¼Œæ·»åŠ åˆ°çˆ¶ç« èŠ‚çš„ children
                parent = stack[-1][1]
                parent.children.append(chapter)
                # æ›´æ–°å­ç« èŠ‚IDï¼ˆåŒ…å«çˆ¶çº§IDï¼‰
                chapter.id = f"{parent.id}_{len(parent.children)}"

            # å°†å½“å‰ç« èŠ‚å…¥æ ˆ
            stack.append((chapter.level, chapter))

        return root_chapters

    def _propagate_skip_status(self, chapter_tree: List[ChapterNode]) -> List[ChapterNode]:
        """
        é€’å½’ä¼ æ’­çˆ¶ç« èŠ‚çš„ skip_recommended çŠ¶æ€åˆ°å­ç« èŠ‚
        å¦‚æœçˆ¶ç« èŠ‚è¢«è·³è¿‡ï¼Œåˆ™æ‰€æœ‰å­ç« èŠ‚åŠå…¶åä»£éƒ½åº”è¯¥è¢«è·³è¿‡

        Args:
            chapter_tree: ç« èŠ‚æ ‘

        Returns:
            æ›´æ–°åçš„ç« èŠ‚æ ‘
        """
        propagated_count = 0

        def propagate_recursive(chapter: ChapterNode):
            """é€’å½’ä¼ æ’­è·³è¿‡çŠ¶æ€"""
            nonlocal propagated_count

            # å¦‚æœå½“å‰ç« èŠ‚è¢«æ ‡è®°ä¸ºè·³è¿‡ï¼Œä¼ æ’­åˆ°æ‰€æœ‰å­ç« èŠ‚å’Œåä»£
            if chapter.skip_recommended:
                for child in chapter.children:
                    if not child.skip_recommended:  # é¿å…é‡å¤è®¡æ•°
                        child.skip_recommended = True
                        child.auto_selected = False
                        propagated_count += 1
                        self.logger.debug(f"  â””â”€ ä¼ æ’­skipçŠ¶æ€: {chapter.title} -> {child.title}")
                    # é€’å½’ä¼ æ’­åˆ°æ‰€æœ‰åä»£
                    propagate_recursive(child)
            else:
                # å³ä½¿å½“å‰ç« èŠ‚ä¸è¢«è·³è¿‡ï¼Œä¹Ÿè¦é€’å½’æ£€æŸ¥å­ç« èŠ‚
                # ï¼ˆå› ä¸ºå­ç« èŠ‚å¯èƒ½è‡ªå·±åŒ¹é…é»‘åå•ï¼‰
                for child in chapter.children:
                    propagate_recursive(child)

        # éå†æ‰€æœ‰æ ¹çº§ç« èŠ‚
        for root_chapter in chapter_tree:
            propagate_recursive(root_chapter)

        if propagated_count > 0:
            self.logger.info(f"é»‘åå•çŠ¶æ€ä¼ æ’­å®Œæˆ: å…±ä¼ æ’­åˆ° {propagated_count} ä¸ªå­ç« èŠ‚")

        return chapter_tree

    def _calculate_statistics(self, chapter_tree: List[ChapterNode]) -> Dict:
        """
        è®¡ç®—ç»Ÿè®¡ä¿¡æ¯

        Args:
            chapter_tree: ç« èŠ‚æ ‘

        Returns:
            ç»Ÿè®¡å­—å…¸
        """
        stats = {
            "total_chapters": 0,
            "auto_selected": 0,
            "skip_recommended": 0,
            "total_words": 0,
            "estimated_processing_cost": 0.0
        }

        def traverse(chapters):
            for ch in chapters:
                stats["total_chapters"] += 1
                if ch.auto_selected:
                    stats["auto_selected"] += 1
                if ch.skip_recommended:
                    stats["skip_recommended"] += 1
                stats["total_words"] += ch.word_count

                # é€’å½’éå†å­ç« èŠ‚
                if ch.children:
                    traverse(ch.children)

        traverse(chapter_tree)

        # ä¼°ç®—å¤„ç†æˆæœ¬ï¼ˆåŸºäºå­—æ•°ï¼‰
        # å‡è®¾ï¼š1000å­— â‰ˆ 1500 tokens â‰ˆ $0.002ï¼ˆGPT-4o-miniï¼‰
        stats["estimated_processing_cost"] = (stats["total_words"] / 1000) * 0.002

        return stats

    def get_selected_chapter_content(self, doc_path: str, selected_chapter_ids: List[str]) -> Dict:
        """
        æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ç« èŠ‚IDï¼Œæå–å¯¹åº”çš„æ–‡æœ¬å†…å®¹

        Args:
            doc_path: Word æ–‡æ¡£è·¯å¾„
            selected_chapter_ids: é€‰ä¸­çš„ç« èŠ‚IDåˆ—è¡¨ï¼Œå¦‚ ["ch_0", "ch_1_2"]

        Returns:
            {
                "success": True/False,
                "chapters": [
                    {
                        "id": "ch_0",
                        "title": "ç¬¬ä¸€ç«  é¡¹ç›®æ¦‚è¿°",
                        "content": "å®Œæ•´ç« èŠ‚æ–‡æœ¬å†…å®¹...",
                        "word_count": 1500
                    },
                    ...
                ],
                "total_words": 8000
            }
        """
        try:
            # é‡æ–°è§£ææ–‡æ¡£ï¼ˆè·å–å®Œæ•´ç« èŠ‚ä¿¡æ¯ï¼‰
            result = self.parse_document_structure(doc_path)
            if not result["success"]:
                return result

            chapters = self._flatten_chapters(result["chapters"])

            # æ‰“å¼€æ–‡æ¡£
            doc = Document(doc_path)

            # æå–é€‰ä¸­ç« èŠ‚çš„å†…å®¹
            selected_chapters = []
            total_words = 0

            for chapter_dict in chapters:
                if chapter_dict["id"] in selected_chapter_ids:
                    # æå–å†…å®¹
                    start_idx = chapter_dict["para_start_idx"]
                    end_idx = chapter_dict["para_end_idx"]

                    content_paras = doc.paragraphs[start_idx : end_idx + 1]
                    content = '\n'.join(p.text for p in content_paras)

                    selected_chapters.append({
                        "id": chapter_dict["id"],
                        "title": chapter_dict["title"],
                        "content": content,
                        "word_count": len(content.replace(' ', '').replace('\n', ''))
                    })

                    total_words += selected_chapters[-1]["word_count"]

            self.logger.info(f"æå–äº† {len(selected_chapters)} ä¸ªç« èŠ‚ï¼Œå…± {total_words} å­—")

            return {
                "success": True,
                "chapters": selected_chapters,
                "total_words": total_words
            }

        except Exception as e:
            self.logger.error(f"æå–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return {
                "success": False,
                "chapters": [],
                "total_words": 0,
                "error": str(e)
            }

    def export_chapter_to_docx(self, doc_path: str, chapter_id: str,
                              output_path: str = None) -> Dict:
        """
        å°†æŒ‡å®šç« èŠ‚å¯¼å‡ºä¸ºç‹¬ç«‹çš„Wordæ–‡æ¡£ï¼ˆä¿ç•™åŸå§‹æ ¼å¼ï¼‰

        Args:
            doc_path: åŸå§‹Wordæ–‡æ¡£è·¯å¾„
            chapter_id: ç« èŠ‚ID (å¦‚ "ch_4")
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸´æ—¶ç›®å½•ï¼‰

        Returns:
            {
                "success": True,
                "file_path": "/path/to/exported.docx",
                "chapter_title": "ç¬¬äº”éƒ¨åˆ† å“åº”æ–‡ä»¶æ ¼å¼",
                "word_count": 1500
            }
        """
        try:
            from docx import Document
            from tempfile import NamedTemporaryFile
            from copy import deepcopy

            # 1. è§£ææ–‡æ¡£ç»“æ„ï¼Œå®šä½ç›®æ ‡ç« èŠ‚
            result = self.parse_document_structure(doc_path)
            if not result["success"]:
                return result

            chapters = self._flatten_chapters(result["chapters"])
            target_chapter = None

            for ch in chapters:
                if ch["id"] == chapter_id:
                    target_chapter = ch
                    break

            if not target_chapter:
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°ç« èŠ‚ID: {chapter_id}"
                }

            # 2. æ‰“å¼€åŸå§‹æ–‡æ¡£
            source_doc = Document(doc_path)

            # 3. åˆ›å»ºæ–°æ–‡æ¡£
            new_doc = Document()

            # 4. å¤åˆ¶ç« èŠ‚å†…å®¹ï¼ˆä¿ç•™æ ¼å¼ï¼‰
            para_start = target_chapter["para_start_idx"]
            para_end = target_chapter.get("para_end_idx")

            if para_end is None:
                para_end = len(source_doc.paragraphs) - 1

            self.logger.info(f"å¯¼å‡ºç« èŠ‚: {target_chapter['title']}")
            self.logger.info(f"æ®µè½èŒƒå›´: {para_start} - {para_end}")

            # å¤åˆ¶æ®µè½ï¼ˆä½¿ç”¨æ·±æ‹·è´ä¿ç•™æ ¼å¼ï¼‰
            for i in range(para_start, min(para_end + 1, len(source_doc.paragraphs))):
                source_para = source_doc.paragraphs[i]

                # ä½¿ç”¨XMLæ·±æ‹·è´ï¼ˆæœ€ä½³æ ¼å¼ä¿ç•™ï¼‰
                # å¯¼å…¥æ®µè½çš„å®Œæ•´XMLèŠ‚ç‚¹
                new_para_element = deepcopy(source_para._element)
                new_doc.element.body.append(new_para_element)

            # 5. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶æˆ–æŒ‡å®šè·¯å¾„
            if output_path is None:
                # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
                temp_file = NamedTemporaryFile(
                    delete=False,
                    suffix='.docx',
                    prefix='chapter_template_'
                )
                output_path = temp_file.name
                temp_file.close()

            new_doc.save(output_path)

            self.logger.info(f"ç« èŠ‚å·²å¯¼å‡º: {output_path}")

            return {
                "success": True,
                "file_path": output_path,
                "chapter_title": target_chapter["title"],
                "word_count": target_chapter.get("word_count", 0),
                "para_count": para_end - para_start + 1
            }

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºç« èŠ‚å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e)
            }

    def export_multiple_chapters_to_docx(self, doc_path: str, chapter_ids: List[str], output_path: str = None) -> Dict:
        """
        å°†å¤šä¸ªç« èŠ‚å¯¼å‡ºä¸ºå•ä¸ªWordæ–‡æ¡£

        Args:
            doc_path: æºæ–‡æ¡£è·¯å¾„
            chapter_ids: ç« èŠ‚IDåˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            {
                "success": bool,
                "file_path": str,
                "chapter_titles": List[str],
                "chapter_count": int
            }
        """
        from docx import Document
        from tempfile import NamedTemporaryFile
        from copy import deepcopy

        try:
            # ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æï¼ˆå…¼å®¹å¤šç§ç¯å¢ƒï¼‰
            resolved_path = resolve_file_path(doc_path)
            if not resolved_path:
                return {"success": False, "error": f"æ–‡æ¡£è·¯å¾„è§£æå¤±è´¥: {doc_path}"}

            doc_path = str(resolved_path)  # ä½¿ç”¨è§£æåçš„ç»å¯¹è·¯å¾„

            # è§£ææ–‡æ¡£ç»“æ„
            result = self.parse_document_structure(doc_path)
            chapters = self._flatten_chapters(result["chapters"])

            # â­ å…³é”®ä¿®å¤ï¼šè¿‡æ»¤æ‰çˆ¶ç« èŠ‚å·²è¢«é€‰ä¸­çš„å­ç« èŠ‚ï¼Œé¿å…é‡å¤å¯¼å‡º
            filtered_chapter_ids = self._filter_redundant_chapters(chapter_ids)
            if len(filtered_chapter_ids) < len(chapter_ids):
                removed_count = len(chapter_ids) - len(filtered_chapter_ids)
                self.logger.info(f"å»é‡å®Œæˆï¼šç§»é™¤äº† {removed_count} ä¸ªå†—ä½™å­ç« èŠ‚")
                self.logger.info(f"  åŸå§‹ç« èŠ‚åˆ—è¡¨: {chapter_ids}")
                self.logger.info(f"  å»é‡åç« èŠ‚åˆ—è¡¨: {filtered_chapter_ids}")

            # æŒ‰IDæŸ¥æ‰¾ç›®æ ‡ç« èŠ‚å¹¶æŒ‰åŸæ–‡æ¡£é¡ºåºæ’åº
            target_chapters = []
            for ch in chapters:
                if ch["id"] in filtered_chapter_ids:
                    target_chapters.append(ch)

            if not target_chapters:
                return {"success": False, "error": "æœªæ‰¾åˆ°æŒ‡å®šç« èŠ‚"}

            # æ‰“å¼€æºæ–‡æ¡£
            source_doc = Document(doc_path)
            # ä½¿ç”¨æºæ–‡æ¡£ä½œä¸ºæ¨¡æ¿ï¼Œä¿ç•™æ‰€æœ‰æ ·å¼å’Œé¡µé¢è®¾ç½®
            new_doc = Document(doc_path)

            # æ¸…ç©ºæ¨¡æ¿æ–‡æ¡£çš„æ‰€æœ‰bodyå†…å®¹ï¼ˆæ®µè½+è¡¨æ ¼ï¼‰ï¼Œä¿ç•™æ ·å¼å®šä¹‰ã€é¡µé¢è®¾ç½®ã€é¡µçœ‰é¡µè„šç­‰
            for element in list(new_doc.element.body):
                element.getparent().remove(element)

            chapter_titles = []

            # ä¾æ¬¡å¤åˆ¶æ¯ä¸ªç« èŠ‚
            for i, chapter in enumerate(target_chapters):
                chapter_titles.append(chapter["title"])

                # æ·»åŠ ç« èŠ‚æ ‡é¢˜ï¼ˆé™¤ç¬¬ä¸€ä¸ªç« èŠ‚å¤–ï¼Œå…¶ä»–ç« èŠ‚å‰åŠ åˆ†é¡µç¬¦ï¼‰
                if i > 0:
                    new_doc.add_page_break()

                # å¤åˆ¶ç« èŠ‚å†…å®¹ï¼ˆåŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼ï¼‰
                para_start = chapter["para_start_idx"]
                para_end = chapter.get("para_end_idx", len(source_doc.paragraphs) - 1)

                # æ„å»ºæ®µè½ç´¢å¼•åˆ°bodyç´¢å¼•çš„æ˜ å°„
                para_count = 0
                start_body_idx = None
                end_body_idx = None

                for body_idx, element in enumerate(source_doc.element.body):
                    if isinstance(element, CT_P):
                        if para_count == para_start and start_body_idx is None:
                            start_body_idx = body_idx
                        if para_count == para_end:
                            end_body_idx = body_idx
                            break
                        para_count += 1

                # å¤åˆ¶èŒƒå›´å†…çš„æ‰€æœ‰å…ƒç´ ï¼ˆæ®µè½+è¡¨æ ¼ï¼‰
                if start_body_idx is not None and end_body_idx is not None:
                    for body_idx in range(start_body_idx, end_body_idx + 1):
                        element = source_doc.element.body[body_idx]
                        new_element = deepcopy(element)
                        new_doc.element.body.append(new_element)

            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            if output_path is None:
                temp_file = NamedTemporaryFile(delete=False, suffix='.docx', prefix='chapters_template_')
                output_path = temp_file.name
                temp_file.close()

            new_doc.save(output_path)

            logger.info(f"æ‰¹é‡å¯¼å‡ºæˆåŠŸ: {len(target_chapters)}ä¸ªç« èŠ‚ -> {output_path}")

            return {
                "success": True,
                "file_path": output_path,
                "chapter_titles": chapter_titles,
                "chapter_count": len(target_chapters)
            }

        except Exception as e:
            logger.error(f"æ‰¹é‡å¯¼å‡ºå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}

    def _flatten_chapters(self, chapter_dicts: List[Dict]) -> List[Dict]:
        """å°†ç« èŠ‚æ ‘æ‰å¹³åŒ–ä¸ºåˆ—è¡¨"""
        flat = []

        def traverse(chapters):
            for ch in chapters:
                flat.append(ch)
                if ch.get("children"):
                    traverse(ch["children"])

        traverse(chapter_dicts)
        return flat

    def _filter_redundant_chapters(self, chapter_ids: List[str]) -> List[str]:
        """
        è¿‡æ»¤æ‰çˆ¶ç« èŠ‚å·²è¢«é€‰ä¸­çš„å­ç« èŠ‚ï¼Œé¿å…é‡å¤å¯¼å‡º

        ä¾‹å¦‚ï¼šå¦‚æœåŒæ—¶é€‰æ‹©äº† ch_3 å’Œ ch_3_2ï¼Œåˆ™åªä¿ç•™ ch_3

        Args:
            chapter_ids: åŸå§‹ç« èŠ‚IDåˆ—è¡¨

        Returns:
            å»é‡åçš„ç« èŠ‚IDåˆ—è¡¨
        """
        filtered_ids = []

        for chapter_id in chapter_ids:
            # æ£€æŸ¥æ˜¯å¦æœ‰çˆ¶ç« èŠ‚å·²åœ¨åˆ—è¡¨ä¸­
            has_parent_selected = False

            # åˆ†æç« èŠ‚IDçš„å±‚çº§ç»“æ„ï¼ˆch_3_2_1 -> ["ch_3", "ch_3_2", "ch_3_2_1"]ï¼‰
            parts = chapter_id.split('_')
            for i in range(1, len(parts)):
                # æ„å»ºå¯èƒ½çš„çˆ¶ç« èŠ‚ID
                parent_id = '_'.join(parts[:i+1])
                if parent_id != chapter_id and parent_id in chapter_ids:
                    has_parent_selected = True
                    self.logger.debug(f"è·³è¿‡å­ç« èŠ‚ {chapter_id}ï¼Œå› ä¸ºçˆ¶ç« èŠ‚ {parent_id} å·²è¢«é€‰ä¸­")
                    break

            # å¦‚æœæ²¡æœ‰çˆ¶ç« èŠ‚è¢«é€‰ä¸­ï¼Œä¿ç•™è¯¥ç« èŠ‚
            if not has_parent_selected:
                filtered_ids.append(chapter_id)

        return filtered_ids

    # ========================================
    # æ–°å¢ï¼šåŸºäºè¯­ä¹‰é”šç‚¹çš„è§£ææ–¹æ³•
    # ========================================

    def remove_leading_patterns(self, text: str) -> Tuple[str, int]:
        """
        ç§»é™¤æ–‡æœ¬å¼€å¤´çš„ç¼–å·æ¨¡å¼ï¼Œè¿”å›çº¯å‡€æ–‡æœ¬å’Œæ¨æµ‹çš„å±‚çº§ï¼ˆæ”¹è¿›4ï¼šæ”¯æŒæ›´å¤šç¼–å·æ ¼å¼ï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            (çº¯å‡€æ–‡æœ¬, å±‚çº§)
            å±‚çº§åˆ¤æ–­ï¼š
            - "ç¬¬Xéƒ¨åˆ†" / "ç¬¬Xç« " -> 1
            - "1." / "ä¸€ã€" -> 1
            - "1.1" -> 2
            - "1.1.1" -> 3
            - "1.1.1.1" -> 4 (æ”¹è¿›4æ–°å¢)
            - "é™„ä»¶X:" / "é™„è¡¨X:" -> 1 (æ”¹è¿›4æ–°å¢)
        """
        text = text.strip()
        original_text = text
        level = 1  # é»˜è®¤å±‚çº§

        # ç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ï¼ˆæ ‡å‡†åŒ–ï¼‰
        text_normalized = re.sub(r'\s+', '', text)

        # æ£€æµ‹å±‚çº§å¹¶ç§»é™¤ç¼–å·ï¼ˆæ”¹è¿›4ï¼šå¢åŠ å››çº§ç¼–å·å’Œé™„ä»¶ç¼–å·æ”¯æŒï¼‰
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+éƒ¨åˆ†', text):
            level = 1
            text = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+éƒ¨åˆ†\s*', '', text)
        elif re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+ç« ', text):
            level = 1
            text = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+ç« \s*', '', text)
        elif re.match(r'^\d+\.\d+\.\d+\.\d+', text_normalized):
            level = 4  # æ”¹è¿›4ï¼šå››çº§ç¼–å·
            text = re.sub(r'^\d+\.\d+\.\d+\.\d+\s*', '', text)
        elif re.match(r'^\d+\.\d+\.\d+', text_normalized):
            level = 3
            text = re.sub(r'^\d+\.\d+\.\d+\s*', '', text)
        elif re.match(r'^\d+\.\d+[^\d]', text_normalized):
            level = 2
            text = re.sub(r'^\d+\.\d+\s*', '', text)
        elif re.match(r'^\d+\.', text_normalized):
            level = 1
            text = re.sub(r'^\d+\.\s*', '', text)
        elif re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', text):
            level = 1
            text = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s*', '', text)
        elif re.match(r'^ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰', text):
            level = 2
            text = re.sub(r'^ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰\s*', '', text)
        elif re.match(r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)', text):
            level = 2
            text = re.sub(r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s*', '', text)
        elif re.match(r'^é™„ä»¶[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]', text):
            level = 1  # æ”¹è¿›4ï¼šé™„ä»¶ç¼–å·
            text = re.sub(r'^é™„ä»¶[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*', '', text)
        elif re.match(r'^é™„è¡¨[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]', text):
            level = 1  # æ”¹è¿›4ï¼šé™„è¡¨ç¼–å·
            text = re.sub(r'^é™„è¡¨[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*', '', text)
        elif re.match(r'^é™„å½•[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]', text):
            level = 1  # æ”¹è¿›4ï¼šé™„å½•ç¼–å·
            text = re.sub(r'^é™„å½•[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*', '', text)

        # ç§»é™¤å¸¸è§åˆ†éš”ç¬¦
        text = text.strip().strip('ï¼š:').strip()

        self.logger.debug(f"ç§»é™¤ç¼–å·: '{original_text}' -> '{text}' (å±‚çº§{level})")

        return text, level

    def fuzzy_match_title(self, text: str, target: str, threshold: float = 0.75) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦

        Args:
            text: å¾…åŒ¹é…æ–‡æœ¬
            target: ç›®æ ‡æ–‡æœ¬ï¼ˆæ¥è‡ªç›®å½•ï¼‰
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)ï¼Œå¤§äºç­‰äº threshold è§†ä¸ºåŒ¹é…
        """
        # æ ‡å‡†åŒ–ï¼šç§»é™¤æ‰€æœ‰ç©ºæ ¼
        text_clean = re.sub(r'\s+', '', text)
        target_clean = re.sub(r'\s+', '', target)

        # å®Œå…¨åŒ¹é…
        if text_clean == target_clean:
            return 1.0

        # åŒ…å«åŒ¹é…
        if text_clean in target_clean or target_clean in text_clean:
            shorter = min(len(text_clean), len(target_clean))
            longer = max(len(text_clean), len(target_clean))
            return shorter / longer

        # ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, text_clean, target_clean).ratio()

        # éƒ¨åˆ†å­ä¸²åŒ¹é…ï¼ˆè§£å†³"å•ä¸€æ¥æºé‡‡è´­è°ˆåˆ¤é‚€è¯·" vs "å•ä¸€æ¥æºé‡‡è´­é‚€è¯·"ï¼‰
        if similarity < threshold and len(target_clean) >= 6:
            # å°è¯•ä»é•¿åˆ°çŸ­çš„å­ä¸²
            for length in range(len(target_clean), 5, -1):
                substr = target_clean[:length]
                if substr in text_clean:
                    match_ratio = length / len(target_clean)
                    if match_ratio >= 0.6:  # è‡³å°‘60%åŒ¹é…
                        return match_ratio

        # ç‰¹æ®Šå¤„ç†ï¼šç›®æ ‡ä»¥"ä¹¦"/"è¡¨"ç­‰å•å­—ç»“å°¾ï¼Œæ­£æ–‡å¯èƒ½æ²¡æœ‰
        # ä¾‹å¦‚ï¼š"æŠ€æœ¯éœ€æ±‚ä¹¦" vs "æŠ€æœ¯éœ€æ±‚"ï¼Œ"XXè¡¨" vs "XX"
        if len(target_clean) > 3 and target_clean[-1] in ['ä¹¦', 'è¡¨', 'å•', 'å†Œ', 'å‡½']:
            target_without_suffix = target_clean[:-1]
            if text_clean == target_without_suffix or text_clean in target_without_suffix:
                return 0.95  # é«˜åˆ†ä½†ä¸æ˜¯æ»¡åˆ†
            # è®¡ç®—å»æ‰åç¼€åçš„ç›¸ä¼¼åº¦
            suffix_similarity = SequenceMatcher(None, text_clean, target_without_suffix).ratio()
            if suffix_similarity > similarity:
                return suffix_similarity

        return similarity

    def is_section_anchor(self, paragraph, toc_targets: List[str], start_idx: int = 0) -> Tuple[bool, Optional[str], int, str]:
        """
        åˆ¤æ–­æ®µè½æ˜¯å¦æ˜¯ç« èŠ‚é”šç‚¹

        Args:
            paragraph: docx Paragraph å¯¹è±¡
            toc_targets: ç›®å½•æ ‡é¢˜åˆ—è¡¨ï¼ˆå·²æ¸…ç†ï¼‰
            start_idx: å½“å‰æ®µè½åœ¨æ–‡æ¡£ä¸­çš„ç´¢å¼•ï¼ˆç”¨äºè·³è¿‡ç›®å½•åŒºåŸŸï¼‰

        Returns:
            (æ˜¯å¦åŒ¹é…, åŒ¹é…çš„ç›®æ ‡æ ‡é¢˜, å±‚çº§, åŒ¹é…åŸå› )
        """
        para_text = paragraph.text.strip()

        # è·³è¿‡ç©ºè¡Œæˆ–å¤ªçŸ­çš„æ®µè½
        if not para_text or len(para_text) < 3:
            return False, None, 0, "ç©ºè¡Œæˆ–å¤ªçŸ­"

        # è·³è¿‡æ˜æ˜¾çš„éæ ‡é¢˜å†…å®¹ï¼ˆå¦‚é•¿æ®µè½ï¼‰
        if len(para_text) > 100:
            return False, None, 0, "æ®µè½è¿‡é•¿"

        # A. ç§»é™¤ç¼–å·ï¼Œè·å–çº¯å‡€æ–‡æœ¬å’Œå±‚çº§
        clean_text, detected_level = self.remove_leading_patterns(para_text)

        # å…³é”®æ”¹è¿›ï¼šåªåŒ¹é…ä¸€çº§ç« èŠ‚ï¼ˆ"ç¬¬Xéƒ¨åˆ†"æ ¼å¼ï¼‰
        # è¿™æ ·å¯ä»¥é¿å…å°†ç›®å½•å†…çš„äºŒä¸‰çº§æ ‡é¢˜è¯¯è¯†åˆ«ä¸ºç« èŠ‚
        has_part_number = bool(re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', para_text))

        # B. è¯­ä¹‰åŒ¹é…ï¼šä¸ç›®å½•ç›®æ ‡è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
        best_match = None
        best_score = 0.0
        best_target = None

        for target in toc_targets:
            # ä¹Ÿç§»é™¤ç›®æ ‡çš„ç¼–å·
            target_clean, _ = self.remove_leading_patterns(target)

            score = self.fuzzy_match_title(clean_text, target_clean, threshold=0.75)

            if score > best_score:
                best_score = score
                best_target = target
                best_match = target_clean

        # åˆ¤æ–­æ˜¯å¦åŒ¹é…æˆåŠŸï¼ˆéœ€è¦æ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼‰
        # ç­–ç•¥ï¼š
        # 1. è¶…é«˜ç›¸ä¼¼åº¦(â‰¥0.90) - ç›´æ¥æ¥å—
        # 2. é«˜ç›¸ä¼¼åº¦(â‰¥0.80) + æ ¼å¼åŒ¹é…
        # 3. ä¸­ç­‰ç›¸ä¼¼åº¦(â‰¥0.70) + æœ‰"ç¬¬Xéƒ¨åˆ†"æ ¼å¼

        if best_score >= 0.90:
            # è¶…é«˜ç›¸ä¼¼åº¦ï¼Œç›´æ¥æ¥å—ï¼ˆå³ä½¿æ²¡æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼‰
            reason = f"è¶…é«˜ç›¸ä¼¼åº¦åŒ¹é… {best_score:.0%} -> '{best_target}'"
            self.logger.info(f"  âœ“ é”šç‚¹è¯†åˆ«æˆåŠŸ: æ®µè½ {start_idx}: '{para_text}' ({reason})")
            return True, best_target, detected_level, reason
        elif best_score >= 0.80:
            # é«˜ç›¸ä¼¼åº¦ï¼Œéœ€è¦é¢å¤–éªŒè¯
            target_has_part = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', best_target))

            # å¦‚æœç›®æ ‡æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼Œä¼˜å…ˆåŒ¹é…æœ‰ç¼–å·çš„æ®µè½
            # ä½†å¦‚æœæ®µè½æ²¡æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼Œä¹Ÿæ¥å—ï¼ˆå¯èƒ½æ˜¯æ–‡æ¡£æ ¼å¼é”™è¯¯ï¼‰
            if target_has_part and has_part_number:
                reason = f"è¯­ä¹‰åŒ¹é… {best_score:.0%} + éƒ¨åˆ†ç¼–å· -> '{best_target}'"
            elif target_has_part and not has_part_number:
                # æ®µè½æ²¡æœ‰"ç¬¬Xéƒ¨åˆ†"ç¼–å·ï¼Œä½†ç›¸ä¼¼åº¦å¤Ÿé«˜ï¼Œå¯èƒ½æ˜¯æ ¼å¼é”™è¯¯
                # åªåœ¨ç›¸ä¼¼åº¦>=0.85æ—¶æ¥å—
                if best_score < 0.85:
                    return False, None, 0, f"ç›®æ ‡æœ‰éƒ¨åˆ†ç¼–å·ä½†æ®µè½æ— ï¼Œä¸”ç›¸ä¼¼åº¦ä¸å¤Ÿé«˜ï¼ˆ{best_score:.0%}ï¼‰"
                reason = f"è¯­ä¹‰åŒ¹é… {best_score:.0%}ï¼ˆæ®µè½ç¼ºç¼–å·ï¼‰ -> '{best_target}'"
            else:
                reason = f"è¯­ä¹‰åŒ¹é… {best_score:.0%} -> '{best_target}'"

            self.logger.info(f"  âœ“ é”šç‚¹è¯†åˆ«æˆåŠŸ: æ®µè½ {start_idx}: '{para_text}' ({reason})")
            return True, best_target, detected_level, reason
        elif best_score >= 0.70 and has_part_number:
            # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œä½†æœ‰"ç¬¬Xéƒ¨åˆ†"æ ¼å¼ï¼Œä¹Ÿæ¥å—
            reason = f"è¯­ä¹‰åŒ¹é… {best_score:.0%} + éƒ¨åˆ†ç¼–å· -> '{best_target}'"
            self.logger.info(f"  âœ“ é”šç‚¹è¯†åˆ«æˆåŠŸ: æ®µè½ {start_idx}: '{para_text}' ({reason})")
            return True, best_target, detected_level, reason

        # C. è¾…åŠ©ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ Heading æ ·å¼ï¼ˆä½œä¸ºæ¬¡è¦ä¾æ®ï¼‰
        heading_level = self._get_heading_level(paragraph)
        if heading_level > 0:
            # å³ä½¿ç›®å½•ä¸­æ²¡æœ‰ï¼Œä½†å¦‚æœæ˜¯ Heading æ ·å¼ä¸”æœ‰ç¼–å·æ ¼å¼ï¼Œä¹Ÿå¯èƒ½æ˜¯ç« èŠ‚
            # ä½†åªæ¥å—ä¸€çº§æ ‡é¢˜ï¼ˆHeading 1ï¼‰
            if heading_level == 1 and any(re.match(pattern, para_text) for pattern in self.NUMBERING_PATTERNS):
                reason = f"Heading{heading_level}æ ·å¼+ç¼–å·æ ¼å¼"
                self.logger.info(f"  âœ“ é”šç‚¹è¯†åˆ«æˆåŠŸï¼ˆæ ·å¼ï¼‰: æ®µè½ {start_idx}: '{para_text}' ({reason})")
                return True, clean_text, heading_level, reason

        return False, None, 0, f"æ— åŒ¹é…ï¼ˆæœ€ä½³ç›¸ä¼¼åº¦{best_score:.0%}ï¼‰"

    def _calculate_content_start_idx(self, doc: Document, toc_end_idx: int, toc_items_count: int) -> int:
        """
        è®¡ç®—æ­£æ–‡èµ·å§‹ä½ç½®ï¼ˆç®€åŒ–ç‰ˆï¼šç›´æ¥ä»ç›®å½•ç»“æŸåå¼€å§‹ï¼‰

        ç­–ç•¥ï¼š
        ç›®å½•å·²é€šè¿‡é‡å¤æ£€æµ‹æ­£ç¡®ç»“æŸï¼Œç›´æ¥ä»ç›®å½•ç»“æŸä½ç½®çš„ä¸‹ä¸€æ®µå¼€å§‹æœç´¢å³å¯

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            toc_end_idx: ç›®å½•ç»“æŸæ®µè½ç´¢å¼•
            toc_items_count: ç›®å½•é¡¹æ•°é‡ï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹è°ƒç”¨ï¼‰

        Returns:
            æ­£æ–‡èµ·å§‹æ®µè½ç´¢å¼•
        """
        # â­ï¸ æœ€ç®€å•ç­–ç•¥ï¼šç›®å½•ç»“æŸåç›´æ¥å¼€å§‹æœç´¢
        # ç†ç”±ï¼š
        # 1. ç›®å½•é‡å¤æ£€æµ‹å·²ç¡®ä¿ç›®å½•æ­£ç¡®ç»“æŸ
        # 2. ç›®å½•åå¯èƒ½åªæœ‰åˆ†é¡µç¬¦æˆ–å‡ è¡Œç©ºç™½
        # 3. è·³è¿‡å¤ªå¤šæ®µä¼šé”™è¿‡çœŸå®ç« èŠ‚ï¼ˆå¦‚"ç«äº‰æ€§ç£‹å•†å…¬å‘Š"ï¼‰
        min_start = toc_end_idx + 1

        self.logger.info(f"æ­£æ–‡èµ·å§‹ä½ç½®: ç›®å½•ç»“æŸäºæ®µè½{toc_end_idx}, ä»æ®µè½{min_start}å¼€å§‹æœç´¢")

        # ç›´æ¥è¿”å›min_startï¼Œè®©è¯­ä¹‰é”šç‚¹ç®—æ³•è‡ªå·±å»æ‰¾ç« èŠ‚
        # ä¸å†åšå¤æ‚çš„æ™ºèƒ½æ£€æµ‹ï¼Œé¿å…è·³è¿‡çœŸå®ç« èŠ‚
        return min_start

    def _is_metadata_section_title(self, title: str) -> bool:
        """
        åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸ºå…ƒæ•°æ®ç« èŠ‚ï¼ˆä¸åº”ä½œä¸ºæ­£æ–‡ç« èŠ‚ï¼‰ï¼ˆæ”¹è¿›5ï¼šæ‰©å±•å…ƒæ•°æ®æ¨¡å¼ï¼‰

        Args:
            title: ç« èŠ‚æ ‡é¢˜

        Returns:
            æ˜¯å¦ä¸ºå…ƒæ•°æ®ç« èŠ‚
        """
        metadata_patterns = [
            # æ–‡æ¡£ç»“æ„ç›¸å…³
            r'.*æ–‡ä»¶æ„æˆ.*',
            r'.*æ‹›æ ‡æ–‡ä»¶ç»„æˆ.*',
            r'.*æ–‡æ¡£ç»„æˆ.*',
            r'.*é‡‡è´­æ–‡ä»¶æ¸…å•.*',
            r'.*æ–‡æ¡£è¯´æ˜.*',
            r'.*æ–‡ä»¶è¯´æ˜.*',
            r'.*æ–‡ä»¶ç›®å½•.*',
            r'.*æ–‡ä»¶æ¸…å•.*',
            # é¡¹ç›®ä¿¡æ¯ç›¸å…³ï¼ˆæ”¹è¿›5æ–°å¢ï¼‰
            r'^é¡¹ç›®ç¼–å·.*',
            r'^é¡¹ç›®åç§°.*',
            r'.*é¡¹ç›®æ¦‚å†µè¡¨.*',
            r'.*é¡¹ç›®ä¿¡æ¯è¡¨.*',
            # ç›®å½•ç±»ï¼ˆæ”¹è¿›5æ–°å¢ï¼‰
            r'^ç›®\s*å½•$',
            r'^contents$',
            r'^ç´¢\s*å¼•$',
            # å‰è¨€ã€åºè¨€ç±»ï¼ˆæ”¹è¿›5æ–°å¢ï¼‰
            r'^å‰\s*è¨€$',
            r'^åº\s*è¨€$',
            r'^å¼•\s*è¨€$',
            # å…¶ä»–å…ƒæ•°æ®ï¼ˆæ”¹è¿›5æ–°å¢ï¼‰
            r'.*ç¼–åˆ¶è¯´æ˜.*',
            r'.*é˜…è¯»è¯´æ˜.*',
            r'.*æ–‡æ¡£ç‰ˆæœ¬.*',
            r'.*ç‰ˆæœ¬å†å².*',
        ]

        for pattern in metadata_patterns:
            if re.match(pattern, title, re.IGNORECASE):
                return True
        return False

    def _is_file_composition_section(self, doc: Document, para_idx: int, toc_targets: List[str]) -> bool:
        """
        æ£€æµ‹å½“å‰æ®µè½æ˜¯å¦ä¸º"æ–‡ä»¶æ„æˆ"éƒ¨åˆ†ï¼ˆè¿ç»­çš„ç« èŠ‚æ ‡é¢˜åˆ—è¡¨ï¼Œæ— å®é™…å†…å®¹ï¼‰

        ç‰¹å¾ï¼š
        1. æ®µè½æ–‡æœ¬åŒ¹é…æŸä¸ªç›®å½•é¡¹
        2. å‰åæ®µè½ä¹Ÿæ˜¯è¿ç»­çš„ç« èŠ‚æ ‡é¢˜
        3. ç« èŠ‚æ ‡é¢˜ä¹‹é—´æ— å†…å®¹æˆ–åªæœ‰æå°‘å†…å®¹
        4. å‰ç½®æ®µè½åŒ…å«"ç”±...ç»„æˆ"ã€"æ–‡ä»¶æ„æˆ"ç­‰å…³é”®è¯

        Bugä¿®å¤: éœ€è¦åŒæ—¶æ»¡è¶³"æœ‰å…³é”®è¯"å’Œ"è¿ç»­æ ‡é¢˜"ä¸¤ä¸ªæ¡ä»¶ï¼Œé¿å…è¯¯åˆ¤çœŸå®ç« èŠ‚

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_idx: å½“å‰æ®µè½ç´¢å¼•
            toc_targets: ç›®å½•æ ‡é¢˜åˆ—è¡¨

        Returns:
            æ˜¯å¦ä¸ºæ–‡ä»¶æ„æˆéƒ¨åˆ†
        """
        # æ–¹æ³•1: æ£€æŸ¥å‰ç½®æ®µè½æ˜¯å¦åŒ…å«"æ–‡ä»¶æ„æˆ"ç›¸å…³å…³é”®è¯
        composition_keywords = [
            'ç”±ä¸‹è¿°éƒ¨åˆ†ç»„æˆ', 'ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆ', 'æ–‡ä»¶æ„æˆ', 'åŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†',
            'åŒ…æ‹¬ä¸‹åˆ—éƒ¨åˆ†', 'ç”±ä¸‹åˆ—éƒ¨åˆ†ç»„æˆ', 'æ–‡ä»¶åŒ…æ‹¬', 'æ–‡ä»¶ç”±',
            'ç”±ä»¥ä¸‹æ–‡ä»¶ç»„æˆ', 'åŒ…å«ä»¥ä¸‹æ–‡ä»¶', 'è°ˆåˆ¤æ–‡ä»¶'
        ]

        has_composition_keyword = False
        # æ£€æŸ¥å‰5ä¸ªæ®µè½ï¼ˆæ‰©å¤§èŒƒå›´ï¼‰
        for i in range(max(0, para_idx - 5), para_idx):
            prev_text = doc.paragraphs[i].text.strip()
            if any(keyword in prev_text for keyword in composition_keywords):
                self.logger.debug(f"æ£€æµ‹åˆ°æ–‡ä»¶æ„æˆå…³é”®è¯: æ®µè½{i} å« '{prev_text[:50]}'")
                has_composition_keyword = True
                break

        # æ–¹æ³•2: æ£€æŸ¥å½“å‰æ®µè½åŠå‰ååŒºåŸŸæ˜¯å¦æœ‰è¿ç»­çš„ç« èŠ‚æ ‡é¢˜ä¸”ä¹‹é—´æ— å†…å®¹
        check_range = 5
        consecutive_titles = 0
        title_positions = []  # è®°å½•æ ‡é¢˜ä½ç½®

        for i in range(max(0, para_idx - check_range), min(len(doc.paragraphs), para_idx + check_range + 1)):
            para_text = doc.paragraphs[i].text.strip()

            # æ£€æŸ¥æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜æ ¼å¼ï¼ˆæ‰©å±•ï¼šåŒ…å«æ•°å­—åˆ—è¡¨æ ¼å¼ï¼‰
            is_chapter_title = bool(
                re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', para_text) or
                re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', para_text) or
                re.match(r'^\d+[\.\ã€][\u4e00-\u9fa5]{3,20}$', para_text)  # åŒ¹é… "1.xxx" æˆ– "2ã€xxx" æ ¼å¼
            )

            if is_chapter_title:
                consecutive_titles += 1
                title_positions.append(i)

        # æ–¹æ³•3: æ£€æŸ¥æ ‡é¢˜ä¹‹é—´æ˜¯å¦æœ‰å®è´¨å†…å®¹
        has_content_between_titles = False
        if len(title_positions) >= 2:
            # æ£€æŸ¥ä»»æ„ä¸¤ä¸ªç›¸é‚»æ ‡é¢˜ä¹‹é—´çš„æ®µè½
            for j in range(len(title_positions) - 1):
                start_pos = title_positions[j]
                end_pos = title_positions[j + 1]

                # è®¡ç®—ä¹‹é—´çš„å†…å®¹å­—æ•°
                content_chars = 0
                for k in range(start_pos + 1, end_pos):
                    content_chars += len(doc.paragraphs[k].text.strip())

                # å¦‚æœä»»æ„ä¸¤ä¸ªæ ‡é¢˜ä¹‹é—´æœ‰è¶…è¿‡100å­—çš„å†…å®¹ï¼Œè¯´æ˜ä¸æ˜¯æ–‡ä»¶æ„æˆåˆ—è¡¨
                if content_chars > 100:
                    has_content_between_titles = True
                    self.logger.debug(f"æ£€æµ‹åˆ°æ ‡é¢˜é—´æœ‰å®è´¨å†…å®¹: æ®µè½{start_pos}-{end_pos}ä¹‹é—´æœ‰{content_chars}å­—")
                    break

        # åˆ¤æ–­é€»è¾‘ï¼šéœ€è¦åŒæ—¶æ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ‰åˆ¤å®šä¸ºæ–‡ä»¶æ„æˆ
        # 1. æœ‰"æ–‡ä»¶æ„æˆ"å…³é”®è¯ AND
        # 2. æœ‰3ä¸ªä»¥ä¸Šè¿ç»­æ ‡é¢˜ AND
        # 3. æ ‡é¢˜ä¹‹é—´æ²¡æœ‰å®è´¨å†…å®¹
        is_composition = (
            has_composition_keyword and
            consecutive_titles >= 3 and
            not has_content_between_titles
        )

        if is_composition:
            self.logger.debug(
                f"ç¡®è®¤ä¸ºæ–‡ä»¶æ„æˆ: æ®µè½{para_idx}, "
                f"æœ‰å…³é”®è¯={has_composition_keyword}, "
                f"è¿ç»­æ ‡é¢˜={consecutive_titles}, "
                f"æ— å®è´¨å†…å®¹={not has_content_between_titles}"
            )

        return is_composition

    def _parse_chapters_by_semantic_anchors(self, doc: Document, toc_targets: List[str], toc_end_idx: int = 0) -> List[ChapterNode]:
        """
        åŸºäºè¯­ä¹‰é”šç‚¹è§£æç« èŠ‚ï¼ˆæ ¸å¿ƒæ–°æ–¹æ³•ï¼‰

        ç­–ç•¥ï¼šä¸¥æ ¼æŒ‰ç›®å½•é¡ºåºåŒ¹é…ï¼Œæ‰¾åˆ°æ¯ä¸ªç›®å½•é¡¹åœ¨æ­£æ–‡ä¸­ç›¸ä¼¼åº¦æœ€é«˜çš„ä½ç½®

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            toc_targets: ç›®å½•æ ‡é¢˜åˆ—è¡¨
            toc_end_idx: ç›®å½•ç»“æŸä½ç½®ï¼ˆä»æ­¤ä¹‹åå¼€å§‹è¯†åˆ«ç« èŠ‚ï¼‰

        Returns:
            ç« èŠ‚åˆ—è¡¨ï¼ˆæ‰å¹³ç»“æ„ï¼Œæœªæ„å»ºæ ‘ï¼‰
        """
        chapters = []
        last_found_idx = toc_end_idx + 1  # ä¸Šä¸€ä¸ªç« èŠ‚æ‰¾åˆ°çš„ä½ç½®ï¼Œç¡®ä¿æŒ‰é¡ºåºæŸ¥æ‰¾

        # æ”¹è¿›1ï¼šä½¿ç”¨æ™ºèƒ½æ£€æµ‹è®¡ç®—æ­£æ–‡èµ·å§‹ä½ç½®
        min_search_start = self._calculate_content_start_idx(doc, toc_end_idx, len(toc_targets))

        # ä¼˜åŒ–1: è®¡ç®—åŠ¨æ€é˜ˆå€¼
        dynamic_threshold = self._calculate_dynamic_threshold(len(toc_targets), len(doc.paragraphs))

        self.logger.info(f"å¼€å§‹æŒ‰ç›®å½•é¡ºåºè§£æç« èŠ‚ï¼Œå…± {len(toc_targets)} ä¸ªç›®æ ‡")
        self.logger.info(f"ç›®å½•ç»“æŸäºæ®µè½ {toc_end_idx}ï¼Œæ­£æ–‡æœç´¢èµ·ç‚¹: æ®µè½ {min_search_start} (è·³è¿‡ {min_search_start - toc_end_idx} æ®µ)")
        self.logger.info(f"ä½¿ç”¨åŠ¨æ€é˜ˆå€¼: {dynamic_threshold:.2f}")

        for i, toc_title in enumerate(toc_targets):
            self.logger.info(f"\n[{i+1}/{len(toc_targets)}] æŸ¥æ‰¾ç›®å½•é¡¹: '{toc_title}'")

            # åœ¨å‰©ä½™æ®µè½ä¸­å¯»æ‰¾æœ€ä½³åŒ¹é…
            best_match_idx = None
            best_score = 0.0
            best_para_text = None

            # ç»Ÿä¸€æœç´¢ç­–ç•¥ï¼šæ‰€æœ‰ç« èŠ‚ä»ç›®å½•ç»“æŸåæˆ–ä¸Šä¸€ä¸ªä½ç½®å¼€å§‹æœç´¢
            search_start = max(last_found_idx, min_search_start)
            search_end = len(doc.paragraphs)
            self.logger.info(f"  æœç´¢èŒƒå›´: æ®µè½ {search_start} - {search_end}")

            # â­ï¸ æ”¹è¿›ï¼šæ”¶é›†æ‰€æœ‰ç¬¦åˆé˜ˆå€¼çš„å€™é€‰ï¼Œè€Œä¸æ˜¯åªè®°å½•æœ€ä½³
            # è¿™æ ·å¯ä»¥åœ¨æœ€ä½³å€™é€‰æ˜¯"æ–‡ä»¶æ„æˆ"æ—¶ï¼Œä½¿ç”¨æ¬¡ä¼˜å€™é€‰
            all_candidates = []  # æ‰€æœ‰ç¬¦åˆé˜ˆå€¼çš„å€™é€‰

            for para_idx in range(search_start, search_end):
                paragraph = doc.paragraphs[para_idx]
                para_text = paragraph.text.strip()

                # è·³è¿‡ç©ºè¡Œæˆ–å¤ªé•¿çš„æ®µè½
                if not para_text or len(para_text) > 100:
                    continue

                # â­ï¸ è·³è¿‡æ¨¡æ¿å ä½ç¬¦æ–‡æœ¬ï¼ˆå¦‚"ï¼ˆé¡¹ç›®åç§°ï¼‰"ã€"ï¼ˆé‡‡è´­ç¼–å·ï¼‰"ï¼‰
                template_placeholders = ['ï¼ˆé¡¹ç›®åç§°ï¼‰', 'ï¼ˆé‡‡è´­ç¼–å·ï¼‰', 'ï¼ˆä¾›åº”å•†åç§°ï¼‰', 'ï¼ˆå§“åã€èŒåŠ¡ï¼‰']
                if any(placeholder in para_text for placeholder in template_placeholders):
                    continue  # è¿™æ˜¯æ¨¡æ¿ç¤ºä¾‹æ–‡å­—ï¼Œä¸æ˜¯çœŸå®ç« èŠ‚æ ‡é¢˜

                # ç§»é™¤ç¼–å·
                clean_para, para_level = self.remove_leading_patterns(para_text)
                clean_toc, toc_level = self.remove_leading_patterns(toc_title)

                # è®¡ç®—ç›¸ä¼¼åº¦ (ä½¿ç”¨æ–°çš„åˆ†é˜¶æ®µåŒ¹é…å‡½æ•°)
                score = self.fuzzy_match_title_v2(para_text, toc_title, threshold=dynamic_threshold)

                # æ£€æŸ¥æ˜¯å¦æœ‰"ç¬¬Xéƒ¨åˆ†"ç¼–å·
                has_part_number = bool(re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', para_text))

                # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—åˆ—è¡¨æ ¼å¼ï¼ˆå¦‚ "1.xxx" æˆ– "2ã€xxx"ï¼‰
                is_numbered_list = bool(re.match(r'^\d+[\.\ã€][\u4e00-\u9fa5]{3,20}$', para_text))

                # æ”¶é›†æ‰€æœ‰ç¬¦åˆé˜ˆå€¼çš„å€™é€‰
                if score >= dynamic_threshold and not is_numbered_list:
                    # ä¼˜å…ˆçº§æƒé‡ï¼šæœ‰"ç¬¬Xéƒ¨åˆ†"çš„åŠ 0.05
                    priority_score = score + (0.05 if has_part_number else 0)
                    all_candidates.append((priority_score, score, para_idx, para_text, has_part_number))

                # é™åˆ¶æœç´¢èŒƒå›´ï¼šæœ€å¤šå‘åæœç´¢800æ®µï¼ˆè¦†ç›–å¤§éƒ¨åˆ†æ ‡ä¹¦ï¼‰
                if para_idx - search_start > 800:
                    break

            # æŒ‰ä¼˜å…ˆçº§åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
            all_candidates.sort(reverse=True, key=lambda x: x[0])

            # â­ï¸ æ ¸å¿ƒæ”¹è¿›ï¼šé€ä¸ªéªŒè¯å€™é€‰ï¼Œè·³è¿‡æ–‡ä»¶æ„æˆï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„
            best_score = 0.0
            best_match_idx = None
            best_para_text = None

            for priority_score, score, para_idx, para_text, has_part_number in all_candidates:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶æ„æˆ
                if not self._is_file_composition_section(doc, para_idx, toc_targets):
                    # ä¸æ˜¯æ–‡ä»¶æ„æˆï¼Œä½¿ç”¨è¿™ä¸ªå€™é€‰ï¼
                    best_score = score
                    best_match_idx = para_idx
                    best_para_text = para_text
                    self.logger.info(
                        f"  âœ“ é€‰æ‹©å€™é€‰({score:.0%}): æ®µè½{para_idx} '{para_text}' "
                        f"{'[æœ‰ç¼–å·]' if has_part_number else ''}"
                    )
                    break
                else:
                    self.logger.debug(
                        f"  âŠ— è·³è¿‡æ–‡ä»¶æ„æˆå€™é€‰({score:.0%}): æ®µè½{para_idx} '{para_text}'"
                    )

            # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½æ˜¯æ–‡ä»¶æ„æˆï¼Œè®°å½•æ—¥å¿—
            if not best_match_idx and all_candidates:
                self.logger.warning(f"  æ‰€æœ‰{len(all_candidates)}ä¸ªå€™é€‰éƒ½æ˜¯æ–‡ä»¶æ„æˆï¼Œå°†å°è¯•é‡æ–°æœç´¢")

            # åˆ¤æ–­æ˜¯å¦æ‰¾åˆ°æœ‰æ•ˆåŒ¹é…
            if best_match_idx is not None:
                # å€™é€‰åˆ—è¡¨éªŒè¯å·²ç¡®ä¿best_match_idxä¸æ˜¯æ–‡ä»¶æ„æˆ
                # ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦å†æ¬¡æ£€æµ‹

                # ä¸‹é¢è¿™æ®µæ—§çš„æ£€æµ‹é€»è¾‘å·²è¢«å€™é€‰åˆ—è¡¨éªŒè¯æ›¿ä»£ï¼Œä¿ç•™ç”¨äºå…¼å®¹
                if False and self._is_file_composition_section(doc, best_match_idx, toc_targets):
                    self.logger.warning(
                        f"  âš  è·³è¿‡æ–‡ä»¶æ„æˆéƒ¨åˆ†: æ®µè½{best_match_idx}æ£€æµ‹åˆ°è¿ç»­ç« èŠ‚æ ‡é¢˜"
                    )
                    # ç»§ç»­å‘åæœç´¢çœŸå®ç« èŠ‚ï¼ˆä»åŒ¹é…ä½ç½®ä¹‹åå¼€å§‹ï¼‰
                    self.logger.info(f"  â†’ ä»æ®µè½{best_match_idx + 10}é‡æ–°æœç´¢çœŸå®ç« èŠ‚")

                    # é‡æ–°æœç´¢ï¼ˆè·³è¿‡æ–‡ä»¶æ„æˆåŒºåŸŸï¼Œè‡³å°‘å‘å10æ®µï¼‰
                    new_search_start = best_match_idx + 10
                    found_real_chapter = False
                    for para_idx in range(new_search_start, len(doc.paragraphs)):
                        paragraph = doc.paragraphs[para_idx]
                        para_text = paragraph.text.strip()

                        if not para_text or len(para_text) > 100:
                            continue

                        clean_para, _ = self.remove_leading_patterns(para_text)
                        clean_toc, _ = self.remove_leading_patterns(toc_title)
                        score = self.fuzzy_match_title_v2(para_text, toc_title, threshold=dynamic_threshold)

                        if score >= dynamic_threshold:
                            # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ä¸ºæ–‡ä»¶æ„æˆ
                            if not self._is_file_composition_section(doc, para_idx, toc_targets):
                                best_match_idx = para_idx
                                best_para_text = para_text
                                best_score = score
                                found_real_chapter = True
                                self.logger.info(f"  âœ“ æ‰¾åˆ°çœŸå®ç« èŠ‚({score:.0%}): æ®µè½{para_idx} '{para_text}'")
                                break

                        if para_idx - new_search_start > 800:
                            break

                    if not found_real_chapter:
                        # å¦‚æœå‘åæ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨æ™ºèƒ½èµ·ç‚¹ä¹‹å‰æœç´¢ï¼ˆå¯èƒ½çœŸå®ç« èŠ‚åœ¨å‰é¢ï¼‰
                        self.logger.info(f"  â†’ å°è¯•åœ¨æ™ºèƒ½èµ·ç‚¹{min_search_start}ä¹‹å‰æœç´¢")
                        for para_idx in range(toc_end_idx + 1, min_search_start):
                            paragraph = doc.paragraphs[para_idx]
                            para_text = paragraph.text.strip()

                            if not para_text or len(para_text) > 100:
                                continue

                            clean_para, _ = self.remove_leading_patterns(para_text)
                            clean_toc, _ = self.remove_leading_patterns(toc_title)
                            score = self.fuzzy_match_title(clean_para, clean_toc, threshold=0.70)

                            if score >= 0.70:
                                # æ£€æŸ¥æ˜¯å¦ä»ä¸ºæ–‡ä»¶æ„æˆ
                                if not self._is_file_composition_section(doc, para_idx, toc_targets):
                                    best_match_idx = para_idx
                                    best_para_text = para_text
                                    best_score = score
                                    found_real_chapter = True
                                    self.logger.info(f"  âœ“ åœ¨å‰é¢æ‰¾åˆ°çœŸå®ç« èŠ‚({score:.0%}): æ®µè½{para_idx} '{para_text}'")
                                    break

                    if not found_real_chapter:
                        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œè·³è¿‡è¿™ä¸ªç›®å½•é¡¹
                        self.logger.warning(f"  âœ— æœªæ‰¾åˆ°çœŸå®ç« èŠ‚ï¼Œè·³è¿‡ç›®å½•é¡¹: '{toc_title}'")
                        continue

                # åˆ›å»ºç« èŠ‚èŠ‚ç‚¹
                clean_title, level = self.remove_leading_patterns(toc_title)

                # åˆ¤æ–­æ˜¯å¦åŒ¹é…ç™½/é»‘åå•
                auto_selected = self._matches_whitelist(toc_title)
                skip_recommended = self._matches_blacklist(toc_title)
                if skip_recommended:
                    auto_selected = False

                # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªç›®å½•é¡¹çš„ä½ç½®ï¼‰
                if i + 1 < len(toc_targets):
                    # æš‚æ—¶è®¾ä¸ºæ–‡æ¡£æœ«å°¾ï¼Œåç»­ä¼šæ›´æ–°
                    para_end_idx = len(doc.paragraphs) - 1
                else:
                    para_end_idx = len(doc.paragraphs) - 1

                chapter = ChapterNode(
                    id=f"ch_{i}",
                    level=level,
                    title=toc_title,  # ä½¿ç”¨ç›®å½•ä¸­çš„æ ‡é¢˜
                    para_start_idx=best_match_idx,
                    para_end_idx=para_end_idx,  # ç¨åæ›´æ–°
                    word_count=0,
                    preview_text="",
                    auto_selected=auto_selected,
                    skip_recommended=skip_recommended
                )

                chapters.append(chapter)
                last_found_idx = best_match_idx + 1  # æ›´æ–°æœç´¢èµ·ç‚¹
            else:
                # Bugä¿®å¤: åœ¨æ™ºèƒ½èµ·ç‚¹ä¹‹åæœªæ‰¾åˆ°ï¼Œå°è¯•å›æº¯åˆ°ç›®å½•ååŒºåŸŸæœç´¢
                self.logger.warning(f"  âœ— æœªæ‰¾åˆ°åŒ¹é…ï¼ˆæœ€ä½³ç›¸ä¼¼åº¦{best_score:.0%}ï¼‰: '{toc_title}'")

                # å¦‚æœæ™ºèƒ½èµ·ç‚¹å¤§äºç›®å½•ç»“æŸä½ç½®ï¼Œè¯´æ˜è·³è¿‡äº†ä¸€äº›æ®µè½ï¼Œå°è¯•åœ¨è·³è¿‡çš„åŒºåŸŸæœç´¢
                if min_search_start > toc_end_idx + 1:
                    self.logger.info(f"  â†’ å›æº¯æœç´¢: å°è¯•åœ¨ç›®å½•ååŒºåŸŸ(æ®µè½{toc_end_idx + 1}-{min_search_start})æœç´¢")

                    found_in_backtrack = False
                    backtrack_best_score = 0.0
                    backtrack_best_idx = None
                    backtrack_best_text = None

                    for para_idx in range(toc_end_idx + 1, min_search_start):
                        paragraph = doc.paragraphs[para_idx]
                        para_text = paragraph.text.strip()

                        if not para_text or len(para_text) > 100:
                            continue

                        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆé™ä½é˜ˆå€¼åˆ°0.70ï¼‰
                        clean_para, _ = self.remove_leading_patterns(para_text)
                        clean_toc, _ = self.remove_leading_patterns(toc_title)
                        score = self.fuzzy_match_title(clean_para, clean_toc, threshold=0.70)

                        if score >= 0.70 and score > backtrack_best_score:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶æ„æˆ
                            if not self._is_file_composition_section(doc, para_idx, toc_targets):
                                backtrack_best_score = score
                                backtrack_best_idx = para_idx
                                backtrack_best_text = para_text

                    if backtrack_best_idx is not None:
                        self.logger.info(f"  âœ“ å›æº¯æ‰¾åˆ°ç« èŠ‚({backtrack_best_score:.0%}): æ®µè½{backtrack_best_idx} '{backtrack_best_text}'")

                        # åˆ›å»ºç« èŠ‚èŠ‚ç‚¹
                        clean_title, level = self.remove_leading_patterns(toc_title)
                        auto_selected = self._matches_whitelist(toc_title)
                        skip_recommended = self._matches_blacklist(toc_title)
                        if skip_recommended:
                            auto_selected = False

                        chapter = ChapterNode(
                            id=f"ch_{i}",
                            level=level,
                            title=toc_title,
                            para_start_idx=backtrack_best_idx,
                            para_end_idx=len(doc.paragraphs) - 1,
                            word_count=0,
                            preview_text="",
                            auto_selected=auto_selected,
                            skip_recommended=skip_recommended
                        )

                        chapters.append(chapter)
                        last_found_idx = backtrack_best_idx + 1
                        found_in_backtrack = True

                    if not found_in_backtrack:
                        self.logger.warning(f"  âœ— å›æº¯æœç´¢ä¹Ÿæœªæ‰¾åˆ°ï¼Œè·³è¿‡ç›®å½•é¡¹: '{toc_title}'")

        # â­ï¸ å…³é”®ä¿®å¤ï¼šæŒ‰æ®µè½ç´¢å¼•æ’åºç« èŠ‚ï¼Œç¡®ä¿ç« èŠ‚é¡ºåºä¸æ–‡æ¡£ç‰©ç†é¡ºåºä¸€è‡´
        # è¿™å¯ä»¥é˜²æ­¢ç´¢å¼•å€’ç½®é—®é¢˜ï¼ˆå¦‚ para_start_idx=542 > para_end_idx=62ï¼‰
        chapters_sorted = sorted(chapters, key=lambda ch: ch.para_start_idx)
        self.logger.info(f"ç« èŠ‚å·²æŒ‰æ®µè½ç´¢å¼•æ’åºï¼Œå…± {len(chapters_sorted)} ä¸ªç« èŠ‚")

        # â­ï¸ å…³é”®ä¿®å¤ï¼šæ’åºåé‡æ–°åˆ†é…ç« èŠ‚IDï¼Œç¡®ä¿IDé¡ºåºä¸æ–‡æ¡£ç‰©ç†é¡ºåºä¸€è‡´
        # é¿å…å‰ç«¯æŒ‰IDæ’åºæ—¶å‡ºç°ä¹±åº
        for idx, chapter in enumerate(chapters_sorted):
            chapter.id = f"ch_{idx}"
        self.logger.info(f"ç« èŠ‚IDå·²æŒ‰ç‰©ç†é¡ºåºé‡æ–°åˆ†é…")

        # æ›´æ–°æ‰€æœ‰ç« èŠ‚çš„ç»“æŸä½ç½®å’Œå†…å®¹
        for i, chapter in enumerate(chapters_sorted):
            if i + 1 < len(chapters_sorted):
                chapter.para_end_idx = chapters_sorted[i + 1].para_start_idx - 1

            # æå–å†…å®¹å’Œé¢„è§ˆ
            self._extract_chapter_content(doc, chapter)

            self.logger.info(
                f"ç« èŠ‚ [{chapter.level}çº§]: {chapter.title} "
                f"({'âœ…è‡ªåŠ¨é€‰ä¸­' if chapter.auto_selected else 'âŒè·³è¿‡' if chapter.skip_recommended else 'âšªé»˜è®¤'}) "
                f"(æ®µè½ {chapter.para_start_idx}-{chapter.para_end_idx}, {chapter.word_count}å­—)"
            )

        self.logger.info(f"\nè¯­ä¹‰é”šç‚¹è§£æå®Œæˆï¼ŒæˆåŠŸè¯†åˆ« {len(chapters_sorted)}/{len(toc_targets)} ä¸ªç« èŠ‚")

        return chapters_sorted

    def _detect_content_tags(self, content_text: str) -> List[str]:
        """
        æ£€æµ‹ç« èŠ‚å†…å®¹æ ‡ç­¾

        åŸºäºå†…å®¹å…³é”®è¯åŒ¹é…ï¼Œæ£€æµ‹ç« èŠ‚åŒ…å«çš„ä¿¡æ¯ç±»å‹

        Args:
            content_text: ç« èŠ‚å†…å®¹æ–‡æœ¬

        Returns:
            æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªæ ‡ç­¾ï¼‰
        """
        tags = []
        content_lower = content_text.lower()

        # å®šä¹‰æ ‡ç­¾åŠå…¶å…³é”®è¯
        tag_rules = {
            "è¯„åˆ†åŠæ³•": ["è¯„åˆ†åŠæ³•", "è¯„åˆ†æ ‡å‡†", "è¯„å®¡åŠæ³•", "æ‰“åˆ†", "è¯„åˆ†ç»†åˆ™", "ç»¼åˆè¯„åˆ†"],
            "è¯„åˆ†è¡¨": ["å‰é™„è¡¨", "è¯„åˆ†è¡¨", "é™„è¡¨", "è¯„å®¡è¡¨"],
            "ä¾›åº”å•†èµ„è´¨": ["ä¾›åº”å•†èµ„è´¨", "èµ„è´¨è¦æ±‚", "èµ„æ ¼è¦æ±‚", "æŠ•æ ‡äººèµ„æ ¼", "ä¾›åº”å•†é¡»çŸ¥",
                       "æŠ•æ ‡é¡»çŸ¥", "æ°‘äº‹è´£ä»»", "å•†ä¸šä¿¡èª‰", "æŠ€æœ¯èƒ½åŠ›"],
            "æ–‡ä»¶æ ¼å¼": ["æ–‡ä»¶æ ¼å¼", "æ ¼å¼è¦æ±‚", "ç¼–åˆ¶è¦æ±‚", "è£…è®¢è¦æ±‚", "å“åº”æ–‡ä»¶ç¼–åˆ¶"],
            "æŠ€æœ¯éœ€æ±‚": ["æŠ€æœ¯è§„èŒƒ", "æŠ€æœ¯è¯´æ˜", "æŠ€æœ¯éœ€æ±‚", "æŠ€æœ¯è¦æ±‚", "æŠ€æœ¯å‚æ•°",
                       "æ€§èƒ½æŒ‡æ ‡", "åŠŸèƒ½è¦æ±‚", "éœ€æ±‚è¯´æ˜"]
        }

        # æ£€æµ‹æ¯ä¸ªæ ‡ç­¾
        for tag, keywords in tag_rules.items():
            for keyword in keywords:
                if keyword in content_lower:
                    tags.append(tag)
                    break  # åŒ¹é…åˆ°ä¸€ä¸ªå…³é”®è¯å³å¯ï¼Œä¸éœ€è¦ç»§ç»­æ£€æŸ¥è¯¥æ ‡ç­¾çš„å…¶ä»–å…³é”®è¯

        return tags

    def _extract_chapter_content(self, doc: Document, chapter: ChapterNode):
        """
        æå–ç« èŠ‚å†…å®¹ã€å­—æ•°å’Œé¢„è§ˆæ–‡æœ¬

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            chapter: ç« èŠ‚èŠ‚ç‚¹ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
        """
        # æå–å†…å®¹ï¼ˆä»æ ‡é¢˜çš„ä¸‹ä¸€æ®µå¼€å§‹ï¼‰
        content_paras = doc.paragraphs[chapter.para_start_idx + 1 : chapter.para_end_idx + 1]

        # è®¡ç®—å­—æ•°
        content_text = '\n'.join(p.text for p in content_paras)
        chapter.word_count = len(content_text.replace(' ', '').replace('\n', ''))

        # æå–é¢„è§ˆæ–‡æœ¬ï¼ˆå‰5è¡Œï¼Œæ¯è¡Œæœ€å¤š100å­—ç¬¦ï¼‰
        preview_lines = []
        for p in content_paras[:5]:
            text = p.text.strip()
            if text:
                preview_lines.append(text[:100] + ('...' if len(text) > 100 else ''))
            if len(preview_lines) >= 5:
                break

        chapter.preview_text = '\n'.join(preview_lines) if preview_lines else "(æ— å†…å®¹)"

        # æ£€æµ‹å†…å®¹æ ‡ç­¾
        chapter.content_tags = self._detect_content_tags(content_text)

        # ã€æ–°å¢ã€‘å¯¹äºlevel 1-2çš„ç« èŠ‚ï¼Œæå–å†…å®¹æ ·æœ¬å¹¶è¿›è¡ŒåˆåŒè¯†åˆ«
        if chapter.level <= 2 and chapter.para_end_idx and chapter.para_end_idx > chapter.para_start_idx:
            # æå–å†…å®¹æ ·æœ¬ç”¨äºåˆåŒè¯†åˆ«
            chapter.content_sample = self._extract_content_sample(
                doc, chapter.para_start_idx, chapter.para_end_idx, sample_size=2000
            )

            # åŸºäºå†…å®¹è¿›è¡ŒåˆåŒè¯†åˆ«
            is_contract, density, reason = self._is_contract_chapter(
                chapter.title, chapter.content_sample
            )

            if is_contract:
                # æ›´æ–°skip_recommendedæ ‡è®°
                if not chapter.skip_recommended:  # é¿å…é‡å¤æ ‡è®°
                    chapter.skip_recommended = True
                    chapter.auto_selected = False
                    self.logger.info(
                        f"  âœ“ åˆåŒç« èŠ‚è¯†åˆ«: '{chapter.title}' - {reason}"
                    )
                else:
                    self.logger.debug(
                        f"  âœ“ åˆåŒç« èŠ‚å·²æ ‡è®°: '{chapter.title}' - {reason}"
                    )


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import sys

    if len(sys.argv) > 1:
        doc_path = sys.argv[1]
    else:
        print("ç”¨æ³•: python structure_parser.py <wordæ–‡æ¡£è·¯å¾„>")
        sys.exit(1)

    parser = DocumentStructureParser()
    result = parser.parse_document_structure(doc_path)

    if result["success"]:
        print(f"\nâœ… è§£ææˆåŠŸï¼")
        print(f"ç»Ÿè®¡ä¿¡æ¯: {result['statistics']}")
        print(f"\nç« èŠ‚ç»“æ„:")

        def print_tree(chapters, indent=0):
            for ch in chapters:
                prefix = "  " * indent
                status = "âœ…" if ch["auto_selected"] else "âŒ" if ch["skip_recommended"] else "âšª"
                print(f"{prefix}{status} [{ch['level']}çº§] {ch['title']} ({ch['word_count']}å­—)")
                if ch.get("children"):
                    print_tree(ch["children"], indent + 1)

        print_tree(result["chapters"])
    else:
        print(f"\nâŒ è§£æå¤±è´¥: {result.get('error')}")
