#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ç»“æ„è§£æå™¨ - ç”¨äº HITL 1 (äººå·¥ç« èŠ‚é€‰æ‹©)
åŠŸèƒ½ï¼š
- è§£æ Word æ–‡æ¡£çš„ç›®å½•ç»“æ„
- è¯†åˆ«ç« èŠ‚å±‚çº§ï¼ˆH1/H2/H3ï¼‰
- åŸºäºç™½åå•è‡ªåŠ¨æ¨èç« èŠ‚
- æä¾›ç« èŠ‚é¢„è§ˆæ–‡æœ¬
"""

import re
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from docx import Document
from docx.oxml import CT_Tbl, CT_P
from difflib import SequenceMatcher

from common import get_module_logger
from common.utils import resolve_file_path
from .level_analyzer import LevelAnalyzer

logger = get_module_logger("structure_parser")


@dataclass
class ChapterNode:
    """ç« èŠ‚èŠ‚ç‚¹æ•°æ®ç±»"""
    id: str                      # å”¯ä¸€IDï¼Œå¦‚ "ch_1_2_3"
    level: int                   # å±‚çº§ï¼š1=H1, 2=H2, 3=H3
    title: str                   # ç« èŠ‚æ ‡é¢˜
    para_start_idx: int          # èµ·å§‹æ®µè½ç´¢å¼•
    para_end_idx: int            # ç»“æŸæ®µè½ç´¢å¼•ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
    word_count: int              # å­—æ•°ç»Ÿè®¡
    preview_text: str            # é¢„è§ˆæ–‡æœ¬ï¼ˆå‰5è¡Œï¼‰
    has_table: bool = False      # ç« èŠ‚æ˜¯å¦åŒ…å«è¡¨æ ¼
    content_tags: List[str] = None  # å†…å®¹æ ‡ç­¾ï¼ˆåŸºäºå†…å®¹å…³é”®è¯æ£€æµ‹ï¼‰
    content_sample: str = None   # å†…å®¹æ ·æœ¬ï¼ˆç”¨äºåˆåŒè¯†åˆ«ï¼Œ1500-2000å­—ï¼‰
    children: List['ChapterNode'] = None  # å­ç« èŠ‚åˆ—è¡¨

    def __post_init__(self):
        if self.children is None:
            self.children = []
        if self.content_tags is None:
            self.content_tags = []

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰"""
        data = asdict(self)
        data['children'] = [child.to_dict() for child in self.children]
        return data


class DocumentStructureParser:
    """æ–‡æ¡£ç»“æ„è§£æå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–è§£æå™¨"""
        self.logger = get_module_logger("structure_parser")

        # ========================================
        # æ–°å¢ï¼šç¼–å·æ¨¡å¼ï¼ˆç”¨äºè¯†åˆ«ç« èŠ‚é”šç‚¹ï¼‰ï¼ˆæ”¹è¿›4ï¼šæ‰©å±•ç¼–å·æ¨¡å¼ï¼‰
        # ========================================
        self.NUMBERING_PATTERNS = [
            # ä¸­æ–‡éƒ¨åˆ†/ç« èŠ‚ç¼–å·
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+éƒ¨åˆ†\s*',
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+ç« \s*',
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+èŠ‚\s*',
            # æ•°å­—ç¼–å·
            r'^\d+\.\s*',           # 1.
            r'^\d+\.\d+\s*',        # 1.1
            r'^\d+\.\d+\.\d+\s*',   # 1.1.1
            r'^\d+\.\d+\.\d+\.\d+\s*',  # 1.1.1.1ï¼ˆå››çº§ç¼–å·ï¼Œæ”¹è¿›4æ–°å¢ï¼‰
            # ä¸­æ–‡åºå·
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s*',
            r'^ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰\s*',
            r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s*',
            # å­—æ¯ç¼–å·
            r'^[A-Z]\.\s*',
            r'^[a-z]\.\s*',
            r'^\([A-Za-z]\)\s*',
            # ç½—é©¬æ•°å­—
            r'^[IVX]+\.\s*',
            r'^[ivx]+\.\s*',
            # é™„ä»¶/é™„è¡¨ç¼–å·ï¼ˆæ”¹è¿›4æ–°å¢ï¼‰
            r'^é™„ä»¶[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*',  # é™„ä»¶1: æˆ– é™„ä»¶ä¸€ï¼š
            r'^é™„è¡¨[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*',  # é™„è¡¨1: æˆ– é™„è¡¨ä¸€ï¼š
            r'^é™„å½•[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[:ï¼š]\s*',  # é™„å½•1: æˆ– é™„å½•ä¸€ï¼š
        ]


        # æ ‡é¢˜æ ·å¼åç§°æ˜ å°„ï¼ˆä¸­è‹±æ–‡ï¼‰
        self.HEADING_STYLES = {
            1: ['Heading 1', 'æ ‡é¢˜ 1', 'heading 1', '1çº§æ ‡é¢˜'],
            2: ['Heading 2', 'æ ‡é¢˜ 2', 'heading 2', '2çº§æ ‡é¢˜'],
            3: ['Heading 3', 'æ ‡é¢˜ 3', 'heading 3', '3çº§æ ‡é¢˜'],
        }

    def parse_document_structure(
        self,
        doc_path: str,
        methods: Optional[List[str]] = None,
        fallback: bool = True
    ) -> Dict:
        """
        è§£ææ–‡æ¡£ç»“æ„ - æ€»è°ƒç”¨å™¨

        æ”¯æŒæŒ‡å®šè§£ææ–¹æ³•æˆ–ä½¿ç”¨é»˜è®¤æ™ºèƒ½ç­–ç•¥

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„
            methods: å¯é€‰ï¼ŒæŒ‡å®šè¦ä½¿ç”¨çš„è§£ææ–¹æ³•åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•
                    å¯é€‰å€¼: ['toc_exact', 'semantic_anchors', 'style', 'hybrid',
                            'azure', 'outline_level', 'gemini']
                    é»˜è®¤Noneè¡¨ç¤ºä½¿ç”¨æ™ºèƒ½ç­–ç•¥ï¼ˆæ ¹æ®æ–‡æ¡£ç‰¹å¾è‡ªåŠ¨é€‰æ‹©ï¼‰
            fallback: å¯é€‰ï¼Œæ˜¯å¦å¯ç”¨å›é€€æœºåˆ¶ï¼ˆå½“å‰æ–¹æ³•å¤±è´¥æ—¶å°è¯•ä¸‹ä¸€ä¸ªï¼‰
                     é»˜è®¤True

        Returns:
            {
                "success": True/False,
                "chapters": [ChapterNode.to_dict(), ...],
                "statistics": {
                    "total_chapters": 10,
                    "total_words": 15000
                },
                "method": "ä½¿ç”¨çš„è§£ææ–¹æ³•åç§°",
                "error": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"
            }
        """
        # æ–¹æ³•æ˜ å°„è¡¨
        method_map = {
            'toc_exact': self.parse_by_toc_exact,
            'azure': self.parse_by_azure,
            'outline_level': self.parse_by_outline_level,
            'gemini': self.parse_by_gemini
        }

        try:
            self.logger.info(f"å¼€å§‹è§£ææ–‡æ¡£ç»“æ„: {doc_path}")

            # â­ï¸ æ–‡ä»¶æ ¼å¼æ£€æµ‹ï¼šä¸æ”¯æŒæ—§ç‰ˆ.docæ ¼å¼
            if doc_path.lower().endswith('.doc'):
                error_message = (
                    "æš‚ä¸æ”¯æŒæ—§ç‰ˆ .doc æ ¼å¼æ–‡æ¡£çš„ç« èŠ‚è§£æã€‚\n\n"
                    "è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n"
                    "1. ä½¿ç”¨ Microsoft Word æˆ– WPS Office æ‰“å¼€è¯¥æ–‡ä»¶\n"
                    '2. ç‚¹å‡»"æ–‡ä»¶" â†’ "å¦å­˜ä¸º"\n'
                    '3. åœ¨"ä¿å­˜ç±»å‹"ä¸­é€‰æ‹© "Word æ–‡æ¡£ (*.docx)"\n'
                    "4. ä¿å­˜åé‡æ–°ä¸Šä¼  .docx æ–‡ä»¶\n\n"
                    "æç¤ºï¼š.docx æ ¼å¼å…¼å®¹æ€§æ›´å¥½ï¼Œæ¨èä½¿ç”¨ã€‚"
                )
                self.logger.error(f".doc æ–‡ä»¶ä¸æ”¯æŒ: {doc_path}")
                raise ValueError(error_message)

            # åœºæ™¯1: ç”¨æˆ·æŒ‡å®šäº†å…·ä½“æ–¹æ³•
            if methods is not None:
                self.logger.info(f"ä½¿ç”¨æŒ‡å®šæ–¹æ³•: {methods}, fallback={fallback}")

                for method_name in methods:
                    if method_name not in method_map:
                        self.logger.warning(f"æœªçŸ¥æ–¹æ³•: {method_name}ï¼Œè·³è¿‡")
                        continue

                    self.logger.info(f"å°è¯•æ–¹æ³•: {method_name}")
                    result = method_map[method_name](doc_path)

                    # åˆ¤æ–­æˆåŠŸæ ‡å‡†
                    if result.get('success'):
                        # æ£€æŸ¥æ˜¯å¦è¯†åˆ«åˆ°è¶³å¤Ÿçš„ç« èŠ‚
                        chapters = result.get('chapters', [])
                        if len(chapters) >= 1:  # è‡³å°‘è¯†åˆ«åˆ°1ä¸ªç« èŠ‚
                            self.logger.info(f"æ–¹æ³• {method_name} æˆåŠŸï¼Œè¯†åˆ«åˆ° {len(chapters)} ä¸ªç« èŠ‚")
                            return result
                        else:
                            self.logger.warning(f"æ–¹æ³• {method_name} æœªè¯†åˆ«åˆ°ç« èŠ‚")

                    # å¦‚æœä¸å¯ç”¨fallbackï¼Œç›´æ¥è¿”å›ç»“æœï¼ˆå³ä½¿å¤±è´¥ï¼‰
                    if not fallback:
                        self.logger.info(f"fallback=Falseï¼Œç›´æ¥è¿”å› {method_name} çš„ç»“æœ")
                        return result

                    # å¦åˆ™ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ–¹æ³•
                    self.logger.warning(f"æ–¹æ³• {method_name} å¤±è´¥æˆ–æ•ˆæœä¸ä½³ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–¹æ³•")

                # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
                return {
                    "success": False,
                    "chapters": [],
                    "statistics": {},
                    "error": f"æ‰€æœ‰æŒ‡å®šæ–¹æ³•({methods})éƒ½å¤±è´¥",
                    "method": "none"
                }

            # åœºæ™¯2: é»˜è®¤æ™ºèƒ½ç­–ç•¥ï¼ˆä½¿ç”¨ parse_smart æ™ºèƒ½è¯†åˆ« + ç« èŠ‚åˆ†ç±»ï¼‰
            self.logger.info("ä½¿ç”¨æ™ºèƒ½ç­–ç•¥ï¼šparse_smartï¼ˆç²¾ç¡®/å¤§çº² â†’ å¼‚å¸¸æ£€æµ‹ â†’ LLMå›é€€ â†’ ç« èŠ‚åˆ†ç±»ï¼‰")

            # è°ƒç”¨æ™ºèƒ½è§£ææ–¹æ³•
            result = self.parse_smart(doc_path, classify_chapters=True)

            # parse_smart è¿”å›çš„ç»“æœéœ€è¦è½¬æ¢ä¸º parse_document_structure çš„æ ‡å‡†æ ¼å¼
            return {
                "success": result.get('success', False),
                "chapters": result.get('chapters', []),
                "statistics": result.get('statistics', {}),
                "method": result.get('method_used', 'smart'),
                "fallback_from": result.get('fallback_from'),
                "fallback_reason": result.get('fallback_reason'),
                "key_sections": result.get('key_sections', {})
            }

        except Exception as e:
            self.logger.error(f"æ–‡æ¡£ç»“æ„è§£æå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "chapters": [],
                "statistics": {},
                "error": str(e),
                "method": "error"
            }

    # ============================================
    # ç‹¬ç«‹è§£ææ–¹æ³• - å¯å•ç‹¬è°ƒç”¨æˆ–ç»„åˆä½¿ç”¨
    # ============================================

    def parse_by_toc_exact(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•0: ç²¾ç¡®åŒ¹é…(åŸºäºç›®å½•)

        ç›´æ¥ä½¿ç”¨ç›®å½•é¡¹ç²¾ç¡®åŒ¹é…æ­£æ–‡ä½ç½®,é€Ÿåº¦å¿«ã€å‡†ç¡®ç‡é«˜

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "toc_exact"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            # æ£€æµ‹ç›®å½•
            toc_idx = self._find_toc_section(doc)
            if toc_idx is None:
                return {
                    "success": False,
                    "error": "æœªæ£€æµ‹åˆ°ç›®å½•,æ— æ³•ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ–¹æ³•",
                    "chapters": [],
                    "statistics": {},
                    "method": "toc_exact"
                }

            toc_items, toc_end_idx = self._parse_toc_items(doc, toc_idx)
            if not toc_items:
                return {
                    "success": False,
                    "error": "ç›®å½•è§£æå¤±è´¥",
                    "chapters": [],
                    "statistics": {},
                    "method": "toc_exact"
                }

            # ä½¿ç”¨ç²¾ç¡®åŒ¹é…
            chapters = self._locate_chapters_by_toc(doc, toc_items, toc_end_idx)
            chapter_tree = self._build_chapter_tree(chapters)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "toc_exact"
            }

        except Exception as e:
            self.logger.error(f"ç²¾ç¡®åŒ¹é…è§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "toc_exact"
            }

    def parse_structure_quick(self, doc_path: str) -> Dict:
        """
        å¿«é€Ÿè§£ææ–‡æ¡£ç»“æ„ï¼ˆé˜¶æ®µ1ï¼‰- åªæå–ç›®å½•é¡¹å¹¶ä½¿ç”¨LLMåˆ†æå±‚çº§

        ä¸åšæ­£æ–‡å®šä½å’Œå­—æ•°ç»Ÿè®¡ï¼Œè¿™äº›åœ¨ enrich_chapters é˜¶æ®µå®Œæˆã€‚

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],  # ç« èŠ‚æ ‘ï¼ˆæ— å­—æ•°ä¿¡æ¯ï¼‰
                "toc_end_idx": int,
                "method": "llm_quick"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            # 1. æ£€æµ‹ç›®å½•ä½ç½®
            toc_idx = self._find_toc_section(doc)
            if toc_idx is None:
                # æ— ç›®å½•ï¼Œå°è¯•ä»æ–‡æ¡£å¼€å¤´è¯†åˆ«ç« èŠ‚æ ‡é¢˜
                self.logger.info("æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œå°è¯•ä»æ–‡æ¡£ä¸­è¯†åˆ«ç« èŠ‚æ ‡é¢˜")
                toc_items = self._extract_chapter_titles_from_doc(doc)
                toc_end_idx = 0
            else:
                # æœ‰ç›®å½•ï¼Œæå–ç›®å½•é¡¹
                toc_items, toc_end_idx = self._parse_toc_items(doc, toc_idx)

            if not toc_items:
                return {
                    "success": False,
                    "error": "æœªèƒ½æå–åˆ°ç›®å½•é¡¹",
                    "chapters": [],
                    "toc_end_idx": 0,
                    "method": "llm_quick"
                }

            self.logger.info(f"æå–åˆ° {len(toc_items)} ä¸ªç›®å½•é¡¹ï¼Œå¼€å§‹LLMå±‚çº§åˆ†æ")

            # 2. ä½¿ç”¨LLMåˆ†æå±‚çº§
            from modules.tender_processing.level_analyzer import LevelAnalyzer
            analyzer = LevelAnalyzer()
            levels = analyzer.analyze_toc_hierarchy_with_llm(toc_items)

            # 3. æ„å»ºç« èŠ‚èŠ‚ç‚¹ï¼ˆä¸å«æ­£æ–‡å®šä½ä¿¡æ¯ï¼‰
            chapters = []
            for i, item in enumerate(toc_items):
                level = levels[i] if i < len(levels) else 1
                chapter = ChapterNode(
                    id=f"ch_{i}",
                    level=level,
                    title=item.get('title', ''),
                    para_start_idx=-1,  # æœªå®šä½
                    para_end_idx=-1,    # æœªå®šä½
                    word_count=0,       # æœªç»Ÿè®¡
                    preview_text=""     # æœªæå–
                )
                chapters.append(chapter)

            # 4. æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self._build_chapter_tree(chapters)

            # 5. ç« èŠ‚åˆ†ç±»
            classified_chapters, _ = self._classify_chapters(
                [ch.to_dict() for ch in chapter_tree]
            )

            return {
                "success": True,
                "chapters": classified_chapters,
                "toc_end_idx": toc_end_idx,
                "method": "llm_quick"
            }

        except Exception as e:
            self.logger.error(f"å¿«é€Ÿè§£æå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "toc_end_idx": 0,
                "method": "llm_quick"
            }

    def enrich_chapters(self, doc_path: str, chapters: List[Dict], toc_end_idx: int = 0) -> Dict:
        """
        è¡¥å……ç« èŠ‚ä¿¡æ¯ï¼ˆé˜¶æ®µ2ï¼‰- æ‰§è¡Œæ­£æ–‡å®šä½å’Œå­—æ•°ç»Ÿè®¡

        æ ¹æ®é˜¶æ®µ1è¯†åˆ«çš„ç« èŠ‚æ ‡é¢˜ï¼Œåœ¨æ–‡æ¡£æ­£æ–‡ä¸­å®šä½æ¯ä¸ªç« èŠ‚çš„ä½ç½®ï¼Œ
        å¹¶ç»Ÿè®¡å­—æ•°ã€æå–é¢„è§ˆæ–‡æœ¬ã€‚

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„
            chapters: é˜¶æ®µ1è¿”å›çš„ç« èŠ‚æ ‘ï¼ˆåµŒå¥—ç»“æ„ï¼‰
            toc_end_idx: ç›®å½•ç»“æŸçš„æ®µè½ç´¢å¼•

        Returns:
            {
                "success": True/False,
                "chapters": [...],  # è¡¥å……å®Œæ•´ä¿¡æ¯çš„ç« èŠ‚æ ‘
                "statistics": {
                    "total_words": int,
                    "chapter_count": int,
                    "estimated_processing_cost": float
                }
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))
            self.logger.info(f"[enrich_chapters] å¼€å§‹è¡¥å……ç« èŠ‚ä¿¡æ¯ï¼Œå…± {len(chapters)} ä¸ªæ ¹ç« èŠ‚")

            # 1. å°†åµŒå¥—çš„ç« èŠ‚æ ‘å±•å¹³ä¸ºåˆ—è¡¨ï¼Œæ–¹ä¾¿å¤„ç†
            flat_chapters = self._flatten_chapters(chapters)
            self.logger.info(f"[enrich_chapters] å±•å¹³åå…± {len(flat_chapters)} ä¸ªç« èŠ‚")

            # 2. å®šä½æ¯ä¸ªç« èŠ‚åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
            last_found_idx = toc_end_idx + 1
            located_chapters = []

            for order_idx, chapter in enumerate(flat_chapters):
                title = chapter.get('title', '')
                level = chapter.get('level', 1)
                chapter_id = chapter.get('id', '')

                # åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾æ ‡é¢˜ä½ç½®
                para_idx = self._find_paragraph_by_title_simple(doc, title, last_found_idx)

                if para_idx is None:
                    self.logger.warning(f"âš ï¸  æœªæ‰¾åˆ°ç« èŠ‚: [{level}çº§] {title}")
                    # ä»ç„¶æ·»åŠ åˆ°åˆ—è¡¨ï¼Œä½†æ ‡è®°ä¸ºæœªå®šä½
                    located_chapters.append({
                        'id': chapter_id,
                        'title': title,
                        'level': level,
                        'para_idx': -1,
                        'para_end_idx': -1,
                        'located': False,
                        'order_index': order_idx,  # ä¿æŒåŸå§‹ç›®å½•é¡ºåº
                        'original': chapter
                    })
                    continue

                located_chapters.append({
                    'id': chapter_id,
                    'title': title,
                    'level': level,
                    'para_idx': para_idx,
                    'para_end_idx': None,  # ç¨åè®¡ç®—
                    'located': True,
                    'order_index': order_idx,  # ä¿æŒåŸå§‹ç›®å½•é¡ºåº
                    'original': chapter
                })

                last_found_idx = para_idx + 1
                self.logger.debug(f"  âœ“ æ‰¾åˆ° [{level}çº§] {title} (æ®µè½ {para_idx})")

            # 3. è®¡ç®—æ¯ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®
            for i, chapter_info in enumerate(located_chapters):
                if not chapter_info['located']:
                    continue

                current_level = chapter_info['level']
                next_start = len(doc.paragraphs)

                # æ‰¾ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§çš„ç« èŠ‚
                for j in range(i + 1, len(located_chapters)):
                    if located_chapters[j]['located'] and located_chapters[j]['level'] <= current_level:
                        next_start = located_chapters[j]['para_idx']
                        break

                chapter_info['para_end_idx'] = next_start - 1

            # 4. è®¡ç®—å­—æ•°å’Œæå–é¢„è§ˆæ–‡æœ¬
            total_words = 0
            for chapter_info in located_chapters:
                if not chapter_info['located']:
                    chapter_info['word_count'] = 0
                    chapter_info['preview_text'] = "(æœªå®šä½)"
                    chapter_info['has_table'] = False
                    continue

                para_idx = chapter_info['para_idx']
                para_end_idx = chapter_info['para_end_idx']

                # æå–ç« èŠ‚å†…å®¹
                content_text, preview_text, has_table = self._extract_chapter_content_with_tables(
                    doc, para_idx, para_end_idx
                )

                word_count = self._calculate_word_count(content_text)

                chapter_info['word_count'] = word_count
                chapter_info['preview_text'] = preview_text if preview_text else "(æ— å†…å®¹)"
                chapter_info['has_table'] = has_table

                total_words += word_count

            # 5. æ›´æ–°åŸå§‹ç« èŠ‚æ ‘çš„ä¿¡æ¯
            enriched_chapters = self._update_chapter_tree(chapters, located_chapters)

            # 6. ç»Ÿè®¡ä¿¡æ¯
            statistics = {
                "total_words": total_words,
                "chapter_count": len(flat_chapters),
                "located_count": sum(1 for c in located_chapters if c['located']),
                "estimated_processing_cost": round(total_words * 0.000002, 4)  # å‡è®¾æ¯å­—0.000002å…ƒ
            }

            self.logger.info(
                f"âœ… [enrich_chapters] å®Œæˆ: å®šä½ {statistics['located_count']}/{statistics['chapter_count']} ä¸ªç« èŠ‚, "
                f"æ€»å­—æ•° {total_words}"
            )

            return {
                "success": True,
                "chapters": enriched_chapters,
                "statistics": statistics
            }

        except Exception as e:
            self.logger.error(f"è¡¥å……ç« èŠ‚ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e),
                "chapters": chapters,
                "statistics": {}
            }
    def _find_paragraph_by_title_simple(self, doc: Document, title: str, start_idx: int) -> int:
        """
        ç®€åŒ–ç‰ˆæ ‡é¢˜å®šä½ï¼šåœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾ä¸æ ‡é¢˜åŒ¹é…çš„æ®µè½

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            title: è¦æŸ¥æ‰¾çš„æ ‡é¢˜
            start_idx: å¼€å§‹æœç´¢çš„æ®µè½ç´¢å¼•

        Returns:
            æ®µè½ç´¢å¼•ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        import re
        from difflib import SequenceMatcher

        # æ ‡å‡†åŒ–æ ‡é¢˜ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦
        def normalize(text: str) -> str:
            text = text.strip()
            text = re.sub(r'\s+', ' ', text)  # å¤šä¸ªç©ºç™½å­—ç¬¦åˆå¹¶ä¸ºä¸€ä¸ªç©ºæ ¼
            text = re.sub(r'\t+', ' ', text)  # åˆ¶è¡¨ç¬¦è½¬ç©ºæ ¼
            return text

        title_clean = normalize(title)
        if not title_clean:
            return None

        for i in range(start_idx, len(doc.paragraphs)):
            para_text = normalize(doc.paragraphs[i].text)
            if not para_text:
                continue

            # å®Œå…¨åŒ¹é…
            if para_text == title_clean:
                return i

            # æ¨¡ç³ŠåŒ¹é…ï¼ˆç›¸ä¼¼åº¦ > 0.85ï¼‰
            ratio = SequenceMatcher(None, para_text, title_clean).ratio()
            if ratio > 0.85:
                return i

            # åŒ…å«åŒ¹é…ï¼ˆæ®µè½ä»¥æ ‡é¢˜å¼€å¤´æˆ–æ ‡é¢˜ä»¥æ®µè½å¼€å¤´ï¼‰
            if para_text.startswith(title_clean) or title_clean.startswith(para_text):
                if len(para_text) < 150:  # é¿å…åŒ¹é…åˆ°é•¿æ®µè½
                    return i

            # å»é™¤ç¼–å·ååŒ¹é…ï¼ˆå¤„ç†"ç¬¬ä¸‰ç«   ç”¨æˆ·éœ€æ±‚ä¹¦" vs "ç¬¬ä¸‰ç«  ç”¨æˆ·éœ€æ±‚ä¹¦"ï¼‰
            title_no_space = title_clean.replace(' ', '')
            para_no_space = para_text.replace(' ', '')
            if para_no_space == title_no_space:
                return i

        return None

    def _update_chapter_tree(self, chapters: List[Dict], located_chapters: List[Dict]) -> List[Dict]:
        """
        æ›´æ–°ç« èŠ‚æ ‘ï¼Œå°†å®šä½ä¿¡æ¯åˆå¹¶å›åµŒå¥—ç»“æ„

        Args:
            chapters: åŸå§‹åµŒå¥—ç« èŠ‚æ ‘
            located_chapters: å±•å¹³åå¸¦å®šä½ä¿¡æ¯çš„ç« èŠ‚åˆ—è¡¨

        Returns:
            æ›´æ–°åçš„åµŒå¥—ç« èŠ‚æ ‘
        """
        # å»ºç«‹IDåˆ°å®šä½ä¿¡æ¯çš„æ˜ å°„
        location_map = {c['id']: c for c in located_chapters}

        def update_recursive(chapter_list: List[Dict]) -> List[Dict]:
            result = []
            for chapter in chapter_list:
                chapter_id = chapter.get('id', '')
                updated = dict(chapter)

                if chapter_id in location_map:
                    loc_info = location_map[chapter_id]
                    updated['para_start_idx'] = loc_info.get('para_idx', -1)
                    updated['para_end_idx'] = loc_info.get('para_end_idx', -1)
                    updated['word_count'] = loc_info.get('word_count', 0)
                    updated['preview_text'] = loc_info.get('preview_text', '')
                    updated['has_table'] = loc_info.get('has_table', False)
                    updated['order_index'] = loc_info.get('order_index', 0)  # ä¿æŒåŸå§‹ç›®å½•é¡ºåº

                # é€’å½’å¤„ç†å­ç« èŠ‚
                if 'children' in updated and updated['children']:
                    updated['children'] = update_recursive(updated['children'])

                result.append(updated)
            return result

        return update_recursive(chapters)

    def _extract_chapter_titles_from_doc(self, doc: Document) -> List[Dict]:
        """
        ä»æ–‡æ¡£ä¸­æå–å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜ï¼ˆç”¨äºæ— ç›®å½•çš„æƒ…å†µï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡

        Returns:
            ç›®å½•é¡¹åˆ—è¡¨æ ¼å¼ï¼š[{'title': '...', 'page_num': 0}, ...]
        """
        from modules.tender_processing.level_analyzer import LevelAnalyzer
        analyzer = LevelAnalyzer()

        titles = []
        for i, para in enumerate(doc.paragraphs[:500]):  # åªæ‰«æå‰500æ®µ
            text = para.text.strip()
            if not text or len(text) > 100:
                continue

            # æ£€æµ‹æ˜¯å¦æœ‰ç¼–å·æ¨¡å¼
            pattern_info = analyzer.extract_numbering_pattern(text)
            if pattern_info.get('type') != 'none':
                titles.append({
                    'title': text,
                    'page_num': 0,
                    'index': i
                })

        self.logger.info(f"ä»æ–‡æ¡£ä¸­æå–åˆ° {len(titles)} ä¸ªæ½œåœ¨ç« èŠ‚æ ‡é¢˜")
        return titles

    def parse_by_outline_level(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•5: Wordå¤§çº²çº§åˆ«è¯†åˆ«

        ä½¿ç”¨Wordæ–‡æ¡£å¤§çº²çº§åˆ«(Outline Level)è¯†åˆ«ç« èŠ‚

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "outline_level"
            }
        """
        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))

            # 1. åŸºäºWordå¤§çº²çº§åˆ«è¯†åˆ«ç« èŠ‚
            chapters = self._parse_chapters_by_outline_level(doc)

            # 2. ğŸ†• ä½¿ç”¨æ™ºèƒ½å±‚çº§åˆ†æå™¨ä¿®æ­£å±‚çº§(ä¸ç²¾ç¡®è¯†åˆ«ä¿æŒä¸€è‡´)
            if chapters:
                from modules.tender_processing.level_analyzer import LevelAnalyzer

                # è½¬æ¢ä¸ºç±»ä¼¼ç›®å½•é¡¹çš„æ ¼å¼
                toc_like_items = [
                    {'title': ch.title, 'level': ch.level}
                    for ch in chapters
                ]

                # ä½¿ç”¨contextualæ–¹æ³•æ™ºèƒ½åˆ†æå±‚çº§
                analyzer = LevelAnalyzer()
                corrected_levels = analyzer.analyze_toc_hierarchy_contextual(toc_like_items)

                # æ›´æ–°ç« èŠ‚å±‚çº§
                for i, level in enumerate(corrected_levels):
                    chapters[i].level = level

                # ç»Ÿè®¡æ—¥å¿—
                from collections import Counter
                level_dist = Counter(corrected_levels)
                self.logger.info(f"âœ… æ™ºèƒ½å±‚çº§åˆ†æå®Œæˆ,ä¿®æ­£ {len(chapters)} ä¸ªç« èŠ‚: {dict(level_dist)}")

            # 3. åç»­å¤„ç†(å®šä½å†…å®¹ã€æ„å»ºæ ‘ã€ç»Ÿè®¡)
            chapters = self._locate_chapter_content(doc, chapters)
            chapter_tree = self._build_chapter_tree(chapters)
            stats = self._calculate_statistics(chapter_tree)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": stats,
                "method": "outline_level"
            }

        except Exception as e:
            self.logger.error(f"å¤§çº²çº§åˆ«è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "outline_level"
            }

    def parse_by_azure(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•4: Azure Form Recognizerè§£æ

        ä½¿ç”¨Azure AIæœåŠ¡è¯†åˆ«æ–‡æ¡£ç»“æ„

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "azure"
            }
        """
        try:
            # æ£€æŸ¥Azureè§£æå™¨æ˜¯å¦å¯ç”¨
            try:
                from modules.tender_processing.azure_parser import AzureDocumentParser, is_azure_available

                if not is_azure_available():
                    return {
                        "success": False,
                        "error": "Azure Form Recognizeræœªé…ç½®æˆ–SDKæœªå®‰è£…",
                        "chapters": [],
                        "statistics": {},
                        "method": "azure"
                    }

                doc_path_abs = resolve_file_path(doc_path)
                if not doc_path_abs:
                    raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

                # ä½¿ç”¨Azureè§£æå™¨
                azure_parser = AzureDocumentParser()
                result = azure_parser.parse_document_structure(str(doc_path_abs))
                return result

            except ImportError as e:
                return {
                    "success": False,
                    "error": f"Azureè§£æå™¨ä¸å¯ç”¨: {str(e)}",
                    "chapters": [],
                    "statistics": {},
                    "method": "azure"
                }

        except Exception as e:
            self.logger.error(f"Azureè§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "azure"
            }

    def parse_by_gemini(self, doc_path: str) -> Dict:
        """
        æ–¹æ³•6: Gemini AIè§£æ

        ä½¿ç”¨Google Gemini AIæ¨¡å‹è¯†åˆ«æ–‡æ¡£ç»“æ„

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "gemini"
            }
        """
        try:
            # æ£€æŸ¥Geminiè§£æå™¨æ˜¯å¦å¯ç”¨
            try:
                from modules.tender_processing.parsers.gemini_parser import GeminiParser

                gemini_parser = GeminiParser()

                if not gemini_parser.is_available():
                    return {
                        "success": False,
                        "error": "Gemini APIå¯†é’¥æœªé…ç½®",
                        "chapters": [],
                        "statistics": {},
                        "method": "gemini"
                    }

                doc_path_abs = resolve_file_path(doc_path)
                if not doc_path_abs:
                    raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

                # ä½¿ç”¨Geminiè§£æå™¨
                result = gemini_parser.parse_structure(str(doc_path_abs))

                # ç¡®ä¿chaptersæ˜¯dictæ ¼å¼
                if result.get('success') and result.get('chapters'):
                    chapters = result['chapters']
                    if chapters and hasattr(chapters[0], 'to_dict'):
                        result['chapters'] = [ch.to_dict() if hasattr(ch, 'to_dict') else ch for ch in chapters]

                return result

            except ImportError as e:
                return {
                    "success": False,
                    "error": f"Geminiè§£æå™¨ä¸å¯ç”¨: {str(e)} (pip install google-generativeai)",
                    "chapters": [],
                    "statistics": {},
                    "method": "gemini"
                }

        except Exception as e:
            self.logger.error(f"Geminiè§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "gemini"
            }

    def parse_smart(self, doc_path: str, classify_chapters: bool = True) -> Dict:
        """
        æ™ºèƒ½è§£æï¼šç»“æ„è¯†åˆ« + ç±»å‹åˆ†ç±»

        æµç¨‹ï¼š
        1. æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æœ‰ç›®å½•
           - æœ‰ç›®å½• â†’ ç²¾ç¡®åŒ¹é…(toc_exact)
           - æ— ç›®å½• â†’ Wordå¤§çº²è¯†åˆ«(docx_native)
        2. æ£€æŸ¥ç»“æœæ˜¯å¦å¼‚å¸¸ï¼ˆç« èŠ‚å¤ªå°‘ã€ç¼–å·è·³è·ƒç­‰ï¼‰
           - æ­£å¸¸ â†’ è¿›è¡Œç« èŠ‚åˆ†ç±»
           - å¼‚å¸¸ â†’ å›é€€åˆ°LLMå±‚çº§åˆ†æ
        3. å¯¹ç« èŠ‚è¿›è¡Œç±»å‹åˆ†ç±»ï¼ˆå¯é€‰ï¼‰

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„
            classify_chapters: æ˜¯å¦å¯¹ç« èŠ‚è¿›è¡Œç±»å‹åˆ†ç±»

        Returns:
            {
                "success": True/False,
                "chapters": [...],
                "statistics": {...},
                "method": "smart",
                "primary_method": "toc_exact|docx_native|llm_level",
                "fallback_from": None|"toc_exact"|"docx_native",
                "fallback_reason": None|str,
                "key_sections": {
                    "business_response": [...],
                    "technical_spec": [...],
                    "contract_content": [...]
                }
            }
        """
        import time
        start_time = time.time()

        try:
            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))
            doc_paragraph_count = len(doc.paragraphs)

            # ========================================
            # é˜¶æ®µ1: ç« èŠ‚ç»“æ„è¯†åˆ«ï¼ˆæ™ºèƒ½å›é€€ï¼‰
            # ========================================
            fallback_from = None
            fallback_reason = None
            primary_method = None

            # æ£€æµ‹æ˜¯å¦æœ‰ç›®å½•
            toc_idx = self._find_toc_section(doc)
            has_toc = toc_idx is not None

            if has_toc:
                # æœ‰ç›®å½•ï¼šä½¿ç”¨ç²¾ç¡®åŒ¹é…
                self.logger.info("ğŸ“Œ æ™ºèƒ½è§£æ: æ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨ç²¾ç¡®åŒ¹é…")
                result = self.parse_by_toc_exact(doc_path)
                primary_method = "toc_exact"
            else:
                # æ— ç›®å½•ï¼šä½¿ç”¨å¤§çº²è¯†åˆ«
                self.logger.info("ğŸ“Œ æ™ºèƒ½è§£æ: æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œä½¿ç”¨å¤§çº²è¯†åˆ«")
                result = self.parse_by_outline_level(doc_path)
                primary_method = "docx_native"

            # æ£€æŸ¥ç»“æœæ˜¯å¦å¼‚å¸¸
            if result.get('success') and result.get('chapters'):
                chapters = result['chapters']
                is_suspicious, reason = self._is_result_suspicious(chapters, doc_paragraph_count)

                if is_suspicious:
                    self.logger.warning(f"âš ï¸ {primary_method} ç»“æœå¼‚å¸¸: {reason}ï¼Œå›é€€åˆ°LLMå±‚çº§åˆ†æ")
                    fallback_from = primary_method
                    fallback_reason = reason

                    # å›é€€åˆ°LLMå±‚çº§åˆ†æ
                    llm_result = self._parse_by_llm_level(doc_path)
                    if llm_result.get('success') and llm_result.get('chapters'):
                        result = llm_result
                        primary_method = "llm_level"
                    else:
                        self.logger.warning("LLMå±‚çº§åˆ†æä¹Ÿå¤±è´¥ï¼Œä¿ç•™åŸç»“æœ")
            elif not result.get('success'):
                # ä¸»æ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•
                self.logger.warning(f"âš ï¸ {primary_method} å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•")
                fallback_from = primary_method

                if has_toc:
                    # ç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å¤§çº²è¯†åˆ«
                    result = self.parse_by_outline_level(doc_path)
                    primary_method = "docx_native"
                else:
                    # å¤§çº²è¯†åˆ«å¤±è´¥ï¼Œå°è¯•LLM
                    result = self._parse_by_llm_level(doc_path)
                    primary_method = "llm_level"

                if not result.get('success'):
                    fallback_reason = "æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥"

            # ========================================
            # é˜¶æ®µ2: ç« èŠ‚ç±»å‹åˆ†ç±»
            # ========================================
            key_sections = {}
            if classify_chapters and result.get('success') and result.get('chapters'):
                try:
                    classified_chapters, key_sections = self._classify_chapters(result['chapters'])
                    result['chapters'] = classified_chapters
                except Exception as e:
                    self.logger.warning(f"ç« èŠ‚åˆ†ç±»å¤±è´¥: {e}")

            # æ„å»ºè¿”å›ç»“æœ
            elapsed = time.time() - start_time
            return {
                "success": result.get('success', False),
                "chapters": result.get('chapters', []),
                "statistics": result.get('statistics', {}),
                "method": "smart",
                "primary_method": primary_method,
                "fallback_from": fallback_from,
                "fallback_reason": fallback_reason,
                "key_sections": key_sections,
                "performance": {
                    "elapsed": elapsed,
                    "elapsed_formatted": f"{elapsed:.2f}s"
                }
            }

        except Exception as e:
            self.logger.error(f"æ™ºèƒ½è§£æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "statistics": {},
                "method": "smart"
            }

    def _is_result_suspicious(self, chapters: List[Dict], doc_paragraph_count: int = 0) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥è¯†åˆ«ç»“æœæ˜¯å¦å¼‚å¸¸

        Args:
            chapters: ç« èŠ‚åˆ—è¡¨
            doc_paragraph_count: æ–‡æ¡£æ®µè½æ€»æ•°

        Returns:
            (æ˜¯å¦å¼‚å¸¸, å¼‚å¸¸åŸå› )
        """
        # æ‰å¹³åŒ–ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…æ‹¬å­ç« èŠ‚ï¼‰
        def flatten(chs):
            result = []
            for ch in chs:
                result.append(ch)
                if ch.get('children'):
                    result.extend(flatten(ch['children']))
            return result

        all_chapters = flatten(chapters) if chapters else []
        level1_chapters = [c for c in all_chapters if c.get('level') == 1]
        level1_count = len(level1_chapters)

        # è§„åˆ™1: ä¸€çº§ç« èŠ‚å¤ªå°‘ï¼ˆæ‹›æ ‡æ–‡ä»¶é€šå¸¸æœ‰5-10ä¸ªä¸€çº§ç« èŠ‚ï¼‰
        if level1_count < 3:
            return True, f"ä¸€çº§ç« èŠ‚å¤ªå°‘ï¼ˆä»…{level1_count}ä¸ªï¼‰"

        # è§„åˆ™2: ç« èŠ‚ç¼–å·æœ‰è·³è·ƒï¼ˆå¦‚åªæœ‰ç¬¬ä¸€ç« å’Œç¬¬äº”ç« ï¼‰
        has_gap, gap_info = self._has_chapter_number_gap(level1_chapters)
        if has_gap:
            return True, f"ç« èŠ‚ç¼–å·æœ‰è·³è·ƒ: {gap_info}"

        # è§„åˆ™3: æ–‡æ¡£å¾ˆé•¿ä½†ç« èŠ‚å¾ˆå°‘ï¼ˆæ¯ç« å¹³å‡è¶…è¿‡100ä¸ªæ®µè½ä¸å¤ªåˆç†ï¼‰
        if doc_paragraph_count > 0 and level1_count > 0:
            avg_paras = doc_paragraph_count / level1_count
            if avg_paras > 150:
                return True, f"æ¯ç« å¹³å‡æ®µè½æ•°è¿‡å¤šï¼ˆ{avg_paras:.0f}æ®µ/ç« ï¼‰"

        return False, None

    def _has_chapter_number_gap(self, level1_chapters: List[Dict]) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥ç« èŠ‚ç¼–å·æ˜¯å¦æœ‰è·³è·ƒ

        Args:
            level1_chapters: ä¸€çº§ç« èŠ‚åˆ—è¡¨

        Returns:
            (æ˜¯å¦æœ‰è·³è·ƒ, è·³è·ƒä¿¡æ¯)
        """
        # ä¸­æ–‡æ•°å­—è½¬æ¢
        chinese_nums = {
            'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
            'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
            'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15
        }

        def chinese_to_number(s):
            if s.isdigit():
                return int(s)
            return chinese_nums.get(s, None)

        # æå–ç« èŠ‚ç¼–å·
        numbers = []
        for ch in level1_chapters:
            title = ch.get('title', '')
            # åŒ¹é… "ç¬¬ä¸€ç« "ã€"ç¬¬1ç« "ã€"ç¬¬ä¸€éƒ¨åˆ†" ç­‰æ ¼å¼
            match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)[ç« éƒ¨åˆ†ç¯‡]', title)
            if match:
                num = chinese_to_number(match.group(1))
                if num:
                    numbers.append(num)

        # æ£€æŸ¥è¿ç»­æ€§
        if len(numbers) >= 2:
            numbers.sort()
            for i in range(1, len(numbers)):
                if numbers[i] - numbers[i-1] > 1:
                    return True, f"ç¬¬{numbers[i-1]}â†’ç¬¬{numbers[i]}"

        return False, None

    def _parse_by_llm_level(self, doc_path: str) -> Dict:
        """
        ä½¿ç”¨LLMå±‚çº§åˆ†æè§£ææ–‡æ¡£ç»“æ„

        Args:
            doc_path: Wordæ–‡æ¡£è·¯å¾„

        Returns:
            è§£æç»“æœå­—å…¸
        """
        try:
            from modules.tender_processing.level_analyzer import LevelAnalyzer

            doc_path_abs = resolve_file_path(doc_path)
            if not doc_path_abs:
                raise FileNotFoundError(f"æ— æ³•è§£ææ–‡ä»¶è·¯å¾„: {doc_path}")

            doc = Document(str(doc_path_abs))
            analyzer = LevelAnalyzer()

            # æå–æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜
            potential_titles = []
            for i, para in enumerate(doc.paragraphs):
                text = para.text.strip()
                if not text or len(text) > 100:
                    continue

                # æ£€æµ‹æ˜¯å¦æœ‰ç¼–å·æ¨¡å¼
                pattern_info = analyzer.extract_numbering_pattern(text)
                if pattern_info.get('type') != 'none':
                    potential_titles.append({
                        'index': i,
                        'title': text,
                        'pattern': pattern_info
                    })

            if not potential_titles:
                return {
                    "success": False,
                    "error": "æœªæ£€æµ‹åˆ°ç« èŠ‚æ ‡é¢˜",
                    "chapters": [],
                    "method": "llm_level"
                }

            # ä½¿ç”¨LLMåˆ†æå±‚çº§
            toc_items = [{'title': t['title'], 'index': t['index']} for t in potential_titles]
            analyzed = analyzer.analyze_toc_hierarchy(toc_items)

            # æ„å»ºç« èŠ‚èŠ‚ç‚¹
            chapters = []
            for item in analyzed:
                chapter = ChapterNode(
                    id=f"ch_{item.get('index', len(chapters))}",
                    level=item.get('level', 1),
                    title=item.get('title', ''),
                    para_start_idx=item.get('index', 0),
                    para_end_idx=item.get('index', 0),
                    word_count=0,
                    preview_text=""
                )
                chapters.append(chapter)

            # æ„å»ºæ ‘å½¢ç»“æ„
            chapter_tree = self._build_chapter_tree(chapters)

            return {
                "success": True,
                "chapters": [ch.to_dict() for ch in chapter_tree],
                "statistics": self._calculate_statistics(chapter_tree),
                "method": "llm_level"
            }

        except Exception as e:
            self.logger.error(f"LLMå±‚çº§åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "chapters": [],
                "method": "llm_level"
            }

    def _classify_chapters(self, chapters: List[Dict]) -> Tuple[List[Dict], Dict]:
        """
        å¯¹ç« èŠ‚è¿›è¡Œç±»å‹åˆ†ç±»

        ç« èŠ‚ç±»å‹ï¼š
        - invitation: æŠ•æ ‡é‚€è¯·ä¹¦
        - bidder_notice: æŠ•æ ‡äººé¡»çŸ¥
        - evaluation: è¯„æ ‡åŠæ³•/è¯„åˆ†æ ‡å‡†
        - contract_terms: åˆåŒæ¡æ¬¾
        - contract_content: åˆåŒæ­£æ–‡ï¼ˆå®é™…éœ€å¡«å†™çš„åˆåŒï¼‰
        - business_response: å•†åŠ¡åº”ç­”æ¨¡æ¿
        - technical_spec: æŠ€æœ¯è§„èŒƒä¹¦/æŠ€æœ¯éœ€æ±‚
        - appendix: é™„ä»¶/é™„å½•
        - other: å…¶ä»–

        Args:
            chapters: ç« èŠ‚åˆ—è¡¨

        Returns:
            (å¸¦ç±»å‹æ ‡æ³¨çš„ç« èŠ‚åˆ—è¡¨, é‡ç‚¹åŒºåŸŸæ±‡æ€»)
        """
        # åˆ†ç±»è§„åˆ™ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
        classification_rules = {
            'invitation': [
                r'æŠ•æ ‡é‚€è¯·', r'æ‹›æ ‡å…¬å‘Š', r'é‚€è¯·ä¹¦', r'è¯¢ä»·å…¬å‘Š', r'æ¯”é€‰å…¬å‘Š'
            ],
            'bidder_notice': [
                r'æŠ•æ ‡äººé¡»çŸ¥', r'åº”ç­”äººé¡»çŸ¥', r'ä¾›åº”å•†é¡»çŸ¥', r'ç£‹å•†é¡»çŸ¥', r'å“åº”äººé¡»çŸ¥'
            ],
            'evaluation': [
                r'è¯„æ ‡åŠæ³•', r'è¯„åˆ†æ ‡å‡†', r'è¯„å®¡åŠæ³•', r'è¯„å®¡æ ‡å‡†', r'è¯„åˆ†ç»†åˆ™', r'è¯„æ ‡æ ‡å‡†'
            ],
            'contract_terms': [
                r'åˆåŒæ¡æ¬¾', r'åˆåŒæ¡ä»¶', r'åè®®æ¡æ¬¾', r'åˆåŒæ ¼å¼'
            ],
            'contract_content': [
                r'æœåŠ¡åˆåŒ', r'é‡‡è´­åˆåŒ', r'åˆåŒæ­£æ–‡', r'åˆåŒä¹¦', r'åˆåŒèŒƒæœ¬',
                r'è´§ç‰©ä¾›åº”åˆåŒ', r'æŠ€æœ¯æœåŠ¡åˆåŒ', r'æ¡†æ¶åˆåŒ'
            ],
            'business_response': [
                r'æŠ•æ ‡æ–‡ä»¶æ ¼å¼', r'å“åº”æ–‡ä»¶æ ¼å¼', r'å•†åŠ¡éƒ¨åˆ†', r'èµ„æ ¼è¯æ˜', r'æŠ¥ä»·è¦æ±‚',
                r'æŠ•æ ‡å‡½', r'æ³•å®šä»£è¡¨äºº', r'æˆæƒå§”æ‰˜ä¹¦', r'æŠ•æ ‡ä¿è¯é‡‘',
                r'ä¸šç»©è¯æ˜', r'è´¢åŠ¡æŠ¥è¡¨', r'èµ„è´¨è¯æ˜'
            ],
            'technical_spec': [
                r'æŠ€æœ¯è§„èŒƒ', r'æŠ€æœ¯è¦æ±‚', r'æŠ€æœ¯éœ€æ±‚', r'æœåŠ¡è¦æ±‚', r'åŠŸèƒ½éœ€æ±‚', r'æŠ€æœ¯å‚æ•°',
                r'æŠ€æœ¯æ–¹æ¡ˆ', r'æœåŠ¡è§„èŒƒ', r'æŠ€æœ¯æœåŠ¡', r'æŠ€æœ¯ï¼ˆæœåŠ¡ï¼‰',
                r'éœ€æ±‚è¯´æ˜', r'äº§å“è§„æ ¼'
            ],
            'appendix': [
                r'^é™„ä»¶', r'^é™„å½•', r'^é™„è¡¨', r'^é™„å›¾'
            ]
        }

        def classify_single(title: str) -> str:
            """å¯¹å•ä¸ªæ ‡é¢˜è¿›è¡Œåˆ†ç±»"""
            title_clean = title.strip()
            for category, patterns in classification_rules.items():
                for pattern in patterns:
                    if re.search(pattern, title_clean, re.IGNORECASE):
                        return category
            return 'other'

        def classify_recursive(chs: List[Dict]) -> List[Dict]:
            """é€’å½’åˆ†ç±»ç« èŠ‚åŠå…¶å­ç« èŠ‚"""
            result = []
            for ch in chs:
                ch_copy = ch.copy()
                ch_copy['chapter_type'] = classify_single(ch.get('title', ''))

                if ch.get('children'):
                    ch_copy['children'] = classify_recursive(ch['children'])

                result.append(ch_copy)
            return result

        # æ‰§è¡Œåˆ†ç±»
        classified = classify_recursive(chapters)

        # æ±‡æ€»é‡ç‚¹åŒºåŸŸ
        key_sections = {
            'business_response': [],
            'technical_spec': [],
            'contract_content': []
        }

        def collect_key_sections(chs: List[Dict]):
            for ch in chs:
                ch_type = ch.get('chapter_type')
                if ch_type in key_sections:
                    key_sections[ch_type].append(ch.get('title', ''))
                if ch.get('children'):
                    collect_key_sections(ch['children'])

        collect_key_sections(classified)

        return classified, key_sections
    def _is_valid_chapter_title(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯åˆæ³•çš„ç« èŠ‚æ ‡é¢˜ (è¿‡æ»¤åˆ—è¡¨é¡¹ã€è¯´æ˜æ€§å†…å®¹ç­‰)

        Args:
            text: æ®µè½æ–‡æœ¬

        Returns:
            True è¡¨ç¤ºå¯èƒ½æ˜¯ç« èŠ‚æ ‡é¢˜ï¼ŒFalse è¡¨ç¤ºåº”è¯¥è¢«è¿‡æ»¤
        """
        if not text or len(text.strip()) == 0:
            return False

        # 1. è¿‡æ»¤çº¯ç¼–å·ï¼ˆå¦‚ "1"ã€"1.1"ï¼‰
        if re.match(r'^[\d\.]+$', text):
            return False

        # 2. è¿‡æ»¤çº¯åˆ—è¡¨é¡¹æ ‡è®°ï¼ˆå¦‚ "1."ã€"2."ã€"(1)"ï¼‰
        if re.match(r'^[\d]+\.$', text):  # "1." "2."
            return False
        if re.match(r'^\([\d]+\)$', text):  # "(1)" "(2)"
            return False
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€$', text):  # "ä¸€ã€" "äºŒã€"
            return False

        # 3. è¿‡æ»¤è¯´æ˜æ€§å†…å®¹çš„ç‰¹å¾å…³é”®è¯
        note_keywords = ['æ³¨ï¼š', 'æ³¨:', 'è¯´æ˜ï¼š', 'è¯´æ˜:', 'å¤‡æ³¨ï¼š', 'å¤‡æ³¨:']
        if any(text.startswith(kw) for kw in note_keywords):
            # "æ³¨ï¼š"æœ¬èº«å¯èƒ½ä¸æ˜¯ç« èŠ‚ï¼Œä½†å¦‚æœåé¢æœ‰å†…å®¹å¯èƒ½æ˜¯
            # å¦‚æœåªæœ‰"æ³¨ï¼š"ä¸¤ä¸ªå­—ï¼Œè‚¯å®šä¸æ˜¯ç« èŠ‚æ ‡é¢˜
            if len(text) <= 4:
                return False

        # 4. è¿‡æ»¤è¶…é•¿å†…å®¹ï¼ˆç« èŠ‚æ ‡é¢˜ä¸€èˆ¬ä¸è¶…è¿‡50ä¸ªå­—ç¬¦ï¼‰
        # ä½†å…è®¸åŒ…å«é•¿æ¡æ¬¾ç¼–å·çš„ç« èŠ‚ï¼ˆå¦‚ "7.3 åŒæ–¹å‡åº”å½“..."ï¼‰
        if len(text) > 100:  # è¶…è¿‡100å­—ç¬¦è‚¯å®šä¸æ˜¯æ ‡é¢˜
            return False

        # 5. è¿‡æ»¤çº¯åˆ—è¡¨é¡¹æ ¼å¼ï¼ˆå¼€å¤´æ˜¯ç¼–å·+ç®€çŸ­å†…å®¹ï¼‰
        # ä¾‹å¦‚: "1.ä»¥ä¸Šå“åº”æ–‡ä»¶çš„æ„æˆä¸ºå¿…é¡»åŒ…å«çš„å†…å®¹..."
        if re.match(r'^[\d]+\.[\u4e00-\u9fa5]{2,}', text):
            # å¦‚æœåŒ¹é… "æ•°å­—.ä¸­æ–‡"ï¼Œä¸”æ²¡æœ‰æ˜ç¡®çš„ç« èŠ‚ç‰¹å¾è¯ï¼Œåˆ™å¯èƒ½æ˜¯åˆ—è¡¨é¡¹
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç« èŠ‚ç‰¹å¾è¯
            chapter_markers = ['ç« ', 'èŠ‚', 'éƒ¨åˆ†', 'ç¬¬', 'æ¡æ¬¾', 'è¦æ±‚', 'æ ‡å‡†', 'æ–¹æ³•', 'åŸåˆ™']
            if not any(marker in text for marker in chapter_markers):
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå¦‚æœå†…å®¹å¾ˆé•¿ï¼ˆ>20å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯è¯´æ˜æ€§åˆ—è¡¨é¡¹
                if len(text) > 30:
                    return False

        # 6. ç‰¹æ®Šè¿‡æ»¤ï¼šæ’é™¤çº¯æ¡æ¬¾ç¼–å·ï¼ˆå¦‚ "1.1.1"ã€"2.3.4.5"ï¼‰
        if re.match(r'^[\d]+\.[\d]+\.[\d]+(\.[\d]+)*$', text):
            return False

        # 7. ç‰¹æ®Šè¿‡æ»¤ï¼šæ’é™¤ä»¥æ¡æ¬¾ç¼–å·å¼€å¤´ä¸”è¿‡é•¿çš„å†…å®¹
        # ä¾‹å¦‚: "7.3 åŒæ–¹å‡åº”å½“ä¸¥æ ¼æŒ‰ç…§ç›¸å…³æ³•å¾‹æ³•è§„..."ï¼ˆè¿™ä¸æ˜¯ç« èŠ‚æ ‡é¢˜ï¼‰
        if re.match(r'^[\d]+\.[\d]+\s+', text) and len(text) > 50:
            return False

        return True

    def _parse_chapters_by_outline_level(self, doc: Document) -> List[ChapterNode]:
        """
        æ–¹æ³•äº”ï¼šåŸºäºWordå¤§çº²çº§åˆ«ï¼ˆoutlineLevelï¼‰è¯†åˆ«ç« èŠ‚

        ä½¿ç”¨å¾®è½¯å®˜æ–¹çš„å¤§çº²çº§åˆ«APIï¼Œæ”¯æŒ0-8çº§æ ‡é¢˜è¯†åˆ«ï¼Œ
        åŒ…å«3å±‚ç²¾ç»†è¿‡æ»¤è§„åˆ™å’ŒHeadingæ ·å¼å¤‡ç”¨æ£€æµ‹ã€‚

        Args:
            doc: python-docx Document å¯¹è±¡

        Returns:
            ç« èŠ‚åˆ—è¡¨ï¼ˆæ‰å¹³ç»“æ„ï¼Œæœªæ„å»ºæ ‘ï¼‰
        """
        import re

        chapters = []
        chapter_counter = 0

        self.logger.info("ä½¿ç”¨æ–¹æ³•äº”ï¼šå¤§çº²çº§åˆ«è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼‰")

        # ç›´æ¥ä»Wordæ–‡æ¡£æå–æ ‡é¢˜ - ä½¿ç”¨å¾®è½¯å®˜æ–¹çš„å¤§çº²çº§åˆ«API
        for para_idx, paragraph in enumerate(doc.paragraphs):
            is_heading = False
            level = 0
            detection_method = ""

            # â­ ä¼˜å…ˆçº§1: æ£€æŸ¥å¤§çº²çº§åˆ« (Outline Level) - å¾®è½¯å®˜æ–¹è¯­ä¹‰æ ‡è®°
            # è¿™æ˜¯Wordå¯¼èˆªçª—æ ¼å’Œå¤§çº²è§†å›¾ä½¿ç”¨çš„ç»“æ„ï¼Œå‡†ç¡®åº¦æœ€é«˜
            try:
                pPr = paragraph._element.pPr
                if pPr is not None:
                    outlineLvl = pPr.outlineLvl
                    if outlineLvl is not None:
                        outline_level_val = int(outlineLvl.val)
                        # Wordå¤§çº²çº§åˆ«: 0-8è¡¨ç¤ºæ ‡é¢˜(0=ä¸€çº§), 9è¡¨ç¤ºæ­£æ–‡
                        if outline_level_val <= 8:
                            # ğŸ”§ æ·»åŠ è¿‡æ»¤è§„åˆ™ï¼Œæ’é™¤å™ªéŸ³å†…å®¹
                            text = paragraph.text.strip()
                            should_skip = False

                            # è¿‡æ»¤1: è·³è¿‡æ–‡æ¡£å‰30æ®µçš„å°é¢/å…ƒæ•°æ®ï¼ˆLevel 0ï¼‰
                            if para_idx < 30 and outline_level_val == 0:
                                metadata_keywords = ['é¡¹ç›®ç¼–å·', 'æ‹›æ ‡äºº', 'ä»£ç†æœºæ„', 'è”ç³»äºº', 'è”ç³»æ–¹å¼',
                                                    'åœ°å€', 'ç”µè¯', 'ä¼ çœŸ', 'é‚®ç¼–', 'ç½‘å€', 'http']
                                if any(kw in text for kw in metadata_keywords):
                                    should_skip = True
                                    self.logger.debug(f"è¿‡æ»¤å°é¢: æ®µè½{para_idx} '{text[:30]}'")

                            # è¿‡æ»¤2: è·³è¿‡Level 3-4çš„é•¿æ¡æ¬¾å†…å®¹
                            if not should_skip and outline_level_val >= 3:
                                # å½¢å¦‚ "1.1 è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„è¯´æ˜æ–‡å­—..." çš„æ˜¯æ¡æ¬¾ï¼Œä¸æ˜¯æ ‡é¢˜
                                if re.match(r'^\d+\.\d+\s+.{15,}', text):
                                    should_skip = True
                                    self.logger.debug(f"è¿‡æ»¤æ¡æ¬¾: æ®µè½{para_idx} '{text[:30]}'")

                            # è¿‡æ»¤3: æ ‡é¢˜é•¿åº¦é™åˆ¶ï¼ˆè¶…è¿‡50å­—çš„é€šå¸¸ä¸æ˜¯æ ‡é¢˜ï¼‰
                            if not should_skip and len(text) > 50:
                                # é™¤éæœ‰æ˜ç¡®çš„ç« èŠ‚ç¼–å·
                                if not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', text):
                                    should_skip = True
                                    self.logger.debug(f"è¿‡æ»¤é•¿æ–‡æœ¬: æ®µè½{para_idx} '{text[:30]}'")

                            if not should_skip:
                                is_heading = True
                                level = outline_level_val + 1  # è½¬æ¢: 0â†’1çº§, 1â†’2çº§, ...
                                detection_method = f"å¤§çº²çº§åˆ«{outline_level_val}"
            except (AttributeError, TypeError, ValueError):
                pass  # æ²¡æœ‰å¤§çº²çº§åˆ«ï¼Œç»§ç»­å…¶ä»–æ–¹æ³•

            # ä¼˜å…ˆçº§2: æ£€æŸ¥æ ‡å‡†Headingæ ·å¼ (å¤‡ç”¨æ–¹æ¡ˆ)
            if not is_heading:
                style_name = paragraph.style.name if paragraph.style else ""

                # åªæ¥å—æ ‡å‡†çš„Headingæ ·å¼ï¼ˆç²¾ç¡®åŒ¹é…ï¼Œé¿å…è¯¯è¯†åˆ«ï¼‰
                if style_name.startswith('Heading '):  # 'Heading 1', 'Heading 2'
                    match = re.search(r'Heading (\d+)', style_name)
                    if match:
                        is_heading = True
                        level = int(match.group(1))
                        detection_method = f"æ ·å¼{style_name}"
                elif style_name.startswith('æ ‡é¢˜ '):  # 'æ ‡é¢˜ 1', 'æ ‡é¢˜ 2'
                    match = re.search(r'æ ‡é¢˜ (\d+)', style_name)
                    if match:
                        is_heading = True
                        level = int(match.group(1))
                        detection_method = f"æ ·å¼{style_name}"

            if is_heading and paragraph.text.strip():
                title = paragraph.text.strip()

                chapter = ChapterNode(
                    id=f"docx_{chapter_counter}",
                    level=level if level > 0 else 1,
                    title=title,
                    para_start_idx=para_idx,
                    para_end_idx=None,  # ç¨åè®¡ç®—
                    word_count=0,       # ç¨åè®¡ç®—
                    preview_text="",    # ç¨åæå–
                    content_tags=['docx_native', detection_method]
                )

                chapters.append(chapter)
                chapter_counter += 1

                self.logger.debug(
                    f"è¯†åˆ«æ ‡é¢˜: æ®µè½{para_idx} [{detection_method}] '{title[:50]}'"
                )

        self.logger.info(f"âœ… æ–¹æ³•äº”è¯†åˆ«å®Œæˆï¼šæ‰¾åˆ° {len(chapters)} ä¸ªæ ‡é¢˜")
        return chapters

    def _get_heading_level(self, paragraph) -> int:
        """
        è·å–æ®µè½çš„æ ‡é¢˜å±‚çº§ (ä¼˜åŒ–6: æ·»åŠ å‰ç½®è¿‡æ»¤å’Œå¤§çº²çº§åˆ«æ£€æŸ¥)

        Args:
            paragraph: python-docx Paragraph å¯¹è±¡

        Returns:
            0: ä¸æ˜¯æ ‡é¢˜
            1-3: æ ‡é¢˜å±‚çº§
        """
        text = paragraph.text.strip()

        # âš ï¸ æ–¹æ³•0ï¼šå‰ç½®è¿‡æ»¤ - æ’é™¤æ˜æ˜¾ä¸æ˜¯ç« èŠ‚çš„å†…å®¹
        if not self._is_valid_chapter_title(text):
            return 0

        # â­ æ–¹æ³•1ï¼šå¤§çº²çº§åˆ«åˆ¤æ–­ï¼ˆæœ€å¯é ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
        try:
            pPr = paragraph._element.pPr
            if pPr is not None:
                outlineLvl = pPr.outlineLvl
                if outlineLvl is not None:
                    level = int(outlineLvl.val)
                    if level <= 2:  # 0=ä¸€çº§, 1=äºŒçº§, 2=ä¸‰çº§
                        self.logger.debug(f"âœ“ å¤§çº²çº§åˆ«è¯†åˆ«: Level {level} â†’ '{text[:30]}'")
                        return level + 1  # è½¬æ¢ä¸º1-3
        except (AttributeError, TypeError):
            pass  # æ²¡æœ‰å¤§çº²çº§åˆ«ï¼Œç»§ç»­å…¶ä»–æ–¹æ³•

        # æ–¹æ³•2ï¼šé€šè¿‡æ ·å¼ååˆ¤æ–­ (ä¼˜å…ˆï¼Œæœ€å¯é )
        if paragraph.style and paragraph.style.name:
            style_name = paragraph.style.name
            for level, style_names in self.HEADING_STYLES.items():
                if any(sn.lower() in style_name.lower() for sn in style_names):
                    return level

        # æ–¹æ³•3ï¼šé€šè¿‡ XML æ ·å¼å±æ€§åˆ¤æ–­ï¼ˆæ›´å‡†ç¡®ï¼‰
        try:
            pPr = paragraph._element.pPr
            if pPr is not None:
                pStyle = pPr.pStyle
                if pStyle is not None:
                    style_val = pStyle.val
                    if 'heading1' in style_val.lower() or style_val.lower() == '1':
                        return 1
                    elif 'heading2' in style_val.lower() or style_val.lower() == '2':
                        return 2
                    elif 'heading3' in style_val.lower() or style_val.lower() == '3':
                        return 3
        except (AttributeError, TypeError):
            pass  # XMLå±æ€§ä¸å¯ç”¨æ—¶ä½¿ç”¨å…¶ä»–æ–¹æ³•

        # æ–¹æ³•4ï¼šé€šè¿‡æ–‡æœ¬æ ¼å¼å¯å‘å¼åˆ¤æ–­ (ä¼˜åŒ–: æ”¶é›†æ‰€æœ‰runçš„ä¿¡æ¯)
        if paragraph.runs:
            # æ”¶é›†æ‰€æœ‰runçš„å­—ä½“ä¿¡æ¯
            sizes = []
            bold_count = 0

            for run in paragraph.runs:
                if run.font.size:
                    sizes.append(run.font.size.pt)
                if run.bold:
                    bold_count += 1

            # è‡³å°‘ä¸€åŠçš„runæ˜¯åŠ ç²—ï¼Œæ‰è®¤ä¸ºæ˜¯æ ‡é¢˜
            if sizes and bold_count >= len(paragraph.runs) / 2:
                avg_size = sum(sizes) / len(sizes)

                # è°ƒæ•´é˜ˆå€¼: æ›´çµæ´»çš„åˆ¤æ–­ (é™ä½é˜ˆå€¼ä»¥æé«˜è¯†åˆ«ç‡)
                if avg_size >= 16:  # 16pt+ â†’ Level 1 (åŸ18pt)
                    return 1
                elif avg_size >= 13:  # 13-15pt â†’ Level 2 (åŸ15pt)
                    return 2
                elif avg_size >= 10:  # 10-12pt â†’ Level 3 (åŸ12pt)
                    return 3

        # æ–¹æ³•5ï¼šé€šè¿‡ç¼–å·æ¨¡å¼åˆ¤æ–­ (å¢å¼ºfallbackæœºåˆ¶)
        # ä¸€çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« éƒ¨åˆ†]', text):
            return 1
        if re.match(r'^\d+\.\s+\S', text) and not re.match(r'^\d+\.\d+', text):  # 1. xxx (ä¸åŒ…å«1.1)
            return 1

        # äºŒçº§æ ‡é¢˜æ¨¡å¼ (å¦‚æœæ–‡æœ¬è¾ƒçŸ­ä¸”æœ‰ç¼–å·)
        if re.match(r'^\d+\.\d+\s+\S', text) and not re.match(r'^\d+\.\d+\.\d+', text):
            if len(text.strip()) <= 100:  # é•¿åº¦é™åˆ¶
                return 2

        # ä¸‰çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^\d+\.\d+\.\d+\s+\S', text):
            if len(text.strip()) <= 100:
                return 3

        return 0

    def _clean_title_v2(self, title: str, aggressive=False) -> str:
        """
        åˆ†é˜¶æ®µæ¸…ç†æ ‡é¢˜æ–‡æœ¬ (ä¼˜åŒ–3: æ¸©å’Œ/æ¿€è¿›ä¸¤ç§æ¨¡å¼)

        Args:
            title: åŸå§‹æ ‡é¢˜
            aggressive: æ˜¯å¦ä½¿ç”¨æ¿€è¿›æ¸…ç†æ¨¡å¼

        Returns:
            æ¸…ç†åçš„æ ‡é¢˜
        """
        if not aggressive:
            # æ¸©å’Œæ¸…ç†: åªåˆ é™¤æ˜æ˜¾çš„ç¼–å·å’Œç©ºæ ¼
            cleaned = re.sub(r'^\d+\.\s*', '', title)  # åˆ é™¤ "1. "
            cleaned = re.sub(r'^\d+\.\d+\s*', '', cleaned)  # åˆ é™¤ "1.1 "
            cleaned = re.sub(r'^\d+\.\d+\.\d+\s*', '', cleaned)  # åˆ é™¤ "1.1.1 "
            cleaned = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*', '', cleaned)  # åˆ é™¤ "ç¬¬ä¸€ç«  "
            cleaned = re.sub(r'\s+', '', cleaned)  # åˆ é™¤ç©ºæ ¼
            return cleaned
        else:
            # æ¿€è¿›æ¸…ç†: åˆ é™¤æ‰€æœ‰ç¼–å·ã€ç¬¦å·å’Œç©ºæ ¼
            cleaned = title
            cleaned = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s*', '', cleaned)
            cleaned = re.sub(r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)\s*', '', cleaned)
            cleaned = re.sub(r'^\d+[-\.]\d*\s*', '', cleaned)
            cleaned = re.sub(r'^[A-Za-z]+\.\s*', '', cleaned)
            cleaned = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚éƒ¨åˆ†]\s*', '', cleaned)
            cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', cleaned)  # åªä¿ç•™ä¸­è‹±æ–‡æ•°å­—
            return cleaned

    def fuzzy_match_title_v2(self, title1: str, title2: str, threshold=0.7) -> float:
        """
        æ¨¡ç³ŠåŒ¹é…æ ‡é¢˜ï¼Œæ”¯æŒå¤šçº§æ¸…ç†å°è¯• (ä¼˜åŒ–3: åˆ†é˜¶æ®µåŒ¹é…)

        Args:
            title1: æ ‡é¢˜1
            title2: æ ‡é¢˜2
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            ç›¸ä¼¼åº¦å¾—åˆ† (0.0-1.0)
        """
        # Level 1: åŸå§‹æ¯”è¾ƒ
        if title1 == title2:
            return 1.0

        # Level 2: æ¸©å’Œæ¸…ç†åæ¯”è¾ƒ
        clean1 = self._clean_title_v2(title1, aggressive=False)
        clean2 = self._clean_title_v2(title2, aggressive=False)

        if clean1 == clean2:
            return 0.95

        if clean1 in clean2 or clean2 in clean1:
            return 0.90

        # Level 3: æ¿€è¿›æ¸…ç†åæ¯”è¾ƒ
        aggr1 = self._clean_title_v2(title1, aggressive=True)
        aggr2 = self._clean_title_v2(title2, aggressive=True)

        if aggr1 == aggr2:
            return 0.85

        if aggr1 in aggr2 or aggr2 in aggr1:
            shorter = aggr1 if len(aggr1) <= len(aggr2) else aggr2
            longer = aggr2 if len(aggr1) <= len(aggr2) else aggr1
            return len(shorter) / len(longer) * 0.80  # åŒ…å«åŒ¹é…ï¼Œæ ¹æ®é•¿åº¦æ¯”ä¾‹æ‰“åˆ†

        # Level 4: SequenceMatcherç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, aggr1, aggr2).ratio()

        return similarity


    def _calculate_contract_density(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬çš„åˆåŒå¯†åº¦ï¼ˆåˆåŒç‰¹å¾è¯å‡ºç°é¢‘ç‡ï¼‰

        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬å†…å®¹

        Returns:
            åˆåŒå¯†åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
        """
        if not text or len(text) < 100:  # æ–‡æœ¬å¤ªçŸ­ï¼Œæ— æ³•åˆ¤æ–­
            return 0.0

        # å®šä¹‰åˆåŒç‰¹å¾è¯åŠå…¶æƒé‡
        contract_keywords = {
            # å¼ºåˆåŒç‰¹å¾ï¼ˆæƒé‡ x3ï¼‰
            'ç”²æ–¹': 3, 'ä¹™æ–¹': 3, 'ä¸™æ–¹': 3,
            # ä¸­ç­‰åˆåŒç‰¹å¾ï¼ˆæƒé‡ x2ï¼‰
            'è¿çº¦é‡‘': 2, 'è¿çº¦è´£ä»»': 2, 'å±¥çº¦ä¿è¯é‡‘': 2, 'æœ¬åˆåŒ': 2,
            'åˆåŒç”Ÿæ•ˆ': 2, 'åˆåŒç»ˆæ­¢': 2, 'åˆåŒè§£é™¤': 2,
            # å¼±åˆåŒç‰¹å¾ï¼ˆæƒé‡ x1ï¼‰
            'ä»˜æ¬¾': 1, 'éªŒæ”¶': 1, 'ä¿å¯†': 1, 'äº‰è®®': 1,
            'ä»²è£': 1, 'ç®¡è¾–': 1, 'åŒæ–¹': 1, 'ç­¾è®¢': 1
        }

        # è®¡ç®—åŠ æƒå‡ºç°æ¬¡æ•°
        total_weight = 0
        for keyword, weight in contract_keywords.items():
            count = text.count(keyword)
            total_weight += count * weight

        # è®¡ç®—å¯†åº¦ï¼ˆæ¯1000å­—ç¬¦çš„åŠ æƒå…³é”®è¯å‡ºç°æ¬¡æ•°ï¼‰
        text_length = len(text)
        density = (total_weight / text_length) * 1000

        # å½’ä¸€åŒ–åˆ°0-1ä¹‹é—´ï¼ˆå‡è®¾å¯†åº¦>50å°±æ˜¯100%çš„åˆåŒï¼‰
        normalized_density = min(density / 50.0, 1.0)

        return normalized_density

    def _is_contract_chapter(self, title: str, content_sample: str = None) -> tuple:
        """
        åˆ¤æ–­ç« èŠ‚æ˜¯å¦ä¸ºåˆåŒç« èŠ‚ï¼ˆå¢å¼ºç‰ˆï¼šç»“åˆæ ‡é¢˜ã€å†…å®¹ã€æ’é™¤è§„åˆ™ï¼‰

        Args:
            title: ç« èŠ‚æ ‡é¢˜
            content_sample: ç« èŠ‚å†…å®¹æ ·æœ¬ï¼ˆå¯é€‰ï¼‰

        Returns:
            (is_contract, density, reason): æ˜¯å¦ä¸ºåˆåŒç« èŠ‚ã€åˆåŒå¯†åº¦ã€åˆ¤æ–­ç†ç”±
        """
        # ğŸ†• 0. æ’é™¤è§„åˆ™ä¼˜å…ˆï¼ˆé¿å…è¯¯åˆ¤ï¼‰
        exclude_keywords = [
            "æŠ•æ ‡äººé¡»çŸ¥", "ä¾›åº”å•†é¡»çŸ¥", "æŠ•æ ‡é‚€è¯·", "æŠ•æ ‡é¡»çŸ¥", "æ‹›æ ‡é¡»çŸ¥",
            "é™„ä»¶", "æŠ•æ ‡æ–‡ä»¶æ ¼å¼", "å“åº”æ–‡ä»¶æ ¼å¼", "æŠ•æ ‡æ–‡ä»¶ç»„æˆ", "è°ˆåˆ¤å“åº”æ–‡ä»¶æ ¼å¼",
            "ç”¨æˆ·éœ€æ±‚", "éœ€æ±‚ä¹¦", "æŠ€æœ¯è¦æ±‚", "æŠ€æœ¯è§„æ ¼", "æŠ€æœ¯éœ€æ±‚ä¹¦", "æŠ€æœ¯éœ€æ±‚",
            "è¯„åˆ†", "è¯„å®¡", "è¯„æ ‡", "å¼€æ ‡", "æŠ¥ä»·", "æŠ•æ ‡æŠ¥ä»·",
            "æŠ•æ ‡é‚€è¯·å‡½", "é‡‡è´­é‚€è¯·", "è°ˆåˆ¤é‚€è¯·"
        ]

        for exclude_kw in exclude_keywords:
            if exclude_kw in title:
                self.logger.debug(f"  â›” æ’é™¤ç« èŠ‚: '{title}' - åŒ¹é…æ’é™¤å…³é”®è¯'{exclude_kw}'")
                return (False, 0.0, f"æ’é™¤è§„åˆ™: '{exclude_kw}'")

        # 1. åŸºäºæ ‡é¢˜çš„å¼ºåˆåŒæ ‡è¯†ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        strong_contract_keywords = [
            "åˆåŒæ¡æ¬¾", "åˆåŒæ–‡æœ¬", "åˆåŒèŒƒæœ¬", "åˆåŒæ ¼å¼", "åˆåŒåè®®",
            "é€šç”¨æ¡æ¬¾", "ä¸“ç”¨æ¡æ¬¾", "åˆåŒä¸»è¦æ¡æ¬¾", "åˆåŒè‰ç¨¿", "æ‹Ÿç­¾åˆåŒ",
            "æœåŠ¡åˆåŒ", "é‡‡è´­åˆåŒ", "ä¹°å–åˆåŒ", "é”€å”®åˆåŒ", "æ–½å·¥åˆåŒ",
            "åˆ†åŒ…åˆåŒ", "åŠ³åŠ¡åˆåŒ", "ç§ŸèµåˆåŒ", "å§”æ‰˜åˆåŒ", "ä»£ç†åˆåŒ",
        ]

        for keyword in strong_contract_keywords:
            if keyword in title:
                return (True, 1.0, f"æ ‡é¢˜å¼ºåŒ¹é…: '{keyword}'")

        # 2. åŸºäºå†…å®¹çš„åˆåŒå¯†åº¦æ£€æµ‹ï¼ˆğŸ†• æé«˜é˜ˆå€¼ï¼Œé™ä½è¯¯åˆ¤ï¼‰
        if content_sample:
            density = self._calculate_contract_density(content_sample)

            # ğŸ†• é«˜å¯†åº¦é˜ˆå€¼ï¼šä»5%æé«˜åˆ°10%
            high_density_threshold = 0.10

            if density > high_density_threshold:
                # é«˜å¯†åº¦éœ€è¦æ ‡é¢˜éªŒè¯ï¼Œé¿å…è¯¯åˆ¤"å¼•ç”¨åˆåŒå†…å®¹"çš„ç« èŠ‚
                title_has_contract_kw = any(kw in title for kw in ["åˆåŒ", "åè®®", "æ¡æ¬¾", "ç”²æ–¹", "ä¹™æ–¹"])

                if title_has_contract_kw:
                    return (True, density, f"é«˜å¯†åº¦+æ ‡é¢˜éªŒè¯: {density:.1%}")
                else:
                    # å¯†åº¦é«˜ä½†æ ‡é¢˜ä¸åƒåˆåŒï¼Œå¯èƒ½æ˜¯å¼•ç”¨åˆåŒå†…å®¹
                    self.logger.debug(f"  âš ï¸  é«˜å¯†åº¦({density:.1%})ä½†æ ‡é¢˜ä¸åŒ¹é…: '{title}'")
                    return (False, density, f"é«˜å¯†åº¦ä½†æ ‡é¢˜ä¸åŒ¹é…: {density:.1%}")

        # 3. å¼±åˆåŒæ ‡è¯†ï¼ˆæ ‡é¢˜æ¨¡ç³Šä½†å¯èƒ½æ˜¯åˆåŒï¼‰- ä¿ç•™åŸæœ‰é€»è¾‘
        weak_contract_patterns = [
            r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|é™„ä»¶\d*)[^\u4e00-\u9fa5]*(åè®®|æ¡æ¬¾|æƒåˆ©|ä¹‰åŠ¡)',
            r'åŒæ–¹.*æƒåˆ©.*ä¹‰åŠ¡',
            r'ç”².*ä¹™.*æ–¹',
        ]

        import re
        for pattern in weak_contract_patterns:
            if re.search(pattern, title):
                # å¦‚æœæœ‰å†…å®¹æ ·æœ¬ï¼Œè¿›ä¸€æ­¥éªŒè¯
                if content_sample:
                    density = self._calculate_contract_density(content_sample)
                    # ğŸ†• æé«˜å¼±åŒ¹é…çš„å¯†åº¦é˜ˆå€¼ï¼šä»3%æé«˜åˆ°5%
                    if density > 0.05:
                        return (True, density, f"æ ‡é¢˜å¼±åŒ¹é…+å†…å®¹éªŒè¯: {density:.1%}")

        return (False, 0.0, "éåˆåŒç« èŠ‚")

    def _extract_content_sample(self, doc: Document, para_start_idx: int, para_end_idx: int, sample_size: int = 2000) -> str:
        """
        æå–ç« èŠ‚å†…å®¹æ ·æœ¬ï¼ˆç”¨äºåˆåŒè¯†åˆ«ï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_start_idx: èµ·å§‹æ®µè½ç´¢å¼•
            para_end_idx: ç»“æŸæ®µè½ç´¢å¼•
            sample_size: æ ·æœ¬å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰

        Returns:
            å†…å®¹æ ·æœ¬å­—ç¬¦ä¸²
        """
        if para_end_idx is None or para_end_idx <= para_start_idx:
            return ""

        # æå–ç« èŠ‚å†…å®¹ï¼ˆè·³è¿‡æ ‡é¢˜æœ¬èº«ï¼‰
        content_paras = doc.paragraphs[para_start_idx + 1 : para_end_idx + 1]

        # åˆå¹¶æ–‡æœ¬
        sample_text = ""
        for para in content_paras:
            text = para.text.strip()
            if text:
                sample_text += text + "\n"
                if len(sample_text) >= sample_size:
                    break

        # æˆªå–æŒ‡å®šé•¿åº¦
        return sample_text[:sample_size]
    def _detect_contract_cluster_in_chapter(self, doc: Document, start_idx: int, end_idx: int) -> Optional[Dict]:
        """
        æ£€æµ‹ç« èŠ‚å†…çš„åˆåŒæ®µè½èšé›†åŒºï¼ˆç²¾ç¡®å®šä½èµ·å§‹ä½ç½®ï¼‰

        ç­–ç•¥ï¼š
        1. ç”¨50æ®µæ»‘åŠ¨çª—å£æ‰«ææ•´ä¸ªç« èŠ‚
        2. æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆåŒå¯†åº¦>20%çš„åŒºåŸŸ
        3. ä»è¯¥åŒºåŸŸå‘å‰ç²¾ç¡®å®šä½èšé›†åŒºèµ·ç‚¹ï¼ˆæ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å«åˆåŒç‰¹å¾çš„æ®µè½ï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            start_idx: ç« èŠ‚èµ·å§‹æ®µè½ç´¢å¼•
            end_idx: ç« èŠ‚ç»“æŸæ®µè½ç´¢å¼•

        Returns:
            å¦‚æœå‘ç°èšé›†åŒºï¼š{'start': int, 'end': int, 'density': float}
            å¦åˆ™è¿”å› None
        """
        # ğŸ†• å‚æ•°éªŒè¯ï¼šé˜²æ­¢ end_idx ä¸º None æˆ–æ— æ•ˆå€¼
        if end_idx is None or start_idx is None:
            self.logger.debug(f"  âš ï¸  å‚æ•°æ— æ•ˆ: start_idx={start_idx}, end_idx={end_idx}ï¼Œè·³è¿‡åˆåŒèšé›†åŒºæ£€æµ‹")
            return None

        if end_idx <= start_idx:
            self.logger.debug(f"  âš ï¸  ç« èŠ‚èŒƒå›´æ— æ•ˆ: start_idx={start_idx}, end_idx={end_idx}ï¼Œè·³è¿‡åˆåŒèšé›†åŒºæ£€æµ‹")
            return None

        if end_idx - start_idx < 50:
            # ç« èŠ‚å¤ªçŸ­ï¼Œä¸éœ€è¦æ£€æµ‹
            return None

        window_size = 50  # çª—å£å¤§å°ï¼š50ä¸ªæ®µè½
        density_threshold = 0.2  # å¯†åº¦é˜ˆå€¼ï¼š20%
        step_size = 10  # æ»‘åŠ¨æ­¥é•¿ï¼šæ¯æ¬¡ç§»åŠ¨10æ®µ

        # æ»‘åŠ¨çª—å£æ‰«æ
        for i in range(start_idx, end_idx - window_size, step_size):
            window_end = min(i + window_size, end_idx)

            # æå–çª—å£å†…æ–‡æœ¬
            window_text = ""
            for j in range(i, window_end):
                if j < len(doc.paragraphs):
                    para_text = doc.paragraphs[j].text.strip()
                    if para_text:
                        window_text += para_text + "\n"

            # è®¡ç®—åˆåŒå¯†åº¦
            density = self._calculate_contract_density(window_text)

            if density > density_threshold:
                # æ‰¾åˆ°é«˜å¯†åº¦åŒºåŸŸï¼Œå‘å‰ç²¾ç¡®å®šä½èµ·ç‚¹
                cluster_start = i

                # å‘å‰æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«å¼ºåˆåŒç‰¹å¾çš„æ®µè½
                strong_contract_keywords = ['ç”²æ–¹', 'ä¹™æ–¹', 'æœ¬åˆåŒ', 'åˆåŒçš„ç»„æˆ', 'åˆåŒç»„æˆ']

                for j in range(i, start_idx - 1, -1):  # å‘å‰æŸ¥æ‰¾
                    if j < len(doc.paragraphs):
                        para_text = doc.paragraphs[j].text.strip()
                        if any(kw in para_text for kw in strong_contract_keywords):
                            cluster_start = j
                        else:
                            # æ‰¾åˆ°ä¸å«åˆåŒç‰¹å¾çš„æ®µè½ï¼Œåœæ­¢
                            break

                # ç¡®å®šèšé›†åŒºç»“æŸä½ç½®ï¼ˆå‘åæ‰«æï¼Œæ‰¾åˆ°å¯†åº¦é™ä½çš„ä½ç½®ï¼‰
                cluster_end = end_idx
                for j in range(window_end, end_idx, 10):
                    check_end = min(j + 50, end_idx)
                    check_text = "\n".join(
                        doc.paragraphs[k].text.strip()
                        for k in range(j, check_end)
                        if k < len(doc.paragraphs) and doc.paragraphs[k].text.strip()
                    )
                    check_density = self._calculate_contract_density(check_text)

                    if check_density < density_threshold:
                        cluster_end = j - 1
                        break

                # ğŸ†• è®¡ç®—èšé›†åŒºå ç« èŠ‚çš„æ¯”ä¾‹
                chapter_length = end_idx - start_idx
                cluster_length = cluster_end - cluster_start
                cluster_ratio = cluster_length / chapter_length if chapter_length > 0 else 0

                # ğŸ†• å æ¯”è¿‡é«˜ï¼ˆ>80%ï¼‰ï¼šå¯èƒ½æ•´ç« éƒ½æ˜¯åˆåŒï¼Œä¸æ‹†åˆ†
                if cluster_ratio > 0.8:
                    self.logger.info(
                        f"  âš ï¸  åˆåŒèšé›†åŒºå æ¯”è¿‡é«˜({cluster_ratio:.1%})ï¼Œä¸æ‹†åˆ†ç« èŠ‚ "
                        f"(æ®µè½{cluster_start}-{cluster_end}, å¯èƒ½æ•´ç« éƒ½æ˜¯åˆåŒæˆ–è¯¯åˆ¤)"
                    )
                    return None

                # ğŸ†• å æ¯”è¿‡ä½ï¼ˆ<20%ï¼‰ï¼šå¯èƒ½æ˜¯è¯¯åˆ¤ï¼ˆåªæ˜¯å¼•ç”¨äº†éƒ¨åˆ†åˆåŒå†…å®¹ï¼‰
                if cluster_ratio < 0.2:
                    self.logger.info(
                        f"  âš ï¸  åˆåŒèšé›†åŒºå æ¯”è¿‡ä½({cluster_ratio:.1%})ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤ "
                        f"(æ®µè½{cluster_start}-{cluster_end}, åªæ˜¯å¼•ç”¨äº†éƒ¨åˆ†åˆåŒå†…å®¹)"
                    )
                    return None

                # å æ¯”é€‚ä¸­ï¼ˆ20%-80%ï¼‰ï¼Œæ‰§è¡Œæ‹†åˆ†
                self.logger.info(
                    f"æ£€æµ‹åˆ°åˆåŒèšé›†åŒº: æ®µè½{cluster_start}-{cluster_end} "
                    f"(ç« èŠ‚èŒƒå›´:{start_idx}-{end_idx}, åˆåŒå¯†åº¦:{density:.1%}, å æ¯”:{cluster_ratio:.1%})"
                )

                return {
                    'start': cluster_start,
                    'end': cluster_end,
                    'density': density,
                    'ratio': cluster_ratio
                }

        return None
    def _find_toc_section(self, doc: Document) -> Optional[int]:
        """
        æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç›®å½•éƒ¨åˆ† (ä¼˜åŒ–ç‰ˆ: æ‰©å±•å…³é”®è¯ + SDTæ”¯æŒ)

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡

        Returns:
            ç›®å½•èµ·å§‹æ®µè½ç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        # ä¼˜åŒ–2: æ‰©å±•ç›®å½•å…³é”®è¯åˆ—è¡¨
        TOC_KEYWORDS = [
            # ä¸­æ–‡
            "ç›®å½•", "ç›®  å½•", "ç´¢å¼•", "ç« èŠ‚ç›®å½•", "å†…å®¹ç›®å½•",
            # è‹±æ–‡
            "contents", "table of contents", "catalogue", "index"
        ]

        # ç¬¬é›¶è½®: æ£€æµ‹SDTå®¹å™¨ä¸­çš„TOCåŸŸï¼ˆWordè‡ªåŠ¨ç›®å½•ï¼‰
        try:
            body = doc.element.body
            ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}

            # æŸ¥æ‰¾æ‰€æœ‰SDTå…ƒç´ 
            sdt_elements = body.findall('.//w:sdt', namespaces=ns)

            for sdt_idx, sdt in enumerate(sdt_elements):
                # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦æ˜¯TOCç±»å‹çš„SDTï¼ˆæ ‡å‡†Wordè‡ªåŠ¨ç›®å½•ï¼‰
                docpart = sdt.find('.//w:docPartObj/w:docPartGallery', namespaces=ns)
                is_toc_sdt = False

                if docpart is not None:
                    gallery_val = docpart.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')
                    if gallery_val == 'Table of Contents':
                        is_toc_sdt = True
                        self.logger.info(f"æ£€æµ‹åˆ°æ ‡å‡†TOCç±»å‹çš„SDT (docPartGallery)")

                # æ–¹æ³•2: æ£€æŸ¥SDTä¸­æ˜¯å¦åŒ…å«"ç›®å½•"å…³é”®è¯ï¼ˆæ‰‹å·¥åˆ›å»ºçš„ç›®å½•SDTï¼‰
                if not is_toc_sdt:
                    sdt_paras = sdt.findall('.//w:p', namespaces=ns)
                    if sdt_paras:
                        # è·å–ç¬¬ä¸€ä¸ªæ®µè½çš„æ–‡æœ¬
                        first_para_texts = sdt_paras[0].findall('.//w:t', namespaces=ns)
                        first_para_text = ''.join([t.text for t in first_para_texts if t.text])

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®å½•å…³é”®è¯
                        for keyword in TOC_KEYWORDS:
                            if keyword in first_para_text:
                                is_toc_sdt = True
                                self.logger.info(f"æ£€æµ‹åˆ°åŒ…å«'{keyword}'çš„SDTï¼Œå¯èƒ½æ˜¯æ‰‹å·¥ç›®å½•")
                                break

                # å¦‚æœç¡®è®¤æ˜¯ç›®å½•SDTï¼Œè¿”å›å¯¹åº”çš„æ®µè½ç´¢å¼•
                if is_toc_sdt:
                    # æ–¹æ³•1: å°è¯•åœ¨doc.paragraphsä¸­æŸ¥æ‰¾"ç›®å½•"æ ‡é¢˜
                    # ï¼ˆé€‚ç”¨äºç›®å½•æ ‡é¢˜åŒæ—¶å‡ºç°åœ¨doc.paragraphsä¸­çš„æƒ…å†µï¼‰
                    toc_keywords_normalized = ["ç›®å½•", "contents", "tableofcontents", "ç´¢å¼•", "ç« èŠ‚ç›®å½•", "å†…å®¹ç›®å½•"]
                    for idx, para in enumerate(doc.paragraphs[:50]):
                        text = para.text.strip().replace(" ", "").replace("\u3000", "").lower()
                        if text in toc_keywords_normalized:
                            self.logger.info(f"æ£€æµ‹åˆ°ç›®å½•SDTï¼Œç›®å½•æ ‡é¢˜åœ¨paragraphs[{idx}]")
                            return idx

                    # æ–¹æ³•2: å¦‚æœç›®å½•æ ‡é¢˜å®Œå…¨åœ¨SDTå†…éƒ¨ï¼ˆä¸åœ¨doc.paragraphsä¸­ï¼‰ï¼Œ
                    # é€šè¿‡è®¡ç®—SDTåœ¨bodyä¸­çš„ä½ç½®æ¥æ¨ç®—æ®µè½ç´¢å¼•
                    sdt_paras = sdt.findall('.//w:p', namespaces=ns)
                    if sdt_paras:
                        para_count = 0
                        for child in body:
                            child_tag = child.tag.split('}')[-1]
                            if child == sdt:
                                # æ‰¾åˆ°å½“å‰SDTï¼Œæ­¤æ—¶para_countå°±æ˜¯å®ƒå‰é¢çš„æ®µè½æ•°
                                self.logger.info(f"æ£€æµ‹åˆ°ç›®å½•SDTï¼ˆç›®å½•åœ¨SDTå†…ï¼‰ï¼Œä½äºbodyçš„ç¬¬{para_count}ä¸ªæ®µè½ä½ç½®")
                                self.logger.info(f"ç›®å½•ä½äºSDTä¸­ï¼ŒåŒ…å«{len(sdt_paras)}ä¸ªæ®µè½")
                                return para_count
                            elif child_tag == 'p':
                                para_count += 1
                            elif child_tag == 'sdt':
                                # ä¹‹å‰çš„SDTä¸­çš„æ®µè½ä¹Ÿè¦è®¡æ•°
                                prev_sdt_paras = child.findall('.//w:p', namespaces=ns)
                                para_count += len(prev_sdt_paras)

                    self.logger.debug("SDTç›®å½•æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯æ£€æµ‹")
        except Exception as e:
            self.logger.debug(f"SDTæ£€æµ‹å¤±è´¥ï¼ˆæ­£å¸¸æƒ…å†µï¼‰: {e}")
            pass

        # ç¬¬ä¸€è½®: æ£€æµ‹æ˜¾å¼ç›®å½•æ ‡é¢˜
        for i, para in enumerate(doc.paragraphs[:50]):  # åªæ£€æŸ¥å‰50æ®µ
            text = para.text.strip()

            # è·³è¿‡ç©ºæ®µè½
            if not text:
                continue

            # æ£€æµ‹ç›®å½•æ ‡é¢˜ (ä½¿ç”¨æ‰©å±•å…³é”®è¯åˆ—è¡¨)
            text_lower = text.lower()
            for keyword in TOC_KEYWORDS:
                if text_lower == keyword.lower() or text.replace(" ", "") == keyword.replace(" ", ""):
                    self.logger.info(f"æ£€æµ‹åˆ°ç›®å½•æ ‡é¢˜ (å…³é”®è¯: '{keyword}')ï¼Œä½äºæ®µè½ {i}: {text}")
                    return i

            # æ£€æµ‹TOCåŸŸï¼ˆWordè‡ªåŠ¨ç”Ÿæˆçš„ç›®å½•ï¼‰
            # é€šè¿‡æ£€æŸ¥æ®µè½çš„XMLæ¥è¯†åˆ«
            try:
                xml_str = para._element.xml.decode() if isinstance(para._element.xml, bytes) else str(para._element.xml)
                if 'TOC' in xml_str and 'fldChar' in xml_str:
                    self.logger.info(f"æ£€æµ‹åˆ°Word TOCåŸŸï¼Œä½äºæ®µè½ {i}")
                    return i
            except (AttributeError, UnicodeDecodeError, TypeError):
                pass  # æ— æ³•è·å–XMLæˆ–è§£æå¤±è´¥æ—¶è·³è¿‡è¯¥æ®µè½

        self.logger.info("æœªæ£€æµ‹åˆ°ç›®å½•ï¼Œå°†ä½¿ç”¨æ ‡é¢˜æ ·å¼è¯†åˆ«æ–¹æ¡ˆ")
        return None

    def _parse_toc_items(self, doc: Document, toc_start_idx: int) -> Tuple[List[Dict], int]:
        """
        è§£æç›®å½•é¡¹ï¼ˆæ”¹è¿›3ï¼šç¡®ä¿ toc_end_idx > toc_start_idx + SDTæ”¯æŒï¼‰

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            toc_start_idx: ç›®å½•èµ·å§‹æ®µè½ç´¢å¼•

        Returns:
            (ç›®å½•é¡¹åˆ—è¡¨, ç›®å½•ç»“æŸç´¢å¼•)
            ç›®å½•é¡¹åˆ—è¡¨æ ¼å¼ï¼š[{'title': '...', 'page_num': 1, 'level': 1}, ...]
        """
        toc_items = []
        consecutive_non_toc = 0  # è¿ç»­éç›®å½•é¡¹è®¡æ•°
        toc_end_idx = toc_start_idx  # ç›®å½•ç»“æŸä½ç½®

        # â­ï¸ ç›®å½•é¡¹æ•°é‡é™åˆ¶ï¼ˆé¿å…æ‰«æè¿‡è¿œï¼‰
        MAX_TOC_ITEMS = 200

        # ğŸ†• ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥ç›®å½•æ˜¯å¦åœ¨SDTä¸­
        try:
            body = doc.element.body
            ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}

            # è®¡ç®—toc_start_idxå¯¹åº”çš„bodyå­å…ƒç´ 
            para_count = 0
            target_sdt = None

            for child in body:
                child_tag = child.tag.split('}')[-1]
                if child_tag == 'p':
                    if para_count == toc_start_idx:
                        # æ‰¾åˆ°äº†,ä½†è¿™ä¸ªä½ç½®ä¸æ˜¯SDT
                        break
                    para_count += 1
                elif child_tag == 'sdt':
                    sdt_para_count = len(child.findall('.//w:p', namespaces=ns))
                    if para_count == toc_start_idx:
                        # toc_start_idxæ­£å¥½æŒ‡å‘è¿™ä¸ªSDT
                        target_sdt = child
                        self.logger.info(f"ç›®å½•ä½äºSDTä¸­ï¼ŒåŒ…å«{sdt_para_count}ä¸ªæ®µè½")
                        break
                    para_count += sdt_para_count

            # å¦‚æœæ‰¾åˆ°SDTç›®å½•ï¼Œç›´æ¥ä»SDTä¸­æå–ç›®å½•é¡¹
            if target_sdt is not None:
                sdt_paras = target_sdt.findall('.//w:p', namespaces=ns)
                self.logger.info(f"ä»SDTä¸­æå–ç›®å½•é¡¹ï¼Œå…±{len(sdt_paras)}ä¸ªæ®µè½")

                for para_elem in sdt_paras[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ª"ç›®å½•"æ ‡é¢˜æ®µè½
                    # æå–æ®µè½æ–‡æœ¬
                    texts = para_elem.findall('.//w:t', namespaces=ns)
                    text = ''.join([t.text for t in texts if t.text])
                    text = text.strip()

                    if not text:
                        continue

                    # è§£æç›®å½•é¡¹ï¼ˆä½¿ç”¨ç›¸åŒçš„æ­£åˆ™æ¨¡å¼ï¼‰
                    # åŒ¹é…ç›®å½•é¡¹æ ¼å¼ï¼ˆå¸¦é¡µç ï¼‰
                    toc_pattern = re.compile(
                        r'^(.+?)'  # æ ‡é¢˜ï¼ˆéè´ªå©ªï¼‰
                        r'[\s\.\u2026]*'  # å¯èƒ½çš„ç‚¹ã€ç©ºæ ¼æˆ–çœç•¥å·
                        r'(\d+)$'  # é¡µç 
                    )

                    match = toc_pattern.match(text)
                    if match:
                        title = match.group(1).strip()
                        page_num = int(match.group(2))

                        if len(toc_items) >= MAX_TOC_ITEMS:
                            self.logger.info(f"SDTç›®å½•é¡¹å·²è¾¾ä¸Šé™({MAX_TOC_ITEMS})ï¼Œåœæ­¢è§£æ")
                            break

                        # ä»XMLå…ƒç´ æ¨æ–­å±‚çº§ï¼ˆå› ä¸ºæ— æ³•ç›´æ¥è®¿é—®Paragraphå¯¹è±¡ï¼‰
                        level = 1
                        if re.match(r'^\d+\.\d+\.\d+', title):  # å…ˆæ£€æŸ¥3çº§
                            level = 3
                        elif re.match(r'^\d+\.\d+[^\d]', title):  # å†æ£€æŸ¥2çº§
                            level = 2

                        toc_items.append({
                            'title': title,
                            'page_num': page_num,
                            'level': level
                        })

                        self.logger.debug(f"SDTç›®å½•é¡¹ [{level}çº§]: {title} (é¡µç :{page_num})")

                # SDTç›®å½•è§£æå®Œæˆ
                if toc_items:
                    toc_end_idx = toc_start_idx - 1  # SDTä¸å ç”¨paragraphsç´¢å¼•,SDTåç¬¬ä¸€ä¸ªæ®µè½å°±æ˜¯toc_start_idx
                    self.logger.info(f"SDTç›®å½•è§£æå®Œæˆï¼Œå…±{len(toc_items)}é¡¹")
                    return (toc_items, toc_end_idx)

        except Exception as e:
            self.logger.debug(f"SDTç›®å½•è§£æå¤±è´¥ï¼Œå›é€€åˆ°å¸¸è§„æ–¹æ³•: {e}")

        # å¸¸è§„æ–¹æ³•ï¼šä»python-docxçš„paragraphsä¸­è§£æ
        # ğŸ”§ æ‰©å±•èŒƒå›´ä»100åˆ°150ï¼Œä»¥è¦†ç›–æ›´é•¿çš„ç›®å½•ï¼ˆå¦‚åŒ…å«100+ä¸ªç›®å½•é¡¹çš„æ‹›æ ‡æ–‡ä»¶ï¼‰
        # ğŸ†• ä» toc_start_idx å¼€å§‹ï¼ˆè€Œä¸æ˜¯ +1ï¼‰ï¼Œå› ä¸º toc_start_idx å¯èƒ½æœ¬èº«å°±æ˜¯ç¬¬ä¸€ä¸ªç›®å½•é¡¹
        #    å¦‚æœ toc_start_idx æ®µè½åªåŒ…å«"ç›®å½•"æ ‡é¢˜ï¼Œä¼šè¢«åç»­é€»è¾‘è·³è¿‡
        for i in range(toc_start_idx, min(toc_start_idx + 150, len(doc.paragraphs))):
            para = doc.paragraphs[i]
            text = para.text.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not text:
                continue

            # å°è¯•åŒ¹é…ç›®å½•é¡¹æ ¼å¼
            # æ ¼å¼1: "æ ‡é¢˜æ–‡æœ¬    é¡µç " (å¤šä¸ªç©ºæ ¼)
            match = re.match(r'^(.+?)\s{2,}(\d+)$', text)
            if not match:
                # æ ¼å¼2: "æ ‡é¢˜æ–‡æœ¬....é¡µç " (ç‚¹å·å¡«å……)
                match = re.match(r'^(.+?)\.{2,}(\d+)$', text)
            if not match:
                # æ ¼å¼3: "æ ‡é¢˜æ–‡æœ¬\té¡µç " (åˆ¶è¡¨ç¬¦)
                match = re.match(r'^(.+?)\t+(\d+)$', text)

            # ğŸ†• æ ¼å¼4: "ç¬¬Xç«  æ ‡é¢˜ é¡µç " (å•ä¸ªç©ºæ ¼ï¼Œä¸“é—¨åŒ¹é…ç« èŠ‚æ ‡é¢˜)
            # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œç¡®ä¿é¡µç å‰çš„ç©ºæ ¼è¢«æ­£ç¡®è¯†åˆ«
            if not match:
                match = re.match(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+[ç« éƒ¨åˆ†èŠ‚ç¯‡].+?)\s+(\d+)$', text)

            # ğŸ†• æ ¼å¼5: "æ ‡é¢˜æ–‡æœ¬ - é¡µç  -" (å¸¦çŸ­æ¨ªçº¿çš„é¡µç æ ¼å¼)
            # æŸäº›æ‹›æ ‡æ–‡ä»¶ä½¿ç”¨æ­¤æ ¼å¼ï¼Œå¦‚ "ç¬¬ä¸€ç« 	æŠ•æ ‡é‚€è¯·ä¹¦	- 3 -"
            if not match:
                match = re.match(r'^(.+?)\s*-\s*(\d+)\s*-\s*$', text)

            if match:
                title = match.group(1).strip()
                page_num = int(match.group(2))

                # â­ï¸ é‡å¤æ£€æµ‹ï¼šå¦‚æœä¸ç¬¬ä¸€é¡¹é‡å¤ï¼Œè¯´æ˜æ‰«æåˆ°æ­£æ–‡äº†
                # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ä»£æ›¿ä¸¥æ ¼ç›¸ç­‰ï¼Œä»¥åº”å¯¹æ–‡æœ¬ç•¥æœ‰å·®å¼‚çš„æƒ…å†µ
                if toc_items:
                    first_title = toc_items[0]['title']
                    similarity = self.fuzzy_match_title_v2(title, first_title)
                    if similarity >= 0.70:  # 70%ä»¥ä¸Šç›¸ä¼¼åº¦è®¤ä¸ºæ˜¯é‡å¤
                        self.logger.info(f"æ£€æµ‹åˆ°é‡å¤ç›®å½•é¡¹ï¼ˆä¸ç¬¬1é¡¹ç›¸ä¼¼åº¦{similarity:.0%}ï¼‰ï¼Œç›®å½•è§£æç»“æŸ")
                        self.logger.debug(f"  ç¬¬1é¡¹: '{first_title}' vs å½“å‰: '{title}'")
                        break

                # â­ï¸ æ•°é‡é™åˆ¶æ£€æŸ¥
                if len(toc_items) >= MAX_TOC_ITEMS:
                    self.logger.info(f"ç›®å½•é¡¹å·²è¾¾ä¸Šé™({MAX_TOC_ITEMS})ï¼Œåœæ­¢è§£æ")
                    break

                # å±‚çº§åˆå§‹åŒ–ä¸º1ï¼Œåç»­ç”± LevelAnalyzer ä¿®æ­£
                level = 1

                toc_items.append({
                    'title': title,
                    'page_num': page_num,
                    'level': level
                })

                self.logger.debug(f"ç›®å½•é¡¹ [{level}çº§]: {title} -> ç¬¬{page_num}é¡µ")

                # æ›´æ–°ç›®å½•ç»“æŸä½ç½®
                toc_end_idx = i
                # é‡ç½®è®¡æ•°
                consecutive_non_toc = 0
            else:
                # æ–°å¢ï¼šå°è¯•è¯†åˆ«æ— é¡µç çš„ç®€å•ç›®å½•é¡¹
                # ğŸ†• é¦–å…ˆè¿‡æ»¤ç›®å½•æ ‡é¢˜æœ¬èº«ï¼Œé¿å…å°†"ç›®å½•"å½“ä½œç›®å½•é¡¹
                TOC_TITLE_KEYWORDS = ['ç›®å½•', 'ç›®  å½•', 'ç›® å½•', 'contents', 'index', 'catalogue']
                text_normalized = text.replace(' ', '').replace('\u3000', '').lower()
                if text_normalized in [k.replace(' ', '').lower() for k in TOC_TITLE_KEYWORDS]:
                    self.logger.debug(f"è·³è¿‡ç›®å½•æ ‡é¢˜æœ¬èº«: '{text}'")
                    continue

                # ç‰¹å¾1ï¼šæœ‰ç»Ÿä¸€ç¼©è¿›ï¼ˆç›®å½•é¡¹é€šå¸¸ç¼©è¿›å¯¹é½ï¼‰
                has_indent = (para.paragraph_format.left_indent and
                              para.paragraph_format.left_indent > 200000)

                # ç‰¹å¾2ï¼šåŒ¹é…ç« èŠ‚æ¨¡å¼
                is_chapter_pattern = (
                    re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', text) or
                    re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', text) or
                    text in ['ç«äº‰æ€§ç£‹å•†å…¬å‘Š', 'æ‹›æ ‡å…¬å‘Š', 'é‡‡è´­å…¬å‘Š', 'è°ˆåˆ¤å…¬å‘Š', 'è¯¢ä»·å…¬å‘Š']
                )

                # ğŸ†• æ’é™¤ Heading æ ·å¼çš„æ®µè½ - Heading æ ·å¼è¯´æ˜è¿™æ˜¯æ­£æ–‡æ ‡é¢˜ï¼Œä¸æ˜¯ç›®å½•é¡¹
                # ç›®å½•é¡¹é€šå¸¸ä½¿ç”¨ TOC æ ·å¼æˆ–æ™®é€šæ ·å¼ï¼Œä¸ä¼šä½¿ç”¨ Heading æ ·å¼
                is_heading_style = para.style and para.style.name.startswith('Heading')
                if is_heading_style and is_chapter_pattern:
                    self.logger.info(f"æ£€æµ‹åˆ°Headingæ ·å¼çš„ç« èŠ‚æ ‡é¢˜ '{text}'ï¼Œåˆ¤å®šä¸ºæ­£æ–‡å¼€å§‹ï¼Œç›®å½•è§£æç»“æŸ")
                    break

                # å¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œä½œä¸ºæ— é¡µç ç›®å½•é¡¹
                if (has_indent or is_chapter_pattern) and len(text) < 50 and not text.startswith('é¡¹ç›®'):
                    title = text.strip()

                    # â­ï¸ é‡å¤æ£€æµ‹ï¼šå¦‚æœä¸ç¬¬ä¸€é¡¹é‡å¤ï¼Œè¯´æ˜æ‰«æåˆ°æ­£æ–‡äº†
                    # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ä»£æ›¿ä¸¥æ ¼ç›¸ç­‰ï¼Œä»¥åº”å¯¹æ–‡æœ¬ç•¥æœ‰å·®å¼‚çš„æƒ…å†µ
                    if toc_items:
                        first_title = toc_items[0]['title']
                        similarity = self.fuzzy_match_title_v2(title, first_title)
                        if similarity >= 0.70:  # 70%ä»¥ä¸Šç›¸ä¼¼åº¦è®¤ä¸ºæ˜¯é‡å¤
                            self.logger.info(f"æ£€æµ‹åˆ°é‡å¤ç›®å½•é¡¹ï¼ˆä¸ç¬¬1é¡¹ç›¸ä¼¼åº¦{similarity:.0%}ï¼‰ï¼Œç›®å½•è§£æç»“æŸ")
                            self.logger.debug(f"  ç¬¬1é¡¹: '{first_title}' vs å½“å‰: '{title}'")
                            break

                    # â­ï¸ æ•°é‡é™åˆ¶æ£€æŸ¥
                    if len(toc_items) >= MAX_TOC_ITEMS:
                        self.logger.info(f"ç›®å½•é¡¹å·²è¾¾ä¸Šé™({MAX_TOC_ITEMS})ï¼Œåœæ­¢è§£æ")
                        break

                    page_num = -1  # æ ‡è®°ä¸ºæ— é¡µç 
                    level = 1  # åˆå§‹å€¼ï¼Œåç»­ç”± LevelAnalyzer ä¿®æ­£

                    toc_items.append({
                        'title': title,
                        'page_num': page_num,
                        'level': level
                    })

                    self.logger.debug(f"ç›®å½•é¡¹(æ— é¡µç ) [{level}çº§]: {title}")
                    toc_end_idx = i
                    consecutive_non_toc = 0
                    continue  # æˆåŠŸè¯†åˆ«ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

                # éç›®å½•é¡¹
                consecutive_non_toc += 1
                # è¿ç»­5è¡Œä¸åŒ¹é…ï¼Œè®¤ä¸ºç›®å½•ç»“æŸ
                if consecutive_non_toc >= 5 and len(toc_items) > 0:
                    self.logger.info(f"ç›®å½•è§£æå®Œæˆï¼Œå…± {len(toc_items)} é¡¹ï¼Œç»“æŸäºæ®µè½ {toc_end_idx}")
                    break
                # æ–°å¢ï¼šå¦‚æœæ‰¾åˆ°äº†ç›®å½•é¡¹ä½†é‡åˆ°ç©ºè¡Œåçš„Headingæ ·å¼ï¼Œè®¤ä¸ºç›®å½•ç»“æŸ
                if (len(toc_items) > 0 and consecutive_non_toc >= 2 and
                    para.style and para.style.name.startswith('Heading')):
                    self.logger.info(f"æ£€æµ‹åˆ°Headingæ ·å¼ï¼Œç›®å½•ç»“æŸäºæ®µè½ {toc_end_idx}")
                    break

        # æ”¹è¿›3ï¼šç¡®ä¿ toc_end_idx ä¸¥æ ¼å¤§äº toc_start_idx
        if toc_end_idx == toc_start_idx:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›®å½•é¡¹ï¼Œè‡³å°‘å‘åç§»åŠ¨1ä¸ªæ®µè½
            toc_end_idx = toc_start_idx + 1
            self.logger.warning(f"æœªè§£æåˆ°ç›®å½•é¡¹ï¼Œå°†ç›®å½•ç»“æŸä½ç½®è®¾ä¸º {toc_end_idx}ï¼ˆé¿å…é€»è¾‘é”™è¯¯ï¼‰")

        # ä½¿ç”¨å±‚çº§åˆ†æå™¨ä¿®æ­£å±‚çº§ï¼ˆåŸºäºç»Ÿè®¡é¢‘ç‡çš„ä¸Šä¸‹æ–‡åˆ†æï¼‰
        if toc_items:
            self.logger.info(f"âš¡ï¸ å¼€å§‹ä½¿ç”¨contextualæ–¹æ³•åˆ†æ {len(toc_items)} ä¸ªTOCé¡¹çš„å±‚çº§")
            analyzer = LevelAnalyzer()
            corrected_levels = analyzer.analyze_toc_hierarchy_contextual(toc_items)
            for i, level in enumerate(corrected_levels):
                toc_items[i]['level'] = level

            from collections import Counter
            level_dist = Counter(corrected_levels)
            self.logger.info(f"ä½¿ç”¨contextualæ–¹æ³•ä¿®æ­£å±‚çº§ï¼Œåˆ†å¸ƒ: {dict(level_dist)}")

        # ğŸ†• è½»é‡çº§åˆåŒæ½œåœ¨æ ‡è®°ï¼ˆåŸºäºæ ‡é¢˜å…³é”®è¯ï¼‰
        if toc_items:
            # æ’é™¤è§„åˆ™ï¼šè¿™äº›ç« èŠ‚ä¸åº”è¢«æ ‡è®°ä¸ºåˆåŒ
            exclude_keywords = [
                "æŠ•æ ‡äººé¡»çŸ¥", "ä¾›åº”å•†é¡»çŸ¥", "æŠ•æ ‡é‚€è¯·", "æŠ•æ ‡é¡»çŸ¥", "æ‹›æ ‡é¡»çŸ¥",
                "é™„ä»¶", "æŠ•æ ‡æ–‡ä»¶æ ¼å¼", "å“åº”æ–‡ä»¶æ ¼å¼", "æŠ•æ ‡æ–‡ä»¶ç»„æˆ", "è°ˆåˆ¤å“åº”æ–‡ä»¶æ ¼å¼",
                "ç”¨æˆ·éœ€æ±‚", "éœ€æ±‚ä¹¦", "æŠ€æœ¯è¦æ±‚", "æŠ€æœ¯è§„æ ¼", "æŠ€æœ¯éœ€æ±‚ä¹¦", "æŠ€æœ¯éœ€æ±‚",
                "è¯„åˆ†", "è¯„å®¡", "è¯„æ ‡", "å¼€æ ‡", "æŠ¥ä»·", "æŠ•æ ‡æŠ¥ä»·",
                "æŠ•æ ‡é‚€è¯·å‡½", "é‡‡è´­é‚€è¯·", "è°ˆåˆ¤é‚€è¯·"
            ]

            # åˆåŒå…³é”®è¯ï¼šåªæœ‰åŒ…å«è¿™äº›å…³é”®è¯æ‰æ ‡è®°ä¸ºæ½œåœ¨åˆåŒ
            contract_keywords = [
                "åˆåŒæ¡æ¬¾", "åˆåŒæ–‡æœ¬", "åˆåŒèŒƒæœ¬", "åˆåŒåè®®", "åˆåŒæ ¼å¼",
                "é€šç”¨æ¡æ¬¾", "ä¸“ç”¨æ¡æ¬¾", "æ‹Ÿç­¾åˆåŒ", "åˆåŒè‰ç¨¿", "åˆåŒä¸»è¦æ¡æ¬¾"
            ]

            contract_potential_count = 0
            for item in toc_items:
                title = item['title']

                # æ’é™¤è§„åˆ™ä¼˜å…ˆ
                is_excluded = any(kw in title for kw in exclude_keywords)
                is_contract_potential = any(kw in title for kw in contract_keywords)

                # åªæœ‰åœ¨ä¸è¢«æ’é™¤ä¸”åŒ…å«åˆåŒå…³é”®è¯æ—¶æ‰æ ‡è®°
                item['is_contract_potential'] = is_contract_potential and not is_excluded

                if item['is_contract_potential']:
                    contract_potential_count += 1
                    self.logger.debug(f"  ğŸ·ï¸  æ½œåœ¨åˆåŒç« èŠ‚: '{title}'")
                elif is_excluded:
                    self.logger.debug(f"  â›” æ’é™¤ç« èŠ‚: '{title}' (åŒ¹é…æ’é™¤è§„åˆ™)")

            if contract_potential_count > 0:
                self.logger.info(f"ğŸ·ï¸  æ ‡è®°äº† {contract_potential_count} ä¸ªæ½œåœ¨åˆåŒç« èŠ‚")

        return toc_items, toc_end_idx

    def _find_paragraph_by_title(self, doc: Document, title: str, start_idx: int = 0, toc_items: Optional[List[Dict]] = None, toc_end_idx: Optional[int] = None) -> Optional[int]:
        """
        åœ¨æ–‡æ¡£ä¸­æœç´¢ä¸æ ‡é¢˜åŒ¹é…çš„æ®µè½

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            title: è¦æœç´¢çš„æ ‡é¢˜æ–‡æœ¬
            start_idx: å¼€å§‹æœç´¢çš„æ®µè½ç´¢å¼•
            toc_items: ç›®å½•é¡¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºæ™ºèƒ½æ£€æµ‹å…ƒæ•°æ®åˆ—è¡¨
            toc_end_idx: ç›®å½•ç»“æŸçš„æ®µè½ç´¢å¼•ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºé™åˆ¶å…ƒæ•°æ®æ£€æµ‹ä¸æ‰«æç›®å½•åŒºåŸŸ

        Returns:
            æ®µè½ç´¢å¼•ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        def aggressive_normalize(text: str) -> str:
            """æ¿€è¿›æ–‡æœ¬è§„èŒƒåŒ–ï¼šç§»é™¤æ‰€æœ‰åˆ†éš”ç¬¦ã€å‰ç¼€ã€ç©ºæ ¼"""
            # ç§»é™¤"é™„ä»¶-"ã€"é™„ä»¶:"ç­‰å‰ç¼€
            text = re.sub(r'^é™„ä»¶[-:ï¼š]?', '', text)
            # ç§»é™¤è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿ã€åˆ¶è¡¨ç¬¦
            text = re.sub(r'[-_\t]+', '', text)
            # ç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œå†’å·ï¼ˆå°†"ç¬¬ä¸€ç« ï¼š"å’Œ"ç¬¬ä¸€ç«  "è§†ä¸ºç­‰ä»·ï¼‰
            text = re.sub(r'[:ï¼š\s]+', '', text)
            return text

        def extract_core_keywords(text: str) -> str:
            """æå–æ ¸å¿ƒå…³é”®è¯ï¼šå»é™¤ç¼–å·å’Œå¸¸è§å‰ç¼€"""
            # ç§»é™¤ç¼–å·ï¼ˆæ”¯æŒå†’å·å’Œç©ºæ ¼ï¼šç¬¬Xç« ï¼šã€ç¬¬Xç«  ã€ç¬¬Xç« ã€ç¬¬Xéƒ¨åˆ†ï¼šç­‰ï¼‰
            text = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« )[:ï¼š\s]*', '', text)
            text = re.sub(r'^(\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', text)
            # ç§»é™¤"é™„ä»¶"å‰ç¼€
            text = re.sub(r'^é™„ä»¶[-:ï¼š]?', '', text)
            # ç§»é™¤åˆ†éš”ç¬¦
            text = re.sub(r'[-_\t]+', '', text)
            # ç§»é™¤ç©ºæ ¼å’Œå†’å·
            text = re.sub(r'[:ï¼š\s]+', '', text)
            return text

        def calculate_similarity(str1: str, str2: str) -> float:
            """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ SequenceMatcherï¼‰"""
            if not str1 or not str2:
                return 0.0

            # ä½¿ç”¨ SequenceMatcher è®¡ç®—çœŸæ­£çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            # è¿™æ¯”åŸºäºå­ä¸²çš„æ–¹æ³•æ›´èƒ½å¤„ç†åŒä¹‰è¯æ›¿æ¢ï¼ˆå¦‚"å“åº”"vs"åº”ç­”"ï¼‰
            return SequenceMatcher(None, str1, str2).ratio()

        # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤å¤šä½™ç©ºæ ¼ï¼‰
        clean_title = re.sub(r'\s+', '', title)

        # æ¿€è¿›è§„èŒƒåŒ–çš„æ ‡é¢˜
        aggressive_title = aggressive_normalize(title)

        # æå–æ ¸å¿ƒå…³é”®è¯
        core_keywords = extract_core_keywords(aggressive_title)

        self.logger.info(f"æœç´¢æ ‡é¢˜: '{title}' (æ¸…ç†å: '{clean_title}', æ ¸å¿ƒ: '{core_keywords}'), ä»æ®µè½ {start_idx} å¼€å§‹")

        # å€™é€‰åŒ¹é…åˆ—è¡¨ï¼ˆç”¨äºå®½æ¾åŒ¹é…çº§åˆ«ï¼‰
        # æ ¼å¼: [(æ®µè½ç´¢å¼•, åŒ¹é…çº§åˆ«, å¾—åˆ†, æ®µè½æ–‡æœ¬, åŒ¹é…åŸå› )]
        loose_match_candidates = []

        # è®°å½•å·²è·³è¿‡çš„å…ƒæ•°æ®åˆ—è¡¨åŒºåŸŸï¼ˆé¿å…é‡å¤æ£€æµ‹å’Œæ—¥å¿—ï¼‰
        skipped_ranges = []

        # ğŸ”‘ ç¬¬ä¸€è½®ï¼šä¸¥æ ¼åŒ¹é… (Level 1-3)ï¼Œæ‰¾åˆ°ç«‹å³è¿”å›
        for i in range(start_idx, len(doc.paragraphs)):
            # æ£€æŸ¥æ˜¯å¦åœ¨å·²è·³è¿‡çš„åŒºåŸŸä¸­
            if any(start <= i <= end for start, end in skipped_ranges):
                continue
            para = doc.paragraphs[i]
            para_text = para.text.strip()

            if not para_text:
                continue

            # æ¸…ç†æ®µè½æ–‡æœ¬
            clean_para = re.sub(r'\s+', '', para_text)

            # æ¿€è¿›è§„èŒƒåŒ–çš„æ®µè½
            aggressive_para = aggressive_normalize(para_text)

            # æ®µè½æ ¸å¿ƒå…³é”®è¯
            para_keywords = extract_core_keywords(aggressive_para)

            # Level 1: å®Œå…¨åŒ¹é…æˆ–åŒ…å«åŒ¹é…
            level1_exact_match = (clean_title == clean_para)
            level1_contain_match = (clean_title in clean_para)

            if level1_exact_match or level1_contain_match:
                # â­ï¸ æ£€æŸ¥æ˜¯å¦åœ¨è¿ç»­ç« èŠ‚æ ‡é¢˜åˆ—è¡¨ä¸­ï¼ˆå¦‚"æ–‡ä»¶æ„æˆè¯´æ˜"ï¼‰
                list_range = self._detect_chapter_title_list_range(doc, i, toc_items, toc_end_idx)

                if list_range:
                    start, end, titles = list_range

                    # ğŸ”‘ å…³é”®ï¼šå¯¹äº"ç¬¬Xç« "ã€"ç¬¬Xéƒ¨åˆ†"è¿™ç§æ˜ç¡®çš„ä¸€çº§ç« èŠ‚æ ‡é¢˜
                    # åªæœ‰å½“å®ƒå…·æœ‰ Heading æ ·å¼æ—¶æ‰è®¤ä¸ºæ˜¯çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜
                    # å¦åˆ™å¯èƒ½åªæ˜¯æ–‡æ¡£ä¸­çš„ä¸€ä¸ªç« èŠ‚åˆ—è¡¨/ç´¢å¼•åŒºåŸŸ
                    is_primary_chapter = re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+[ç« éƒ¨åˆ†]', para_text.strip())
                    is_heading_style = para.style and ('heading' in para.style.name.lower() or 'æ ‡é¢˜' in para.style.name.lower())

                    if is_primary_chapter and is_heading_style:
                        self.logger.info(f"  âœ“ æ‰¾åˆ°ä¸€çº§ç« èŠ‚æ ‡é¢˜ï¼ˆHeadingæ ·å¼ï¼Œä¸è·³è¿‡ï¼‰: æ®µè½ {i}: '{para_text}'")
                        return i

                    # å¦‚æœæ˜¯"ç¬¬Xç« "ä½†ä¸æ˜¯Headingæ ·å¼ï¼Œå¾ˆå¯èƒ½æ˜¯æ–‡æ¡£ä¸­çš„ç« èŠ‚åˆ—è¡¨åŒºåŸŸ
                    if is_primary_chapter and not is_heading_style:
                        self.logger.info(f"  â­ï¸  è·³è¿‡æ®µè½ {i}ï¼ˆç¬¬Xç« æ ¼å¼ä½†éHeadingæ ·å¼ï¼Œå¯èƒ½æ˜¯ç« èŠ‚åˆ—è¡¨ï¼‰: '{para_text}'")
                        # è®°å½•è·³è¿‡åŒºåŸŸ
                        skipped_ranges.append((start, end))
                        continue

                    self.logger.info(
                        f"  âš ï¸  æ£€æµ‹åˆ°'æ–‡ä»¶æ„æˆè¯´æ˜'åˆ—è¡¨ (æ®µè½{start}-{end}ï¼Œ"
                        f"åŒ…å«{len(titles)}ä¸ªç« èŠ‚æ ‡é¢˜)ï¼Œè·³è¿‡è¯¥åŒºåŸŸ"
                    )
                    # æ˜¾ç¤ºåˆ—è¡¨ä¸­çš„ç« èŠ‚ï¼ˆæœ€å¤š5ä¸ªï¼‰
                    for idx, title_text in titles[:5]:
                        self.logger.info(f"      - æ®µè½{idx}: {title_text}")
                    if len(titles) > 5:
                        self.logger.info(f"      ... è¿˜æœ‰{len(titles)-5}ä¸ª")

                    # è®°å½•è·³è¿‡åŒºåŸŸ
                    skipped_ranges.append((start, end))

                    # ç»§ç»­æœç´¢ï¼ˆä¸è¿”å›æ­¤åŒ¹é…ï¼‰
                    continue

                # ğŸ”‘ å¯¹äºåŒ…å«åŒ¹é…ï¼ˆéå®Œå…¨åŒ¹é…ï¼‰ï¼Œéœ€è¦é¢å¤–éªŒè¯
                if level1_contain_match and not level1_exact_match:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯Headingæ ·å¼ï¼ˆæ›´å¯èƒ½æ˜¯çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ï¼‰
                    is_heading = para.style and ('heading' in para.style.name.lower() or 'æ ‡é¢˜' in para.style.name.lower())
                    # æˆ–è€…æ–‡æœ¬å¾ˆçŸ­ï¼ˆâ‰¤20å­—ï¼Œæ›´å¯èƒ½æ˜¯æ ‡é¢˜è€Œä¸æ˜¯æ­£æ–‡ï¼‰
                    is_short = len(para_text) <= 20
                    # ğŸ†• æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åœ¨æ®µè½å¼€å¤´ä½ç½®ï¼ˆç« èŠ‚æ ‡é¢˜åº”è¯¥åœ¨å¼€å¤´ï¼Œä¸æ˜¯ä¸­é—´ï¼‰
                    title_at_start = clean_para.startswith(clean_title)

                    if not is_heading:
                        # é Heading æ ·å¼æ—¶ï¼Œéœ€è¦æ»¡è¶³ï¼šçŸ­æ–‡æœ¬ æˆ– æ ‡é¢˜åœ¨å¼€å¤´
                        if not (is_short or title_at_start):
                            # æ ‡é¢˜åœ¨æ®µè½ä¸­é—´ â†’ å¯èƒ½æ˜¯æ­£æ–‡ä¸­æåˆ°æ ‡é¢˜ â†’ è·³è¿‡
                            self.logger.debug(f"  â­ï¸  è·³è¿‡æ®µè½ {i}ï¼ˆåŒ…å«åŒ¹é…ä½†æ ‡é¢˜ä¸åœ¨å¼€å¤´ä¸”éçŸ­æ–‡æœ¬ï¼‰: '{para_text[:60]}'")
                            continue

                self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 1-å®Œå…¨): æ®µè½ {i}: '{para_text}'")
                return i

            # Level 2: æ¿€è¿›è§„èŒƒåŒ–åçš„å®Œå…¨åŒ¹é…æˆ–åŒ…å«åŒ¹é…
            level2_exact_match = (aggressive_title == aggressive_para)
            level2_contain_match = (aggressive_title in aggressive_para)

            if level2_exact_match or level2_contain_match:
                # ğŸ”‘ å¯¹äºåŒ…å«åŒ¹é…ï¼ˆéå®Œå…¨åŒ¹é…ï¼‰ï¼Œéœ€è¦é¢å¤–éªŒè¯
                if level2_contain_match and not level2_exact_match:
                    is_heading = para.style and ('heading' in para.style.name.lower() or 'æ ‡é¢˜' in para.style.name.lower())
                    is_short = len(para_text) <= 20
                    # ğŸ†• æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åœ¨æ®µè½å¼€å¤´ä½ç½®ï¼ˆç« èŠ‚æ ‡é¢˜åº”è¯¥åœ¨å¼€å¤´ï¼Œä¸æ˜¯ä¸­é—´ï¼‰
                    title_at_start = aggressive_para.startswith(aggressive_title)

                    if not is_heading:
                        # é Heading æ ·å¼æ—¶ï¼Œéœ€è¦æ»¡è¶³ï¼šçŸ­æ–‡æœ¬ æˆ– æ ‡é¢˜åœ¨å¼€å¤´
                        if not (is_short or title_at_start):
                            # æ ‡é¢˜åœ¨æ®µè½ä¸­é—´ â†’ å¯èƒ½æ˜¯æ­£æ–‡ä¸­æåˆ°æ ‡é¢˜ â†’ è·³è¿‡
                            self.logger.debug(f"  â­ï¸  è·³è¿‡æ®µè½ {i}ï¼ˆåŒ…å«åŒ¹é…ä½†æ ‡é¢˜ä¸åœ¨å¼€å¤´ä¸”éçŸ­æ–‡æœ¬ï¼‰: '{para_text[:60]}'")
                            continue

                self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 2-è§„èŒƒåŒ–): æ®µè½ {i}: '{para_text}'")
                return i

            # æ£€æŸ¥æ ‡é¢˜å’Œæ®µè½æ˜¯å¦åŒ…å«"ç¬¬Xéƒ¨åˆ†"ï¼ˆç”¨äºLevel 3å’ŒLevel 4çº¦æŸï¼‰
            title_has_part_number = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', title))
            para_has_part_number = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', para_text))

            # Level 3: å»é™¤ç¼–å·åçš„åŒ¹é…
            # æ”¯æŒå¤šç§ç¼–å·æ ¼å¼ï¼šç¬¬Xéƒ¨åˆ†ã€ç¬¬Xç« ã€1.ã€1.1ã€ä¸€ã€ç­‰
            title_without_number = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', clean_title)
            para_without_number = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', clean_para)

            if title_without_number and para_without_number and title_without_number == para_without_number:
                # å¦‚æœTOCæ ‡é¢˜æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼Œåˆ™æ®µè½ä¹Ÿå¿…é¡»æœ‰"ç¬¬Xéƒ¨åˆ†"ï¼ˆé¿å…åŒ¹é…åˆ°TOCå†…çš„ç¼–å·å†…å®¹ï¼‰
                # ğŸ†• ä¾‹å¤–ï¼šå¦‚æœæ­£æ–‡æ®µè½ä½¿ç”¨ Heading æ ·å¼ï¼Œè¯´æ˜æ˜¯çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ï¼Œå³ä½¿æ²¡æœ‰"ç¬¬Xéƒ¨åˆ†"å‰ç¼€ä¹Ÿåº”åŒ¹é…
                is_heading = para.style and ('heading' in para.style.name.lower() or 'æ ‡é¢˜' in para.style.name.lower())
                if title_has_part_number and not para_has_part_number:
                    if is_heading:
                        # æ­£æ–‡ä½¿ç”¨ Heading æ ·å¼ï¼Œè§†ä¸ºæœ‰æ•ˆçš„ç« èŠ‚æ ‡é¢˜ï¼Œå¯ä»¥åŒ¹é…
                        self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 3-å»ç¼–å·+Headingæ ·å¼): æ®µè½ {i}: '{para_text}'")
                        return i
                    else:
                        pass  # è·³è¿‡ï¼Œä¸åŒ¹é…
                else:
                    self.logger.info(f"  âœ“ æ‰¾åˆ°åŒ¹é… (Level 3-å»ç¼–å·): æ®µè½ {i}: '{para_text}'")
                    return i

        # ğŸ”‘ ç¬¬äºŒè½®ï¼šå®½æ¾åŒ¹é… (Level 4-7)ï¼Œæ”¶é›†æ‰€æœ‰å€™é€‰
        self.logger.info(f"  ä¸¥æ ¼åŒ¹é… (Level 1-3) æœªæ‰¾åˆ°ï¼Œå¼€å§‹å®½æ¾åŒ¹é… (Level 4-7)")

        for i in range(start_idx, len(doc.paragraphs)):
            # æ£€æŸ¥æ˜¯å¦åœ¨å·²è·³è¿‡çš„åŒºåŸŸä¸­
            if any(start <= i <= end for start, end in skipped_ranges):
                continue
            para = doc.paragraphs[i]
            para_text = para.text.strip()

            if not para_text:
                continue

            # æ¸…ç†æ®µè½æ–‡æœ¬
            clean_para = re.sub(r'\s+', '', para_text)

            # æ¿€è¿›è§„èŒƒåŒ–çš„æ®µè½
            aggressive_para = aggressive_normalize(para_text)

            # æ®µè½æ ¸å¿ƒå…³é”®è¯
            para_keywords = extract_core_keywords(aggressive_para)

            # æ£€æŸ¥æ ‡é¢˜å’Œæ®µè½æ˜¯å¦åŒ…å«"ç¬¬Xéƒ¨åˆ†"
            title_has_part_number = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', title))
            para_has_part_number = bool(re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†', para_text))

            # å»é™¤ç¼–å·
            title_without_number = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€)\s*', '', clean_title)
            para_without_number = re.sub(r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+éƒ¨åˆ†|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |\d+\.|\d+\.\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ã€)\s*', '', clean_para)

            # Level 4: æ ¸å¿ƒå…³é”®è¯åŒ¹é…ï¼ˆé•¿åº¦â‰¥4å­—ï¼‰
            if len(core_keywords) >= 4 and len(para_keywords) >= 4:
                # å®Œå…¨ç›¸ç­‰åŒ¹é…ï¼ˆå¾—åˆ†æœ€é«˜ï¼‰
                if core_keywords == para_keywords:
                    loose_match_candidates.append((i, 4, 100, para_text, f"å…³é”®è¯å®Œå…¨ç›¸ç­‰: '{core_keywords}'"))
                # æ ‡é¢˜å…³é”®è¯åŒ…å«æ®µè½å…³é”®è¯ï¼ˆå¾—åˆ†ä¸­ç­‰ï¼‰
                elif core_keywords in para_keywords:
                    loose_match_candidates.append((i, 4, 70, para_text, f"å…³é”®è¯åŒ…å«: '{core_keywords}' in '{para_keywords}'"))
                # æ®µè½å…³é”®è¯åŒ…å«æ ‡é¢˜å…³é”®è¯ï¼ˆå¾—åˆ†è¾ƒä½ï¼Œå®¹æ˜“è¯¯åŒ¹é…ï¼‰
                elif para_keywords in core_keywords:
                    loose_match_candidates.append((i, 4, 50, para_text, f"è¢«åŒ…å«: '{para_keywords}' in '{core_keywords}'"))

            # Level 4.5: éƒ¨åˆ†å­ä¸²åŒ¹é…
            if len(core_keywords) >= 6 and title_has_part_number:
                for substr_len in range(len(core_keywords), 5, -1):
                    substr = core_keywords[:substr_len]
                    if substr in para_keywords and len(substr) >= 6:
                        if para_has_part_number and len(para_text) <= 50:
                            match_ratio = len(substr) / len(core_keywords)
                            score = 65 + match_ratio * 10  # 65-75åˆ†
                            loose_match_candidates.append((i, 4.5, score, para_text, f"éƒ¨åˆ†å­ä¸²{match_ratio:.0%}: '{substr}'"))
                        break

            # Level 5: ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆç›¸ä¼¼åº¦â‰¥60%ï¼‰
            if len(core_keywords) >= 4:
                similarity = calculate_similarity(core_keywords, para_keywords)
                if similarity >= 0.6:
                    # ğŸ†• æ£€æŸ¥ç« èŠ‚ç¼–å·æ˜¯å¦åŒ¹é…ï¼ˆå¦‚"ç¬¬ä¸‰ç« "ï¼‰
                    title_chapter_match = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ç« ', title)
                    para_chapter_match = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ç« ', para_text)

                    if title_chapter_match and para_chapter_match:
                        # ä¸¤è€…éƒ½æœ‰ç« èŠ‚ç¼–å·
                        if title_chapter_match.group(1) == para_chapter_match.group(1):
                            # ç« èŠ‚ç¼–å·ç›¸åŒï¼ˆå¦‚éƒ½æ˜¯"ç¬¬ä¸‰ç« "ï¼‰ï¼Œå¤§å¹…æé«˜åˆ†æ•°
                            # è¿™ç§æƒ…å†µä¸‹ï¼Œå³ä½¿å†…å®¹ç•¥æœ‰ä¸åŒï¼ˆå¦‚"å“åº”"vs"åº”ç­”"ï¼‰ï¼Œä¹Ÿåº”è¯¥ä¼˜å…ˆåŒ¹é…
                            score = 80 + similarity * 20  # 80-100åˆ†
                            loose_match_candidates.append((i, 5, score, para_text, f"ç« èŠ‚ç¼–å·åŒ¹é…+ç›¸ä¼¼åº¦{similarity:.0%}"))
                        else:
                            # ç« èŠ‚ç¼–å·ä¸åŒï¼Œé™ä½åˆ†æ•°
                            score = similarity * 30  # 18-30åˆ†
                            loose_match_candidates.append((i, 5, score, para_text, f"ç›¸ä¼¼åº¦{similarity:.0%}(ç¼–å·ä¸åŒ)"))
                    else:
                        # æ™®é€šç›¸ä¼¼åº¦åŒ¹é…
                        score = similarity * 60  # 36-60åˆ†
                        loose_match_candidates.append((i, 5, score, para_text, f"ç›¸ä¼¼åº¦{similarity:.0%}"))

            # Level 6: å®½æ¾å…³é”®è¯åŒ¹é…ï¼ˆè‡³å°‘6å­—æ ‡é¢˜ï¼‰
            if len(title_without_number) >= 6:
                if title_without_number in clean_para:
                    loose_match_candidates.append((i, 6, 40, para_text, f"åŒ…å«å»ç¼–å·æ ‡é¢˜: '{title_without_number}'"))

            # Level 7: è½¬æ¢ç¼–å·ååŒ¹é…
            def convert_chinese_to_number(text):
                """å°†ç¬¬ä¸€/ç¬¬äºŒ/ç¬¬ä¸‰ç­‰è½¬æ¢ä¸º1/2/3"""
                mapping = {'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
                          'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10'}
                match = re.match(r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)éƒ¨åˆ†(.*)$', text)
                if match:
                    num = mapping.get(match.group(1), match.group(1))
                    return f"{num}.{match.group(2)}"
                return text

            converted_title = convert_chinese_to_number(clean_title)
            if converted_title != clean_title and clean_para.startswith(converted_title[:3]):
                converted_para_without_num = re.sub(r'^\d+\.', '', clean_para)
                converted_title_without_num = re.sub(r'^\d+\.', '', converted_title)
                if converted_title_without_num == converted_para_without_num:
                    loose_match_candidates.append((i, 7, 30, para_text, f"è½¬æ¢ç¼–å·ååŒ¹é…"))

        # ğŸ”‘ ä»å€™é€‰ä¸­é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        if loose_match_candidates:
            # æŒ‰å¾—åˆ†æ’åº
            loose_match_candidates.sort(key=lambda x: x[2], reverse=True)
            best = loose_match_candidates[0]
            para_idx, level, score, para_text, reason = best

            self.logger.info(f"  âœ“ ä» {len(loose_match_candidates)} ä¸ªå®½æ¾å€™é€‰ä¸­é€‰æ‹©æœ€ä½³åŒ¹é…:")
            self.logger.info(f"     æ®µè½ {para_idx} (Level {level}, å¾—åˆ†{score:.0f}): '{para_text[:60]}'")
            self.logger.info(f"     åŒ¹é…åŸå› : {reason}")

            # æ˜¾ç¤ºå…¶ä»–å€™é€‰ï¼ˆå‰3ä¸ªï¼‰
            if len(loose_match_candidates) > 1:
                self.logger.info(f"  å…¶ä»–å€™é€‰:")
                for candidate in loose_match_candidates[1:4]:
                    c_idx, c_level, c_score, c_text, c_reason = candidate
                    self.logger.info(f"     æ®µè½ {c_idx} (Level {c_level}, å¾—åˆ†{c_score:.0f}): '{c_text[:60]}'")

            return para_idx

        # æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…
        self.logger.warning(f"æœªæ‰¾åˆ°æ ‡é¢˜åŒ¹é…: '{title}'")
        return None
    def _locate_chapters_by_toc(self, doc: Document, toc_items: List[Dict], toc_end_idx: int) -> List[ChapterNode]:
        """
        æ ¹æ®ç›®å½•é¡¹å®šä½ç« èŠ‚åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®ï¼Œå¹¶æ„å»ºæ ‘å½¢ç»“æ„

        æ ¸å¿ƒæ”¹è¿›: å®Œå…¨ä½¿ç”¨TOCä¸­å·²è¯†åˆ«çš„å±‚çº§ä¿¡æ¯ï¼Œä¸å†é‡æ–°æ£€æµ‹å­ç« èŠ‚
        è¿™æ ·å¯ä»¥ä¿ç•™ analyze_toc_hierarchy_contextual() çš„ç²¾ç¡®åˆ†æç»“æœ

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            toc_items: ç›®å½•é¡¹åˆ—è¡¨ï¼ˆå·²åŒ…å«ç²¾ç¡®çš„å±‚çº§ä¿¡æ¯ï¼‰
            toc_end_idx: ç›®å½•ç»“æŸçš„æ®µè½ç´¢å¼•

        Returns:
            ç« èŠ‚æ ‘ï¼ˆåªåŒ…å«æ ¹èŠ‚ç‚¹ï¼Œå­èŠ‚ç‚¹åœ¨childrenå±æ€§ä¸­ï¼‰
        """
        # æ­¥éª¤1: å®šä½æ‰€æœ‰TOCé¡¹åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
        all_chapters = []
        last_found_idx = toc_end_idx + 1
        self.logger.info(f"â­ å¼€å§‹å®šä½ {len(toc_items)} ä¸ªTOCé¡¹çš„ä½ç½®ï¼ˆç›®å½•ç»“æŸäºæ®µè½ {toc_end_idx}ï¼‰")

        for i, item in enumerate(toc_items):
            title = item['title']
            level = item['level']

            # åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾æ ‡é¢˜ä½ç½®ï¼ˆä¼ å…¥toc_itemså’Œtoc_end_idxç”¨äºæ™ºèƒ½æ£€æµ‹å…ƒæ•°æ®åˆ—è¡¨ï¼‰
            para_idx = self._find_paragraph_by_title(doc, title, last_found_idx, toc_items, toc_end_idx)

            if para_idx is None:
                self.logger.warning(f"âš ï¸  æœªæ‰¾åˆ°ç›®å½•é¡¹: [{level}çº§] {title}")
                continue

            # è®°å½•æ‰¾åˆ°çš„ç« èŠ‚ï¼ˆæš‚ä¸ç¡®å®šç»“æŸä½ç½®ï¼‰
            all_chapters.append({
                'toc_index': i,
                'title': title,
                'level': level,
                'para_idx': para_idx,
                'para_end_idx': None  # ç¨åè®¡ç®—
            })

            last_found_idx = para_idx + 1
            self.logger.debug(f"  âœ“ æ‰¾åˆ° [{level}çº§] {title} (æ®µè½ {para_idx})")

        # æ­¥éª¤2: è®¡ç®—æ¯ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®
        # â­ å…³é”®ä¿®å¤ï¼šé‡‡ç”¨"åŒçº§æˆ–æ›´é«˜çº§"é€»è¾‘ï¼Œç¡®ä¿çˆ¶ç« èŠ‚åŒ…å«æ‰€æœ‰å­ç« èŠ‚çš„å†…å®¹
        for i, chapter_info in enumerate(all_chapters):
            current_level = chapter_info['level']
            next_start = len(doc.paragraphs)  # é»˜è®¤åˆ°æ–‡æ¡£æœ«å°¾

            # æ‰¾ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§çš„ç« èŠ‚ï¼ˆä¸æ–¹æ³•2é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
            for j in range(i + 1, len(all_chapters)):
                if all_chapters[j]['level'] <= current_level:
                    next_start = all_chapters[j]['para_idx']
                    break

            chapter_info['para_end_idx'] = next_start - 1

        # ğŸ†• æ­¥éª¤2.5: é‡å‹åˆåŒæ£€æµ‹ï¼ˆä»…å¤„ç†æ ‡è®°ä¸ºæ½œåœ¨åˆåŒçš„ç« èŠ‚ï¼‰
        contract_chapters_to_insert = []  # å­˜å‚¨éœ€è¦æ’å…¥çš„åˆåŒç« èŠ‚
        contract_check_count = 0
        contract_confirmed_count = 0

        for i, chapter_info in enumerate(all_chapters):
            toc_item = toc_items[chapter_info['toc_index']]

            # ğŸ¯ åªå¯¹æ ‡è®°ä¸ºæ½œåœ¨åˆåŒçš„ç« èŠ‚æ‰§è¡Œé‡å‹æ£€æµ‹
            if toc_item.get('is_contract_potential', False):
                contract_check_count += 1
                self.logger.info(f"  ğŸ” å¯¹æ½œåœ¨åˆåŒç« èŠ‚æ‰§è¡Œé‡å‹æ£€æµ‹: '{chapter_info['title']}'")

                para_idx = chapter_info['para_idx']
                para_end_idx = chapter_info['para_end_idx']

                # é‡å‹ä½œä¸š1: æå–å®Œæ•´å†…å®¹æ ·æœ¬ï¼ˆ2000å­—ï¼‰
                content_sample = self._extract_content_sample(
                    doc, para_idx, para_end_idx, sample_size=2000
                )

                # é‡å‹ä½œä¸š2: è®¡ç®—ç²¾ç¡®çš„åˆåŒå¯†åº¦
                is_contract, density, reason = self._is_contract_chapter(
                    chapter_info['title'],
                    content_sample
                )

                if is_contract:
                    contract_confirmed_count += 1
                    chapter_info['is_contract_confirmed'] = True
                    self.logger.info(f"  âœ“ åˆåŒç« èŠ‚ç¡®è®¤: '{chapter_info['title']}' - {reason}")

                    # é‡å‹ä½œä¸š3: æ£€æµ‹åˆåŒèšé›†åŒºï¼ˆç”¨äºç« èŠ‚æ‹†åˆ†ï¼‰
                    contract_cluster = self._detect_contract_cluster_in_chapter(
                        doc, para_idx, para_end_idx
                    )

                    if contract_cluster:
                        # å æ¯”å·²åœ¨æ£€æµ‹æ–¹æ³•ä¸­é™åˆ¶ï¼ˆ20%-80%ï¼‰ï¼Œè¿™é‡Œç›´æ¥å¤„ç†
                        cluster_start = contract_cluster['start']
                        cluster_end = contract_cluster['end']
                        cluster_ratio = contract_cluster['ratio']

                        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‹†åˆ†æ¡ä»¶
                        min_content_length = 1000
                        min_paragraph_gap = 5

                        if cluster_start > para_idx + min_paragraph_gap:
                            # è®¡ç®—å‰åŠéƒ¨åˆ†å­—æ•°
                            front_content = "\n".join(
                                doc.paragraphs[j].text.strip()
                                for j in range(para_idx + 1, cluster_start)
                                if j < len(doc.paragraphs)
                            )
                            front_word_count = self._calculate_word_count(front_content)

                            if front_word_count >= min_content_length:
                                self.logger.warning(
                                    f"âœ‚ï¸  å‡†å¤‡æ‹†åˆ†ç« èŠ‚: '{chapter_info['title']}' "
                                    f"â†’ æ­£å¸¸éƒ¨åˆ†({para_idx}-{cluster_start-1}, {front_word_count}å­—) "
                                    f"+ åˆåŒéƒ¨åˆ†({cluster_start}-{para_end_idx}, å æ¯”{cluster_ratio:.1%})"
                                )

                                # æˆªæ–­åŸç« èŠ‚ï¼ˆåªä¿ç•™åˆåŒä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                                chapter_info['para_end_idx'] = cluster_start - 1

                                # åˆ›å»ºåˆåŒç« èŠ‚ï¼ˆè®°å½•å¾…æ’å…¥ï¼‰
                                contract_chapter = {
                                    'toc_index': chapter_info['toc_index'] + 0.5,  # æ ‡è®°ä¸ºæ’å…¥çš„ç« èŠ‚
                                    'title': '[æ£€æµ‹åˆ°çš„åˆåŒæ¡æ¬¾-éœ€äººå·¥ç¡®è®¤]',
                                    'level': chapter_info['level'],  # ä¸åŸç« èŠ‚åŒçº§
                                    'para_idx': cluster_start,
                                    'para_end_idx': para_end_idx,
                                    'is_contract_confirmed': True
                                }

                                # è®°å½•å¾…æ’å…¥ä½ç½®ï¼ˆæ’å…¥åˆ°åŸç« èŠ‚åé¢ï¼‰
                                contract_chapters_to_insert.append((i + 1, contract_chapter))
                            else:
                                self.logger.info(
                                    f"  â­ï¸  è·³è¿‡æ‹†åˆ†: '{chapter_info['title']}' - "
                                    f"å‰åŠéƒ¨åˆ†å†…å®¹ä¸è¶³({front_word_count}å­— < {min_content_length}å­—)"
                                )
                        else:
                            self.logger.info(
                                f"  â­ï¸  è·³è¿‡æ‹†åˆ†: '{chapter_info['title']}' - "
                                f"åˆåŒèšé›†åŒºèµ·ç‚¹å¤ªé å‰(æ®µè½{cluster_start})"
                            )
                else:
                    self.logger.info(f"  âœ— éåˆåŒç« èŠ‚: '{chapter_info['title']}' - {reason}")
            else:
                # éæ½œåœ¨åˆåŒç« èŠ‚ï¼Œè·³è¿‡é‡å‹æ£€æµ‹
                self.logger.debug(f"  â­ï¸  è·³è¿‡é‡å‹æ£€æµ‹: '{chapter_info['title']}'")

        # æ’å…¥æ‹†åˆ†å‡ºçš„åˆåŒç« èŠ‚ï¼ˆå€’åºæ’å…¥ï¼Œé¿å…ç´¢å¼•åç§»ï¼‰
        for insert_pos, contract_chapter in reversed(contract_chapters_to_insert):
            all_chapters.insert(insert_pos, contract_chapter)
            self.logger.info(
                f"  â• æ’å…¥åˆåŒç« èŠ‚: '{contract_chapter['title']}' "
                f"(æ®µè½{contract_chapter['para_idx']}-{contract_chapter['para_end_idx']})"
            )

        if contract_check_count > 0:
            self.logger.info(
                f"ğŸ” é‡å‹æ£€æµ‹å®Œæˆ: æ£€æµ‹äº† {contract_check_count} ä¸ªæ½œåœ¨ç« èŠ‚ï¼Œç¡®è®¤ {contract_confirmed_count} ä¸ªåˆåŒç« èŠ‚ï¼Œ"
                f"æ‹†åˆ†å‡º {len(contract_chapters_to_insert)} ä¸ªåˆåŒç« èŠ‚"
            )

        # æ­¥éª¤3: åˆ›å»ºæ‰€æœ‰ChapterNodeå¯¹è±¡
        chapter_nodes = []
        for chapter_info in all_chapters:
            para_idx = chapter_info['para_idx']
            para_end_idx = chapter_info['para_end_idx']

            # æå–ç« èŠ‚å†…å®¹ï¼ˆåŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼ï¼‰
            content_text, preview_text, has_table = self._extract_chapter_content_with_tables(
                doc, para_idx, para_end_idx
            )

            # è®¡ç®—å­—æ•°
            word_count = self._calculate_word_count(content_text)

            # å¦‚æœæ²¡æœ‰é¢„è§ˆæ–‡æœ¬ï¼Œè®¾ç½®é»˜è®¤å€¼
            if not preview_text:
                preview_text = "(æ— å†…å®¹)"

            node = ChapterNode(
                id=f"ch_{chapter_info['toc_index']}",
                level=chapter_info['level'],  # â­ ä½¿ç”¨TOCä¸­çš„å±‚çº§ï¼Œä¸é‡æ–°æ£€æµ‹
                title=chapter_info['title'],
                para_start_idx=para_idx,
                para_end_idx=para_end_idx,
                word_count=word_count,
                preview_text=preview_text,
                has_table=has_table
            )

            chapter_nodes.append(node)

        # æ­¥éª¤4: æ„å»ºæ ‘å½¢ç»“æ„ï¼ˆåŸºäºlevelå±æ€§ï¼‰
        root_chapters = self._build_chapter_tree(chapter_nodes)

        self.logger.info(f"âœ… æˆåŠŸæ„å»ºç« èŠ‚æ ‘: {len(root_chapters)} ä¸ªæ ¹ç« èŠ‚")
        return root_chapters
    def _locate_chapter_content(self, doc: Document, chapters: List[ChapterNode]) -> List[ChapterNode]:
        """
        å®šä½æ¯ä¸ªç« èŠ‚çš„å†…å®¹èŒƒå›´

        Args:
            doc: Word æ–‡æ¡£å¯¹è±¡
            chapters: ç« èŠ‚åˆ—è¡¨

        Returns:
            æ›´æ–°åçš„ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å« para_end_idxã€word_countã€preview_textï¼‰
        """
        # â­ï¸ å…³é”®ä¿®å¤ï¼šæŒ‰æ®µè½ç´¢å¼•æ’åºï¼Œç¡®ä¿ç« èŠ‚é¡ºåºä¸æ–‡æ¡£ç‰©ç†é¡ºåºä¸€è‡´
        # é˜²æ­¢ç´¢å¼•å€’ç½®é—®é¢˜ï¼ˆå¦‚ para_start_idx=542 > para_end_idx=62ï¼‰
        chapters_sorted = sorted(chapters, key=lambda ch: ch.para_start_idx)
        self.logger.info(f"ç« èŠ‚å·²æŒ‰æ®µè½ç´¢å¼•æ’åºï¼Œå…± {len(chapters_sorted)} ä¸ªç« èŠ‚")

        total_paras = len(doc.paragraphs)

        # ğŸ†• ç”¨äºæ”¶é›†éœ€è¦æ’å…¥çš„åˆåŒç« èŠ‚
        contract_chapters_to_insert = []

        for i, chapter in enumerate(chapters_sorted):
            # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜çš„å‰ä¸€ä¸ªæ®µè½ï¼‰
            next_start = total_paras  # é»˜è®¤åˆ°æ–‡æ¡£æœ«å°¾

            for j in range(i + 1, len(chapters_sorted)):
                if chapters_sorted[j].level <= chapter.level:
                    next_start = chapters_sorted[j].para_start_idx
                    break

            chapter.para_end_idx = next_start - 1

            # æå–ç« èŠ‚å†…å®¹ï¼ˆåŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼ï¼‰
            content_text, preview_text, has_table = self._extract_chapter_content_with_tables(
                doc, chapter.para_start_idx, chapter.para_end_idx
            )

            # è®¡ç®—å­—æ•°
            chapter.word_count = self._calculate_word_count(content_text)
            chapter.preview_text = preview_text if preview_text else "(æ— å†…å®¹)"
            chapter.has_table = has_table

            # ã€æ–°å¢ã€‘å¯¹äºlevel 1-2çš„ç« èŠ‚ï¼Œæå–å†…å®¹æ ·æœ¬å¹¶è¿›è¡ŒåˆåŒè¯†åˆ«
            if chapter.level <= 2:
                # æå–å†…å®¹æ ·æœ¬ç”¨äºåˆåŒè¯†åˆ«
                chapter.content_sample = self._extract_content_sample(
                    doc, chapter.para_start_idx, chapter.para_end_idx, sample_size=2000
                )

                # åŸºäºå†…å®¹è¿›è¡ŒåˆåŒè¯†åˆ«
                is_contract, density, reason = self._is_contract_chapter(
                    chapter.title, chapter.content_sample
                )

                if is_contract:
                    self.logger.info(
                        f"  âœ“ åˆåŒç« èŠ‚è¯†åˆ«: '{chapter.title}' - {reason}"
                    )

            # ğŸ†• æ–°å¢ï¼šæ£€æµ‹ç« èŠ‚å†…æ˜¯å¦æœ‰åˆåŒèšé›†åŒºï¼ˆç”¨äºæ‹†åˆ†ç« èŠ‚ï¼‰
            contract_cluster = self._detect_contract_cluster_in_chapter(
                doc, chapter.para_start_idx, chapter.para_end_idx
            )

            if contract_cluster:
                cluster_start = contract_cluster['start']
                cluster_end = contract_cluster['end']
                density = contract_cluster['density']

                # ç¡®ä¿èšé›†åŒºèµ·ç‚¹åœ¨ç« èŠ‚å†…ä¸”æœ‰è¶³å¤Ÿçš„å‰ç½®å†…å®¹
                min_content_length = 1000  # å‰åŠéƒ¨åˆ†è‡³å°‘1000å­—

                if cluster_start > chapter.para_start_idx + 5:  # è‡³å°‘è·³è¿‡5ä¸ªæ®µè½
                    # è®¡ç®—å‰åŠéƒ¨åˆ†çš„å­—æ•°
                    front_content = "\n".join(
                        doc.paragraphs[j].text.strip()
                        for j in range(chapter.para_start_idx + 1, cluster_start)
                        if j < len(doc.paragraphs)
                    )
                    front_word_count = self._calculate_word_count(front_content)

                    if front_word_count >= min_content_length:
                        self.logger.warning(
                            f"âš ï¸ ç« èŠ‚å°†è¢«æ‹†åˆ†: '{chapter.title}' "
                            f"â†’ æ­£å¸¸éƒ¨åˆ†({chapter.para_start_idx}-{cluster_start-1}, {front_word_count}å­—) "
                            f"+ åˆåŒéƒ¨åˆ†({cluster_start}-{cluster_end}, å¯†åº¦{density:.1%})"
                        )

                        # æˆªæ–­å½“å‰ç« èŠ‚ï¼ˆåªä¿ç•™åˆåŒä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                        original_end = chapter.para_end_idx
                        chapter.para_end_idx = cluster_start - 1

                        # é‡æ–°è®¡ç®—ç¼©çŸ­åçš„ç« èŠ‚å†…å®¹
                        content_text, preview_text, has_table = self._extract_chapter_content_with_tables(
                            doc, chapter.para_start_idx, chapter.para_end_idx
                        )
                        chapter.word_count = self._calculate_word_count(content_text)
                        chapter.preview_text = preview_text
                        chapter.has_table = has_table

                        # ğŸ†• åˆ›å»ºåˆåŒç« èŠ‚ï¼ˆæ ‡è®°ä¸ºå¾…æ’å…¥ï¼‰
                        contract_chapter = ChapterNode(
                            id=f"ch_{i}_contract",  # ä¸´æ—¶IDï¼Œåç»­ä¼šé‡æ–°åˆ†é…
                            level=chapter.level,  # ä¸åŸç« èŠ‚åŒçº§
                            title="[æ£€æµ‹åˆ°çš„åˆåŒæ¡æ¬¾-éœ€äººå·¥ç¡®è®¤]",
                            para_start_idx=cluster_start,
                            para_end_idx=original_end,
                            word_count=0,
                            preview_text=""
                        )

                        # è®¡ç®—åˆåŒç« èŠ‚å†…å®¹
                        contract_content, contract_preview, contract_has_table = self._extract_chapter_content_with_tables(
                            doc, contract_chapter.para_start_idx, contract_chapter.para_end_idx
                        )
                        contract_chapter.word_count = self._calculate_word_count(contract_content)
                        contract_chapter.preview_text = contract_preview
                        contract_chapter.has_table = contract_has_table

                        # æ·»åŠ åˆ°å¾…æ’å…¥åˆ—è¡¨ï¼ˆè®°å½•æ’å…¥ä½ç½®ï¼‰
                        contract_chapters_to_insert.append((i + 1, contract_chapter))

                        self.logger.info(
                            f"âœ‚ï¸ ç« èŠ‚æ‹†åˆ†å®Œæˆ: "
                            f"æ­£å¸¸éƒ¨åˆ†({chapter.para_start_idx}-{chapter.para_end_idx}, {chapter.word_count}å­—) "
                            f"+ åˆåŒéƒ¨åˆ†({contract_chapter.para_start_idx}-{contract_chapter.para_end_idx}, {contract_chapter.word_count}å­—)"
                        )
                    else:
                        self.logger.info(
                            f"è·³è¿‡æ‹†åˆ†: '{chapter.title}' å‰åŠéƒ¨åˆ†å†…å®¹ä¸è¶³({front_word_count}å­— < {min_content_length}å­—)"
                        )
                else:
                    self.logger.info(
                        f"è·³è¿‡æ‹†åˆ†: '{chapter.title}' åˆåŒèšé›†åŒºèµ·ç‚¹å¤ªé å‰(æ®µè½{cluster_start})"
                    )

        # ğŸ†• æ’å…¥æ‰€æœ‰æ£€æµ‹åˆ°çš„åˆåŒç« èŠ‚
        if contract_chapters_to_insert:
            # æŒ‰æ’å…¥ä½ç½®å€’åºæ’å…¥ï¼ˆé¿å…ç´¢å¼•åç§»ï¼‰
            for insert_pos, contract_chapter in reversed(contract_chapters_to_insert):
                chapters_sorted.insert(insert_pos, contract_chapter)

            self.logger.info(f"å·²æ’å…¥ {len(contract_chapters_to_insert)} ä¸ªåˆåŒç« èŠ‚")

            # ğŸ†• é‡æ–°åˆ†é…ç« èŠ‚IDï¼Œç¡®ä¿IDè¿ç»­
            for idx, ch in enumerate(chapters_sorted):
                ch.id = f"ch_{idx}"

            self.logger.info(f"ç« èŠ‚IDå·²é‡æ–°åˆ†é…ï¼Œå½“å‰å…± {len(chapters_sorted)} ä¸ªç« èŠ‚")

        return chapters_sorted

    def _extract_chapter_content_with_tables(self, doc: Document, para_start_idx: int, para_end_idx: int) -> tuple:
        """
        æå–ç« èŠ‚å†…å®¹,åŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_start_idx: èµ·å§‹æ®µè½ç´¢å¼•
            para_end_idx: ç»“æŸæ®µè½ç´¢å¼•

        Returns:
            (å®Œæ•´å†…å®¹æ–‡æœ¬, é¢„è§ˆæ–‡æœ¬, æ˜¯å¦åŒ…å«è¡¨æ ¼)
        """
        # æ„å»ºæ®µè½ç´¢å¼•åˆ°bodyå…ƒç´ ç´¢å¼•çš„æ˜ å°„
        para_count = 0
        start_body_idx = None
        end_body_idx = None

        for body_idx, element in enumerate(doc.element.body):
            if isinstance(element, CT_P):
                if para_count == para_start_idx and start_body_idx is None:
                    start_body_idx = body_idx
                if para_count == para_end_idx:
                    end_body_idx = body_idx
                    break
                para_count += 1

        if start_body_idx is None:
            return "", "", False

        if end_body_idx is None:
            end_body_idx = len(doc.element.body) - 1

        # æå–å†…å®¹(è·³è¿‡ç« èŠ‚æ ‡é¢˜,ä»start+1å¼€å§‹)
        content_parts = []
        preview_lines = []
        preview_limit = 5
        has_table = False  # æ ‡è®°æ˜¯å¦åŒ…å«è¡¨æ ¼

        for body_idx in range(start_body_idx + 1, end_body_idx + 1):
            element = doc.element.body[body_idx]

            if isinstance(element, CT_P):
                # æ®µè½
                from docx.text.paragraph import Paragraph
                para = Paragraph(element, doc)
                text = para.text.strip()
                if text:
                    content_parts.append(text)
                    # æ·»åŠ åˆ°é¢„è§ˆ
                    if len(preview_lines) < preview_limit:
                        preview_lines.append(text[:100] + ('...' if len(text) > 100 else ''))

            elif isinstance(element, CT_Tbl):
                # è¡¨æ ¼
                from docx.table import Table
                table = Table(element, doc)

                # æå–è¡¨æ ¼æ–‡æœ¬
                table_text_parts = []
                table_preview_parts = []

                for row_idx, row in enumerate(table.rows):
                    row_data = []
                    for cell in row.cells:
                        cell_text = '\n'.join(p.text.strip() for p in cell.paragraphs if p.text.strip())
                        row_data.append(cell_text)

                    if any(cell.strip() for cell in row_data):  # éç©ºè¡Œ
                        row_text = ' | '.join(row_data)
                        table_text_parts.append(row_text)

                        # æ·»åŠ åˆ°é¢„è§ˆ(è¡¨æ ¼çš„å‰å‡ è¡Œ)
                        if len(preview_lines) < preview_limit and row_idx < 3:
                            table_preview_parts.append(row_text[:100] + ('...' if len(row_text) > 100 else ''))

                if table_text_parts:
                    has_table = True  # å‘ç°è¡¨æ ¼
                    # æ·»åŠ è¡¨æ ¼æ ‡è¯†
                    table_content = f"[è¡¨æ ¼]\n" + '\n'.join(table_text_parts)
                    content_parts.append(table_content)

                    # æ·»åŠ è¡¨æ ¼é¢„è§ˆ
                    if len(preview_lines) < preview_limit:
                        preview_lines.append("[è¡¨æ ¼]")
                        preview_lines.extend(table_preview_parts[:preview_limit - len(preview_lines)])

        full_content = '\n'.join(content_parts)
        preview_text = '\n'.join(preview_lines)

        return full_content, preview_text, has_table

    def _calculate_word_count(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„å­—æ•°ï¼ˆä¸­æ–‡å­—ç¬¦æ•°ï¼‰

        ç»Ÿè®¡è§„åˆ™ï¼šå»é™¤ç©ºæ ¼å’Œæ¢è¡Œåçš„å­—ç¬¦æ€»æ•°

        Args:
            text: å¾…ç»Ÿè®¡çš„æ–‡æœ¬

        Returns:
            å­—æ•°ï¼ˆå»é™¤ç©ºæ ¼å’Œæ¢è¡Œåçš„å­—ç¬¦æ•°ï¼‰

        Examples:
            >>> parser._calculate_word_count("Hello World\\næµ‹è¯•")
            13  # "HelloWorldæµ‹è¯•" = 13ä¸ªå­—ç¬¦

            >>> parser._calculate_word_count("")
            0

        Note:
            æ­¤æ–¹æ³•æä¾›ç»Ÿä¸€çš„å­—æ•°ç»Ÿè®¡é€»è¾‘ï¼Œç”¨äºï¼š
            - ç« èŠ‚å†…å®¹å­—æ•°ç»Ÿè®¡
            - åˆ¤æ–­å†…å®¹æ˜¯å¦è¶³å¤Ÿï¼ˆå¦‚åˆåŒæ‹†åˆ†å‰çš„å­—æ•°æ£€æŸ¥ï¼‰
            - é¢„è§ˆæ˜¾ç¤ºçš„å­—æ•°ä¿¡æ¯
        """
        if not text:
            return 0
        return len(text.replace(' ', '').replace('\n', ''))

    def _build_chapter_tree(self, chapters: List[ChapterNode]) -> List[ChapterNode]:
        """
        æ„å»ºç« èŠ‚å±‚çº§æ ‘

        Args:
            chapters: æ‰å¹³ç« èŠ‚åˆ—è¡¨

        Returns:
            æ ¹çº§ç« èŠ‚åˆ—è¡¨ï¼ˆåŒ…å«å­ç« èŠ‚ï¼‰
        """
        if not chapters:
            return []

        # ä½¿ç”¨æ ˆæ¥æ„å»ºæ ‘
        root_chapters = []
        stack = []  # [(level, chapter), ...]

        for chapter in chapters:
            # å¼¹å‡ºæ‰€æœ‰å±‚çº§ >= å½“å‰ç« èŠ‚å±‚çº§çš„èŠ‚ç‚¹
            while stack and stack[-1][0] >= chapter.level:
                stack.pop()

            if not stack:
                # å½“å‰æ˜¯æ ¹çº§ç« èŠ‚
                root_chapters.append(chapter)
            else:
                # å½“å‰æ˜¯å­ç« èŠ‚ï¼Œæ·»åŠ åˆ°çˆ¶ç« èŠ‚çš„ children
                parent = stack[-1][1]
                parent.children.append(chapter)
                # æ›´æ–°å­ç« èŠ‚IDï¼ˆåŒ…å«çˆ¶çº§IDï¼‰
                chapter.id = f"{parent.id}_{len(parent.children)}"

            # å°†å½“å‰ç« èŠ‚å…¥æ ˆ
            stack.append((chapter.level, chapter))

        return root_chapters

    def _calculate_statistics(self, chapter_tree: List[ChapterNode]) -> Dict:
        """
        è®¡ç®—ç»Ÿè®¡ä¿¡æ¯

        Args:
            chapter_tree: ç« èŠ‚æ ‘

        Returns:
            ç»Ÿè®¡å­—å…¸
        """
        stats = {
            "total_chapters": 0,
            "total_words": 0,
            "estimated_processing_cost": 0.0
        }

        def traverse(chapters, is_root_level=False):
            for ch in chapters:
                stats["total_chapters"] += 1

                # ğŸ”‘ åªç»Ÿè®¡æ ¹èŠ‚ç‚¹ï¼ˆ1çº§ç« èŠ‚ï¼‰çš„å­—æ•°ï¼Œé¿å…é‡å¤ç»Ÿè®¡
                # å› ä¸ºçˆ¶èŠ‚ç‚¹çš„å­—æ•°å·²ç»åŒ…å«äº†æ‰€æœ‰å­èŠ‚ç‚¹çš„å†…å®¹
                if is_root_level:
                    stats["total_words"] += ch.word_count

                # é€’å½’éå†å­ç« èŠ‚
                if ch.children:
                    traverse(ch.children, is_root_level=False)

        traverse(chapter_tree, is_root_level=True)

        # ä¼°ç®—å¤„ç†æˆæœ¬ï¼ˆåŸºäºå­—æ•°ï¼‰
        # å‡è®¾ï¼š1000å­— â‰ˆ 1500 tokens â‰ˆ $0.002ï¼ˆGPT-4o-miniï¼‰
        stats["estimated_processing_cost"] = (stats["total_words"] / 1000) * 0.002

        return stats

    def get_selected_chapter_content(self, doc_path: str, selected_chapter_ids: List[str]) -> Dict:
        """
        æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ç« èŠ‚IDï¼Œæå–å¯¹åº”çš„æ–‡æœ¬å†…å®¹

        Args:
            doc_path: Word æ–‡æ¡£è·¯å¾„
            selected_chapter_ids: é€‰ä¸­çš„ç« èŠ‚IDåˆ—è¡¨ï¼Œå¦‚ ["ch_0", "ch_1_2"]

        Returns:
            {
                "success": True/False,
                "chapters": [
                    {
                        "id": "ch_0",
                        "title": "ç¬¬ä¸€ç«  é¡¹ç›®æ¦‚è¿°",
                        "content": "å®Œæ•´ç« èŠ‚æ–‡æœ¬å†…å®¹...",
                        "word_count": 1500
                    },
                    ...
                ],
                "total_words": 8000
            }
        """
        try:
            # é‡æ–°è§£ææ–‡æ¡£ï¼ˆè·å–å®Œæ•´ç« èŠ‚ä¿¡æ¯ï¼‰
            result = self.parse_document_structure(doc_path)
            if not result["success"]:
                return result

            chapters = self._flatten_chapters(result["chapters"])

            # æ‰“å¼€æ–‡æ¡£
            doc = Document(doc_path)

            # æå–é€‰ä¸­ç« èŠ‚çš„å†…å®¹
            selected_chapters = []
            total_words = 0

            for chapter_dict in chapters:
                if chapter_dict["id"] in selected_chapter_ids:
                    # æå–å†…å®¹
                    start_idx = chapter_dict["para_start_idx"]
                    end_idx = chapter_dict["para_end_idx"]

                    content_paras = doc.paragraphs[start_idx : end_idx + 1]
                    content = '\n'.join(p.text for p in content_paras)

                    selected_chapters.append({
                        "id": chapter_dict["id"],
                        "title": chapter_dict["title"],
                        "content": content,
                        "word_count": self._calculate_word_count(content)
                    })

                    total_words += selected_chapters[-1]["word_count"]

            self.logger.info(f"æå–äº† {len(selected_chapters)} ä¸ªç« èŠ‚ï¼Œå…± {total_words} å­—")

            return {
                "success": True,
                "chapters": selected_chapters,
                "total_words": total_words
            }

        except Exception as e:
            self.logger.error(f"æå–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return {
                "success": False,
                "chapters": [],
                "total_words": 0,
                "error": str(e)
            }

    def export_chapter_to_docx(self, doc_path: str, chapter_id: str,
                              output_path: str = None) -> Dict:
        """
        å°†æŒ‡å®šç« èŠ‚å¯¼å‡ºä¸ºç‹¬ç«‹çš„Wordæ–‡æ¡£ï¼ˆä¿ç•™åŸå§‹æ ¼å¼ï¼‰

        Args:
            doc_path: åŸå§‹Wordæ–‡æ¡£è·¯å¾„
            chapter_id: ç« èŠ‚ID (å¦‚ "ch_4")
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸´æ—¶ç›®å½•ï¼‰

        Returns:
            {
                "success": True,
                "file_path": "/path/to/exported.docx",
                "chapter_title": "ç¬¬äº”éƒ¨åˆ† å“åº”æ–‡ä»¶æ ¼å¼",
                "word_count": 1500
            }
        """
        try:
            from docx import Document
            from tempfile import NamedTemporaryFile
            from copy import deepcopy

            # 1. è§£ææ–‡æ¡£ç»“æ„ï¼Œå®šä½ç›®æ ‡ç« èŠ‚
            result = self.parse_document_structure(doc_path)
            if not result["success"]:
                return result

            chapters = self._flatten_chapters(result["chapters"])
            target_chapter = None

            for ch in chapters:
                if ch["id"] == chapter_id:
                    target_chapter = ch
                    break

            if not target_chapter:
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°ç« èŠ‚ID: {chapter_id}"
                }

            # 2. æ‰“å¼€åŸå§‹æ–‡æ¡£
            source_doc = Document(doc_path)

            # 3. åˆ›å»ºæ–°æ–‡æ¡£
            new_doc = Document()

            # 4. å¤åˆ¶ç« èŠ‚å†…å®¹ï¼ˆä¿ç•™æ ¼å¼ï¼‰
            para_start = target_chapter["para_start_idx"]
            para_end = target_chapter.get("para_end_idx")

            if para_end is None:
                para_end = len(source_doc.paragraphs) - 1

            self.logger.info(f"å¯¼å‡ºç« èŠ‚: {target_chapter['title']}")
            self.logger.info(f"æ®µè½èŒƒå›´: {para_start} - {para_end}")

            # å¤åˆ¶æ®µè½ï¼ˆä½¿ç”¨æ·±æ‹·è´ä¿ç•™æ ¼å¼ï¼‰
            for i in range(para_start, min(para_end + 1, len(source_doc.paragraphs))):
                source_para = source_doc.paragraphs[i]

                # ä½¿ç”¨XMLæ·±æ‹·è´ï¼ˆæœ€ä½³æ ¼å¼ä¿ç•™ï¼‰
                # å¯¼å…¥æ®µè½çš„å®Œæ•´XMLèŠ‚ç‚¹
                new_para_element = deepcopy(source_para._element)
                new_doc.element.body.append(new_para_element)

            # 5. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶æˆ–æŒ‡å®šè·¯å¾„
            if output_path is None:
                # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
                temp_file = NamedTemporaryFile(
                    delete=False,
                    suffix='.docx',
                    prefix='chapter_template_'
                )
                output_path = temp_file.name
                temp_file.close()

            new_doc.save(output_path)

            self.logger.info(f"ç« èŠ‚å·²å¯¼å‡º: {output_path}")

            return {
                "success": True,
                "file_path": output_path,
                "chapter_title": target_chapter["title"],
                "word_count": target_chapter.get("word_count", 0),
                "para_count": para_end - para_start + 1
            }

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºç« èŠ‚å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e)
            }

    def export_multiple_chapters_to_docx(self, doc_path: str, chapter_ids: List[str], output_path: str = None) -> Dict:
        """
        å°†å¤šä¸ªç« èŠ‚å¯¼å‡ºä¸ºå•ä¸ªWordæ–‡æ¡£

        Args:
            doc_path: æºæ–‡æ¡£è·¯å¾„
            chapter_ids: ç« èŠ‚IDåˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            {
                "success": bool,
                "file_path": str,
                "chapter_titles": List[str],
                "chapter_count": int
            }
        """
        from docx import Document
        from tempfile import NamedTemporaryFile
        from copy import deepcopy

        try:
            # ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æï¼ˆå…¼å®¹å¤šç§ç¯å¢ƒï¼‰
            resolved_path = resolve_file_path(doc_path)
            if not resolved_path:
                return {"success": False, "error": f"æ–‡æ¡£è·¯å¾„è§£æå¤±è´¥: {doc_path}"}

            doc_path = str(resolved_path)  # ä½¿ç”¨è§£æåçš„ç»å¯¹è·¯å¾„

            # è§£ææ–‡æ¡£ç»“æ„
            result = self.parse_document_structure(doc_path)
            chapters = self._flatten_chapters(result["chapters"])

            # â­ å…³é”®ä¿®å¤ï¼šè¿‡æ»¤æ‰çˆ¶ç« èŠ‚å·²è¢«é€‰ä¸­çš„å­ç« èŠ‚ï¼Œé¿å…é‡å¤å¯¼å‡º
            filtered_chapter_ids = self._filter_redundant_chapters(chapter_ids)
            if len(filtered_chapter_ids) < len(chapter_ids):
                removed_count = len(chapter_ids) - len(filtered_chapter_ids)
                self.logger.info(f"å»é‡å®Œæˆï¼šç§»é™¤äº† {removed_count} ä¸ªå†—ä½™å­ç« èŠ‚")
                self.logger.info(f"  åŸå§‹ç« èŠ‚åˆ—è¡¨: {chapter_ids}")
                self.logger.info(f"  å»é‡åç« èŠ‚åˆ—è¡¨: {filtered_chapter_ids}")

            # æŒ‰IDæŸ¥æ‰¾ç›®æ ‡ç« èŠ‚å¹¶æŒ‰åŸæ–‡æ¡£é¡ºåºæ’åº
            target_chapters = []
            for ch in chapters:
                if ch["id"] in filtered_chapter_ids:
                    target_chapters.append(ch)

            if not target_chapters:
                return {"success": False, "error": "æœªæ‰¾åˆ°æŒ‡å®šç« èŠ‚"}

            # æ‰“å¼€æºæ–‡æ¡£
            source_doc = Document(doc_path)
            # ä½¿ç”¨æºæ–‡æ¡£ä½œä¸ºæ¨¡æ¿ï¼Œä¿ç•™æ‰€æœ‰æ ·å¼å’Œé¡µé¢è®¾ç½®
            new_doc = Document(doc_path)

            # æ¸…ç©ºæ¨¡æ¿æ–‡æ¡£çš„æ‰€æœ‰bodyå†…å®¹ï¼ˆæ®µè½+è¡¨æ ¼ï¼‰ï¼Œä¿ç•™æ ·å¼å®šä¹‰ã€é¡µé¢è®¾ç½®ã€é¡µçœ‰é¡µè„šç­‰
            for element in list(new_doc.element.body):
                element.getparent().remove(element)

            chapter_titles = []

            # ä¾æ¬¡å¤åˆ¶æ¯ä¸ªç« èŠ‚
            for i, chapter in enumerate(target_chapters):
                chapter_titles.append(chapter["title"])

                # æ·»åŠ ç« èŠ‚æ ‡é¢˜ï¼ˆé™¤ç¬¬ä¸€ä¸ªç« èŠ‚å¤–ï¼Œå…¶ä»–ç« èŠ‚å‰åŠ åˆ†é¡µç¬¦ï¼‰
                if i > 0:
                    new_doc.add_page_break()

                # å¤åˆ¶ç« èŠ‚å†…å®¹ï¼ˆåŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼ï¼‰
                para_start = chapter["para_start_idx"]
                para_end = chapter.get("para_end_idx", len(source_doc.paragraphs) - 1)

                # æ„å»ºæ®µè½ç´¢å¼•åˆ°bodyç´¢å¼•çš„æ˜ å°„
                para_count = 0
                start_body_idx = None
                end_body_idx = None

                for body_idx, element in enumerate(source_doc.element.body):
                    if isinstance(element, CT_P):
                        if para_count == para_start and start_body_idx is None:
                            start_body_idx = body_idx
                        if para_count == para_end:
                            end_body_idx = body_idx
                            break
                        para_count += 1

                # å¤åˆ¶èŒƒå›´å†…çš„æ‰€æœ‰å…ƒç´ ï¼ˆæ®µè½+è¡¨æ ¼ï¼‰
                if start_body_idx is not None and end_body_idx is not None:
                    for body_idx in range(start_body_idx, end_body_idx + 1):
                        element = source_doc.element.body[body_idx]
                        new_element = deepcopy(element)
                        new_doc.element.body.append(new_element)

            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            if output_path is None:
                temp_file = NamedTemporaryFile(delete=False, suffix='.docx', prefix='chapters_template_')
                output_path = temp_file.name
                temp_file.close()

            new_doc.save(output_path)

            logger.info(f"æ‰¹é‡å¯¼å‡ºæˆåŠŸ: {len(target_chapters)}ä¸ªç« èŠ‚ -> {output_path}")

            return {
                "success": True,
                "file_path": output_path,
                "chapter_titles": chapter_titles,
                "chapter_count": len(target_chapters)
            }

        except Exception as e:
            logger.error(f"æ‰¹é‡å¯¼å‡ºå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}

    def _flatten_chapters(self, chapter_dicts: List[Dict]) -> List[Dict]:
        """å°†ç« èŠ‚æ ‘æ‰å¹³åŒ–ä¸ºåˆ—è¡¨"""
        flat = []

        def traverse(chapters):
            for ch in chapters:
                flat.append(ch)
                if ch.get("children"):
                    traverse(ch["children"])

        traverse(chapter_dicts)
        return flat

    def _filter_redundant_chapters(self, chapter_ids: List[str]) -> List[str]:
        """
        è¿‡æ»¤æ‰çˆ¶ç« èŠ‚å·²è¢«é€‰ä¸­çš„å­ç« èŠ‚ï¼Œé¿å…é‡å¤å¯¼å‡º

        ä¾‹å¦‚ï¼šå¦‚æœåŒæ—¶é€‰æ‹©äº† ch_3 å’Œ ch_3_2ï¼Œåˆ™åªä¿ç•™ ch_3

        Args:
            chapter_ids: åŸå§‹ç« èŠ‚IDåˆ—è¡¨

        Returns:
            å»é‡åçš„ç« èŠ‚IDåˆ—è¡¨
        """
        filtered_ids = []

        for chapter_id in chapter_ids:
            # æ£€æŸ¥æ˜¯å¦æœ‰çˆ¶ç« èŠ‚å·²åœ¨åˆ—è¡¨ä¸­
            has_parent_selected = False

            # åˆ†æç« èŠ‚IDçš„å±‚çº§ç»“æ„ï¼ˆch_3_2_1 -> ["ch_3", "ch_3_2", "ch_3_2_1"]ï¼‰
            parts = chapter_id.split('_')
            for i in range(1, len(parts)):
                # æ„å»ºå¯èƒ½çš„çˆ¶ç« èŠ‚ID
                parent_id = '_'.join(parts[:i+1])
                if parent_id != chapter_id and parent_id in chapter_ids:
                    has_parent_selected = True
                    self.logger.debug(f"è·³è¿‡å­ç« èŠ‚ {chapter_id}ï¼Œå› ä¸ºçˆ¶ç« èŠ‚ {parent_id} å·²è¢«é€‰ä¸­")
                    break

            # å¦‚æœæ²¡æœ‰çˆ¶ç« èŠ‚è¢«é€‰ä¸­ï¼Œä¿ç•™è¯¥ç« èŠ‚
            if not has_parent_selected:
                filtered_ids.append(chapter_id)

        return filtered_ids

    # ========================================
        """
        æ£€æµ‹è¿ç»­ç« èŠ‚æ ‡é¢˜åˆ—è¡¨çš„å®Œæ•´èŒƒå›´ï¼ˆç”¨äºè·³è¿‡"æ–‡ä»¶æ„æˆè¯´æ˜"ç­‰å…ƒæ•°æ®åˆ—è¡¨ï¼‰

        ç­–ç•¥ï¼ˆåŸºäºTOCå¯¹æ¯”ï¼Œæ›´æ™ºèƒ½ï¼‰ï¼š
        1. ä»å½“å‰æ®µè½å‘å‰å‘åæ‰«æçŸ­æ–‡æœ¬ï¼ˆâ‰¤50å­—ï¼‰
        2. æ£€æŸ¥è¿™äº›æ–‡æœ¬æ˜¯å¦ä¸TOCé¡¹åŒ¹é…
        3. å¦‚æœè¿ç»­3ä¸ªä»¥ä¸ŠTOCåŒ¹é…ï¼Œä¸”æ ‡é¢˜é—´æ— å®è´¨å†…å®¹ï¼Œåˆ¤å®šä¸ºå…ƒæ•°æ®åˆ—è¡¨

        Args:
            doc: Wordæ–‡æ¡£å¯¹è±¡
            para_idx: å½“å‰æ®µè½ç´¢å¼•
            toc_items: ç›®å½•é¡¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™åŸºäºTOCå¯¹æ¯”
            toc_end_idx: ç›®å½•ç»“æŸçš„æ®µè½ç´¢å¼•ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºé™åˆ¶ä¸æ‰«æç›®å½•åŒºåŸŸ

        Returns:
            (start, end, titles) æˆ– None
            - start: åˆ—è¡¨èµ·å§‹æ®µè½ç´¢å¼•
            - end: åˆ—è¡¨ç»“æŸæ®µè½ç´¢å¼•
            - titles: [(idx, text), ...] åˆ—è¡¨ä¸­çš„æ‰€æœ‰ç« èŠ‚æ ‡é¢˜
        """
        # å¦‚æœæ²¡æœ‰TOCï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½æ£€æµ‹
        if not toc_items:
            return None

        # ğŸ”‘ æ‰«æèŒƒå›´ï¼šå½“å‰æ®µè½å‰åï¼Œä½†ä¸æ‰«æç›®å½•åŒºåŸŸï¼ˆç›®å½•å’Œæ­£æ–‡åˆ†ç¦»ï¼‰
        if toc_end_idx is not None:
            # ç¡®ä¿æ‰«æèµ·ç‚¹åœ¨ç›®å½•ç»“æŸä¹‹åï¼ˆæ­£æ–‡åŒºåŸŸï¼‰
            scan_start = max(toc_end_idx + 1, para_idx - 10)
        else:
            scan_start = max(0, para_idx - 10)
        scan_end = min(len(doc.paragraphs), para_idx + 20)

        # æ”¶é›†ä¸TOCåŒ¹é…çš„æ®µè½
        toc_matched_paras = []

        for i in range(scan_start, scan_end):
            para = doc.paragraphs[i]
            text = para.text.strip()

            # åªæ£€æŸ¥çŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
            if not text or len(text) > 50:
                continue

            # æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ï¼‰
            clean_text = re.sub(r'[\s\u3000]+', '', text)

            # æ£€æŸ¥æ˜¯å¦ä¸ä»»ä½•TOCé¡¹åŒ¹é…
            for toc_item in toc_items:
                toc_title = toc_item['title']
                clean_toc = re.sub(r'[\s\u3000]+', '', toc_title)

                # ğŸ”‘ åªä½¿ç”¨å®Œå…¨åŒ¹é…ï¼ˆé¿å…è¯¯åˆ¤å­æ ‡é¢˜ä¸ºå…ƒæ•°æ®ï¼‰
                # ä¾‹å¦‚ï¼š"æŠ•æ ‡äººé¡»çŸ¥å‰é™„è¡¨"ä¸åº”è¯¥åŒ¹é…"æŠ•æ ‡äººé¡»çŸ¥å‰é™„è¡¨åŠæŠ•æ ‡äººé¡»çŸ¥"
                if clean_text == clean_toc:
                    toc_matched_paras.append((i, text, toc_item))
                    break

        # è‡³å°‘éœ€è¦3ä¸ªè¿ç»­çš„TOCåŒ¹é…
        if len(toc_matched_paras) < 3:
            return None

        # æ£€æŸ¥æ˜¯å¦è¿ç»­ï¼ˆå…è®¸é—´éš”1-3ä¸ªæ®µè½ï¼‰
        consecutive_groups = []
        current_group = [toc_matched_paras[0]]

        for j in range(1, len(toc_matched_paras)):
            gap = toc_matched_paras[j][0] - toc_matched_paras[j-1][0]

            if gap <= 4:  # å…è®¸é—´éš”æœ€å¤š3ä¸ªç©ºæ®µ
                current_group.append(toc_matched_paras[j])
            else:
                if len(current_group) >= 3:
                    consecutive_groups.append(current_group)
                current_group = [toc_matched_paras[j]]

        # æ£€æŸ¥æœ€åä¸€ç»„
        if len(current_group) >= 3:
            consecutive_groups.append(current_group)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿ç»­ç»„ï¼Œè¿”å›None
        if not consecutive_groups:
            return None

        # é€‰æ‹©åŒ…å«å½“å‰æ®µè½çš„ç»„ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼Œé€‰æ‹©æœ€å¤§çš„ï¼‰
        target_group = None
        for group in consecutive_groups:
            group_start = group[0][0]
            group_end = group[-1][0]
            if group_start <= para_idx <= group_end:
                if target_group is None or len(group) > len(target_group):
                    target_group = group

        # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šå¦‚æœå½“å‰æ®µè½ä¸åœ¨ä»»ä½•ç»„ä¸­ï¼Œè¿”å› Noneï¼ˆä¸æ˜¯å…ƒæ•°æ®ï¼‰
        if target_group is None:
            return None

        # æ£€æŸ¥è¯¥ç»„çš„æ ‡é¢˜ä¹‹é—´æ˜¯å¦æ— å®è´¨å†…å®¹
        has_substantial_content = False
        for j in range(len(target_group) - 1):
            start_idx = target_group[j][0]
            end_idx = target_group[j + 1][0]

            # è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜ä¹‹é—´çš„å†…å®¹å­—æ•°
            content_chars = sum(
                len(doc.paragraphs[k].text.strip())
                for k in range(start_idx + 1, end_idx)
            )

            # å¦‚æœæ ‡é¢˜é—´æœ‰è¶…è¿‡100å­—ï¼Œè¯´æ˜ä¸æ˜¯å…ƒæ•°æ®åˆ—è¡¨
            if content_chars > 100:
                has_substantial_content = True
                break

        if not has_substantial_content:
            # â­ å…³é”®é€»è¾‘ï¼šå³ä½¿æ ‡é¢˜é—´æ— å®è´¨å†…å®¹ï¼Œä»éœ€éªŒè¯è¿™ä¸æ˜¯çœŸæ­£ç« èŠ‚çš„å¼€å§‹
            #
            # æ£€æŸ¥ï¼šå¦‚æœåˆ—è¡¨ä¸­æ‰€æœ‰æ®µè½éƒ½æœ‰Headingæ ·å¼ â†’ è¿™æ˜¯çœŸæ­£çš„ç« èŠ‚åºåˆ—ï¼Œä¸æ˜¯å…ƒæ•°æ®
            # ç¤ºä¾‹ï¼šè¿ç»­çš„Heading 1æ®µè½ = çœŸæ­£ç« èŠ‚
            #       è¿ç»­çš„Normalæ ·å¼ = å…ƒæ•°æ®åˆ—è¡¨ï¼ˆæ–‡ä»¶æ„æˆè¯´æ˜ï¼‰
            all_headings = all(
                doc.paragraphs[idx].style and
                ('heading' in doc.paragraphs[idx].style.name.lower() or 'æ ‡é¢˜' in doc.paragraphs[idx].style.name.lower())
                for idx, _, _ in target_group
            )

            if all_headings:
                self.logger.info(f"  æ£€æµ‹åˆ°è¿ç»­Headingæ ·å¼ç« èŠ‚ï¼ˆæ®µè½{target_group[0][0]}-{target_group[-1][0]}ï¼‰ï¼Œ"
                                f"åˆ¤å®šä¸ºçœŸæ­£ç« èŠ‚åºåˆ—ï¼Œä¸æ˜¯å…ƒæ•°æ®")
                return None  # ä¸æ˜¯å…ƒæ•°æ®åˆ—è¡¨ï¼Œæ˜¯çœŸæ­£çš„ç« èŠ‚

            # å¦‚æœä¸æ˜¯å…¨éƒ¨Headingï¼Œåˆ™åˆ¤å®šä¸ºå…ƒæ•°æ®åˆ—è¡¨
            # è¿”å›åˆ—è¡¨èŒƒå›´
            return (
                target_group[0][0],  # start
                target_group[-1][0],  # end
                [(idx, text) for idx, text, _ in target_group]  # titles
            )

        return None
if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import sys

    if len(sys.argv) > 1:
        doc_path = sys.argv[1]
    else:
        print("ç”¨æ³•: python structure_parser.py <wordæ–‡æ¡£è·¯å¾„>")
        sys.exit(1)

    parser = DocumentStructureParser()
    result = parser.parse_document_structure(doc_path)

    if result["success"]:
        print(f"\nâœ… è§£ææˆåŠŸï¼")
        print(f"ç»Ÿè®¡ä¿¡æ¯: {result['statistics']}")
        print(f"\nç« èŠ‚ç»“æ„:")

        def print_tree(chapters, indent=0):
            for ch in chapters:
                prefix = "  " * indent
                print(f"{prefix}[{ch['level']}çº§] {ch['title']} ({ch['word_count']}å­—)")
                if ch.get("children"):
                    print_tree(ch["children"], indent + 1)

        print_tree(result["chapters"])
    else:
        print(f"\nâŒ è§£æå¤±è´¥: {result.get('error')}")
