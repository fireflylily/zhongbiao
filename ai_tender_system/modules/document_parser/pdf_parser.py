#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFæ–‡æ¡£è§£æå™¨
ä½¿ç”¨PyMuPDFå’Œpdfplumberè¿›è¡Œé«˜è´¨é‡PDFè§£æ
æ”¯æŒOCRè¯†åˆ«æ‰«æPDF
"""

import fitz  # PyMuPDF
import pdfplumber
import re
import asyncio
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from common.logger import get_module_logger

logger = get_module_logger("document_parser.pdf")


class PDFParser:
    """PDFæ–‡æ¡£è§£æå™¨ - æ”¯æŒåŸç”ŸPDFå’Œæ‰«æPDF(OCR)"""

    def __init__(self, enable_ocr: bool = None, ocr_min_chars: int = None):
        """
        åˆå§‹åŒ–PDFè§£æå™¨

        Args:
            enable_ocr: æ˜¯å¦å¯ç”¨OCRï¼ˆNone=ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤Trueï¼‰
            ocr_min_chars: è§¦å‘OCRçš„æœ€å°å­—ç¬¦æ•°é˜ˆå€¼ï¼ˆNone=ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤50ï¼‰
        """
        self.logger = logger
        self.max_file_size = 100 * 1024 * 1024  # 100MBé™åˆ¶

        # OCRé…ç½®
        if enable_ocr is None:
            enable_ocr = os.getenv('ENABLE_OCR', 'true').lower() == 'true'
        if ocr_min_chars is None:
            ocr_min_chars = int(os.getenv('OCR_MIN_CHARS_PER_PAGE', '50'))

        self.enable_ocr = enable_ocr
        self.ocr_min_chars = ocr_min_chars
        self._ocr_parser = None

        if self.enable_ocr:
            self.logger.info(f"OCRåŠŸèƒ½å·²å¯ç”¨ (é˜ˆå€¼: {self.ocr_min_chars}å­—ç¬¦/é¡µ)")

    def _get_ocr_parser(self):
        """å»¶è¿Ÿåˆå§‹åŒ–OCRè§£æå™¨ï¼ˆä»…åœ¨éœ€è¦æ—¶åŠ è½½ï¼‰"""
        if self._ocr_parser is None and self.enable_ocr:
            try:
                from .ocr_parser import OCRParser

                use_gpu = os.getenv('OCR_USE_GPU', 'false').lower() == 'true'
                lang = os.getenv('OCR_LANG', 'ch')

                self._ocr_parser = OCRParser(use_gpu=use_gpu, lang=lang)
                self.logger.info("OCRè§£æå™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                self.logger.warning(f"OCRè§£æå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨OCR: {e}")
                self.enable_ocr = False

        return self._ocr_parser

    async def parse(self, file_path: str) -> Tuple[str, Dict]:
        """
        è§£æPDFæ–‡æ¡£

        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            Tuple[str, Dict]: (æå–çš„æ–‡æœ¬å†…å®¹, å…ƒæ•°æ®)
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size
        if file_size > self.max_file_size:
            raise ValueError(f"æ–‡ä»¶è¿‡å¤§: {file_size / (1024*1024):.1f}MB > {self.max_file_size / (1024*1024)}MB")

        self.logger.info(f"å¼€å§‹è§£æPDFæ–‡æ¡£: {file_path}")

        try:
            # å¹¶è¡Œæ‰§è¡Œæ–‡æœ¬æå–å’Œè¡¨æ ¼æå–
            text_task = asyncio.create_task(self._extract_text_with_fitz(str(file_path)))
            table_task = asyncio.create_task(self._extract_tables_with_pdfplumber(str(file_path)))
            metadata_task = asyncio.create_task(self._extract_metadata(str(file_path)))

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            text_content, structure_info = await text_task
            tables = await table_task
            metadata = await metadata_task

            # åˆå¹¶æ–‡æœ¬å’Œè¡¨æ ¼å†…å®¹
            full_content = self._merge_content_and_tables(text_content, tables, structure_info)

            # å¢å¼ºå…ƒæ•°æ®
            metadata.update({
                'total_pages': len(structure_info.get('pages', [])),
                'tables_count': len(tables),
                'has_images': any(page.get('images', 0) > 0 for page in structure_info.get('pages', [])),
                'extraction_time': datetime.now().isoformat(),
                'file_size_mb': round(file_size / (1024 * 1024), 2)
            })

            self.logger.info(f"PDFè§£æå®Œæˆ: pages={metadata['total_pages']}, tables={len(tables)}")

            return full_content, metadata

        except Exception as e:
            self.logger.error(f"PDFè§£æå¤±è´¥: {file_path}, error={e}")
            raise

    async def _extract_text_with_fitz(self, file_path: str) -> Tuple[str, Dict]:
        """ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬å’Œç»“æ„ä¿¡æ¯ï¼Œå¯¹æ‰«æé¡µè‡ªåŠ¨å¯ç”¨OCR"""

        def extract_text():
            doc = fitz.open(file_path)
            text_content = ""
            structure_info = {
                'pages': [],
                'headings': [],
                'total_chars': 0,
                'scanned_pages': []  # è®°å½•æ‰«æé¡µ
            }

            try:
                for page_num in range(len(doc)):
                    page = doc[page_num]

                    # æå–æ–‡æœ¬
                    page_text = page.get_text()

                    # æå–é¡µé¢ç»“æ„ä¿¡æ¯
                    page_info = {
                        'page_num': page_num + 1,
                        'char_count': len(page_text),
                        'images': len(page.get_images()),
                        'links': len(page.get_links()),
                        'is_scanned': False
                    }

                    # æ£€æµ‹æ˜¯å¦ä¸ºæ‰«æé¡µï¼ˆæ–‡å­—è¿‡å°‘ï¼‰
                    if len(page_text.strip()) < self.ocr_min_chars:
                        page_info['is_scanned'] = True
                        structure_info['scanned_pages'].append(page_num)
                        self.logger.debug(f"ğŸ” æ£€æµ‹åˆ°æ‰«æé¡µ: ç¬¬{page_num + 1}é¡µ (ä»…{len(page_text)}å­—ç¬¦)")

                    text_content += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n"
                    text_content += page_text

                    # æå–æ ‡é¢˜ï¼ˆåŸºäºå­—ä½“å¤§å°å’Œæ ·å¼ï¼‰
                    blocks = page.get_text("dict")
                    page_headings = self._extract_headings_from_blocks(blocks, page_num + 1)
                    structure_info['headings'].extend(page_headings)

                    structure_info['pages'].append(page_info)
                    structure_info['total_chars'] += len(page_text)

            finally:
                doc.close()

            return text_content, structure_info

        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡
        loop = asyncio.get_event_loop()
        text_content, structure_info = await loop.run_in_executor(None, extract_text)

        # å¦‚æœæ£€æµ‹åˆ°æ‰«æé¡µä¸”OCRå·²å¯ç”¨ï¼Œè¿›è¡ŒOCRè¯†åˆ«
        scanned_pages = structure_info.get('scanned_pages', [])
        if scanned_pages and self.enable_ocr:
            self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ° {len(scanned_pages)} ä¸ªæ‰«æé¡µé¢ï¼Œå¯åŠ¨OCRè¯†åˆ«...")

            ocr_parser = self._get_ocr_parser()
            if ocr_parser:
                try:
                    # å¯¹æ‰«æé¡µè¿›è¡ŒOCRè¯†åˆ«
                    ocr_results = await ocr_parser.ocr_pdf(file_path, scanned_pages)

                    # å°†OCRç»“æœåˆå¹¶åˆ°æ–‡æœ¬ä¸­
                    if ocr_results:
                        text_content = self._merge_ocr_results(
                            text_content,
                            ocr_results,
                            structure_info
                        )

                        # æ›´æ–°å­—ç¬¦ç»Ÿè®¡
                        total_ocr_chars = sum(len(text) for text in ocr_results.values())
                        structure_info['total_chars'] += total_ocr_chars
                        structure_info['ocr_chars'] = total_ocr_chars

                        self.logger.info(f"âœ… OCRè¯†åˆ«å®Œæˆï¼Œé¢å¤–æå– {total_ocr_chars} å­—ç¬¦")

                except Exception as e:
                    self.logger.error(f"OCRè¯†åˆ«å¤±è´¥: {e}")

        return text_content, structure_info

    async def _extract_tables_with_pdfplumber(self, file_path: str) -> List[Dict]:
        """ä½¿ç”¨pdfplumberæå–è¡¨æ ¼"""

        def extract_tables():
            tables = []

            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    # æå–é¡µé¢ä¸­çš„è¡¨æ ¼
                    page_tables = page.extract_tables()

                    for table_index, table in enumerate(page_tables):
                        if table and len(table) > 1:  # ç¡®ä¿è¡¨æ ¼æœ‰æ•°æ®
                            # æ¸…ç†è¡¨æ ¼æ•°æ®
                            cleaned_table = []
                            for row in table:
                                cleaned_row = [cell.strip() if cell else "" for cell in row]
                                # è¿‡æ»¤ç©ºè¡Œ
                                if any(cell for cell in cleaned_row):
                                    cleaned_table.append(cleaned_row)

                            if cleaned_table:
                                table_info = {
                                    'page': page_num,
                                    'table_index': table_index,
                                    'rows': len(cleaned_table),
                                    'columns': len(cleaned_table[0]) if cleaned_table else 0,
                                    'data': cleaned_table,
                                    'text_representation': self._table_to_text(cleaned_table)
                                }
                                tables.append(table_info)

            return tables

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, extract_tables)

    async def _extract_metadata(self, file_path: str) -> Dict:
        """æå–PDFå…ƒæ•°æ®"""

        def extract_metadata():
            metadata = {}

            try:
                doc = fitz.open(file_path)

                # è·å–æ–‡æ¡£å±æ€§
                doc_metadata = doc.metadata
                metadata.update({
                    'title': doc_metadata.get('title', ''),
                    'author': doc_metadata.get('author', ''),
                    'subject': doc_metadata.get('subject', ''),
                    'creator': doc_metadata.get('creator', ''),
                    'producer': doc_metadata.get('producer', ''),
                    'creation_date': doc_metadata.get('creationDate', ''),
                    'modification_date': doc_metadata.get('modDate', ''),
                    'format': doc_metadata.get('format', 'PDF'),
                    'encrypted': doc.is_encrypted,
                    'pages': doc.page_count
                })

                doc.close()

            except Exception as e:
                logger.warning(f"æå–PDFå…ƒæ•°æ®å¤±è´¥: {e}")
                metadata['error'] = str(e)

            return metadata

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, extract_metadata)

    def _extract_headings_from_blocks(self, blocks: Dict, page_num: int) -> List[Dict]:
        """ä»æ–‡æœ¬å—ä¸­æå–æ ‡é¢˜"""
        headings = []

        if 'blocks' not in blocks:
            return headings

        for block in blocks['blocks']:
            if 'lines' not in block:
                continue

            for line in block['lines']:
                if 'spans' not in line:
                    continue

                # åˆ†æå­—ä½“å¤§å°å’Œæ ·å¼
                for span in line['spans']:
                    text = span.get('text', '').strip()
                    if not text:
                        continue

                    font_size = span.get('size', 0)
                    font_flags = span.get('flags', 0)
                    is_bold = font_flags & 2 ** 4  # ç²—ä½“æ ‡å¿—

                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜ï¼ˆåŸºäºå­—ä½“å¤§å°å’Œæ ·å¼ï¼‰
                    if font_size > 14 or is_bold:
                        # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ ‡é¢˜æ¨¡å¼
                        if self._is_likely_heading(text):
                            heading = {
                                'page': page_num,
                                'text': text,
                                'font_size': font_size,
                                'is_bold': bool(is_bold),
                                'level': self._determine_heading_level(font_size, is_bold)
                            }
                            headings.append(heading)

        return headings

    def _is_likely_heading(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜"""
        # æ ‡é¢˜ç‰¹å¾æ£€æµ‹
        if len(text) > 100:  # æ ‡é¢˜é€šå¸¸ä¸ä¼šå¤ªé•¿
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡é¢˜å…³é”®è¯
        title_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]',
            r'\d+\.?\d*\s*.{1,50}',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€\.]',
            r'^(æ‘˜è¦|æ¦‚è¿°|å¼•è¨€|ç»“è®º|æ€»ç»“|é™„å½•)',
            r'^[A-Z][A-Z\s]{2,20}$',  # å…¨å¤§å†™è‹±æ–‡æ ‡é¢˜
        ]

        for pattern in title_patterns:
            if re.search(pattern, text):
                return True

        return False

    def _determine_heading_level(self, font_size: float, is_bold: bool) -> int:
        """ç¡®å®šæ ‡é¢˜çº§åˆ«"""
        if font_size >= 18:
            return 1
        elif font_size >= 16:
            return 2
        elif font_size >= 14:
            return 3
        elif is_bold:
            return 4
        else:
            return 5

    def _table_to_text(self, table_data: List[List[str]]) -> str:
        """å°†è¡¨æ ¼æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤º"""
        if not table_data:
            return ""

        # è®¡ç®—æ¯åˆ—çš„æœ€å¤§å®½åº¦
        max_widths = []
        for row in table_data:
            for i, cell in enumerate(row):
                if i >= len(max_widths):
                    max_widths.append(0)
                max_widths[i] = max(max_widths[i], len(str(cell)))

        # ç”Ÿæˆè¡¨æ ¼æ–‡æœ¬
        text_lines = []
        for row in table_data:
            formatted_row = []
            for i, cell in enumerate(row):
                width = max_widths[i] if i < len(max_widths) else 10
                formatted_row.append(str(cell).ljust(width))
            text_lines.append(" | ".join(formatted_row))

        return "\n".join(text_lines)

    def _merge_content_and_tables(self, text_content: str, tables: List[Dict], structure_info: Dict) -> str:
        """åˆå¹¶æ–‡æœ¬å†…å®¹å’Œè¡¨æ ¼å†…å®¹"""
        if not tables:
            return text_content

        # å°†è¡¨æ ¼å†…å®¹æ’å…¥åˆ°ç›¸åº”çš„é¡µé¢ä½ç½®
        lines = text_content.split('\n')
        merged_lines = []
        current_page = 0

        for line in lines:
            merged_lines.append(line)

            # æ£€æŸ¥æ˜¯å¦æ˜¯é¡µé¢åˆ†å‰²çº¿
            if line.startswith('--- ç¬¬') and line.endswith('é¡µ ---'):
                # æå–é¡µç 
                page_match = re.search(r'ç¬¬(\d+)é¡µ', line)
                if page_match:
                    current_page = int(page_match.group(1))

                    # æ’å…¥è¯¥é¡µé¢çš„æ‰€æœ‰è¡¨æ ¼
                    page_tables = [t for t in tables if t['page'] == current_page]
                    for table in page_tables:
                        merged_lines.append(f"\n[è¡¨æ ¼ {table['table_index'] + 1}]")
                        merged_lines.append(table['text_representation'])
                        merged_lines.append("")

        return '\n'.join(merged_lines)

    def _merge_ocr_results(self, text_content: str, ocr_results: Dict[int, str], structure_info: Dict) -> str:
        """
        å°†OCRè¯†åˆ«ç»“æœåˆå¹¶åˆ°åŸæ–‡æœ¬ä¸­

        Args:
            text_content: åŸå§‹æ–‡æœ¬å†…å®¹
            ocr_results: OCRè¯†åˆ«ç»“æœ {é¡µç : OCRæ–‡æœ¬}
            structure_info: ç»“æ„ä¿¡æ¯

        Returns:
            str: åˆå¹¶åçš„æ–‡æœ¬
        """
        lines = text_content.split('\n')
        merged_lines = []

        for line in lines:
            merged_lines.append(line)

            # æ£€æŸ¥æ˜¯å¦æ˜¯é¡µé¢åˆ†å‰²çº¿
            if line.startswith('--- ç¬¬') and line.endswith('é¡µ ---'):
                # æå–é¡µç 
                page_match = re.search(r'ç¬¬(\d+)é¡µ', line)
                if page_match:
                    page_num = int(page_match.group(1)) - 1  # è½¬æ¢ä¸ºä»0å¼€å§‹çš„ç´¢å¼•

                    # å¦‚æœè¯¥é¡µæœ‰OCRç»“æœï¼Œæ’å…¥OCRæ–‡æœ¬
                    if page_num in ocr_results:
                        ocr_text = ocr_results[page_num]
                        if ocr_text.strip():
                            merged_lines.append(f"\n[OCRè¯†åˆ«å†…å®¹]")
                            merged_lines.append(ocr_text)
                            merged_lines.append("")
                            self.logger.debug(f"å·²æ’å…¥ç¬¬{page_num + 1}é¡µçš„OCRå†…å®¹ ({len(ocr_text)}å­—ç¬¦)")

        return '\n'.join(merged_lines)

    def identify_audit_report_key_pages(self, pdf_path: str, max_pages: int = 20) -> List[int]:
        """
        æ™ºèƒ½è¯†åˆ«å®¡è®¡æŠ¥å‘Šä¸­çš„å…³é”®é¡µ

        å…³é”®é¡µåŒ…æ‹¬ï¼š
        1. å°é¢ï¼ˆç¬¬1é¡µï¼‰
        2. å®¡è®¡æŠ¥å‘Šæ­£æ–‡é¡µï¼ˆå®¡è®¡æ„è§ã€å®¡è®¡åŸºç¡€ç­‰ï¼‰
        3. ä¸»è¦è´¢åŠ¡æŠ¥è¡¨ï¼ˆèµ„äº§è´Ÿå€ºè¡¨ã€åˆ©æ¶¦è¡¨ã€ç°é‡‘æµé‡è¡¨ã€æ‰€æœ‰è€…æƒç›Šå˜åŠ¨è¡¨ï¼‰
        4. ä¼šè®¡å¸ˆç­¾å­—é¡µ

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            max_pages: æœ€å¤šæå–çš„å…³é”®é¡µæ•°ï¼ˆé˜²æ­¢è¯¯åˆ¤ï¼‰

        Returns:
            å…³é”®é¡µç åˆ—è¡¨ï¼ˆä»1å¼€å§‹ï¼‰ï¼Œå¦‚ [1, 2, 5, 6, 7, 8, 9, 10]
        """
        try:
            doc = fitz.open(pdf_path)
            key_pages = set()

            # å°é¢ï¼ˆå¿…é€‰ï¼‰
            key_pages.add(1)

            # å®šä¹‰å…³é”®å†…å®¹æ ‡è¯†è¯ï¼ˆä¼˜å…ˆçº§æ’åºï¼‰
            critical_keywords = [
                # å®¡è®¡æŠ¥å‘Šæ ¸å¿ƒéƒ¨åˆ†ï¼ˆæœ€é‡è¦ï¼‰
                'å®¡è®¡æŠ¥å‘Š',
                'å®¡è®¡æ„è§',
                'ä¿ç•™æ„è§',
                'æ— ä¿ç•™æ„è§',
                'å¦å®šæ„è§',
                'æ— æ³•è¡¨ç¤ºæ„è§',
                'å½¢æˆå®¡è®¡æ„è§çš„åŸºç¡€',
                'ç®¡ç†å±‚.*è´£ä»»',
                'æ³¨å†Œä¼šè®¡å¸ˆ.*è´£ä»»',

                # ä¸»è¦è´¢åŠ¡æŠ¥è¡¨ï¼ˆé‡è¦ï¼‰
                'èµ„äº§è´Ÿå€ºè¡¨',
                'åˆ©æ¶¦è¡¨',
                'ç°é‡‘æµé‡è¡¨',
                'æ‰€æœ‰è€…æƒç›Šå˜åŠ¨è¡¨',
                'åˆå¹¶èµ„äº§è´Ÿå€ºè¡¨',
                'åˆå¹¶åˆ©æ¶¦è¡¨',

                # ç­¾å­—é¡µï¼ˆé‡è¦ï¼‰
                'ä¼šè®¡å¸ˆäº‹åŠ¡æ‰€',
                'æ³¨å†Œä¼šè®¡å¸ˆ.*ç­¾å­—',
                'ä¸­å›½æ³¨å†Œä¼šè®¡å¸ˆ',
            ]

            # æ¬¡è¦å…³é”®è¯ï¼ˆè¡¥å……ï¼‰
            secondary_keywords = [
                'è´¢åŠ¡æŠ¥è¡¨é™„æ³¨',
                'ä¸»è¦ä¼šè®¡æ”¿ç­–',
                'é‡è¦ä¼šè®¡ä¼°è®¡',
                'æˆ–æœ‰äº‹é¡¹',
                'æ‰¿è¯ºäº‹é¡¹',
            ]

            # æ‰«ææ¯ä¸€é¡µ
            for page_num in range(min(len(doc), 50)):  # åªæ‰«æå‰50é¡µ
                page = doc[page_num]
                text = page.get_text()

                if not text.strip():
                    continue

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å†…å®¹
                page_score = 0
                matched_keywords = []

                for keyword in critical_keywords:
                    if re.search(keyword, text, re.IGNORECASE):
                        page_score += 10
                        matched_keywords.append(keyword)

                for keyword in secondary_keywords:
                    if re.search(keyword, text, re.IGNORECASE):
                        page_score += 2
                        matched_keywords.append(keyword)

                # å¦‚æœå¾—åˆ†è¶³å¤Ÿé«˜ï¼Œæ ‡è®°ä¸ºå…³é”®é¡µ
                if page_score >= 10:
                    key_pages.add(page_num + 1)
                    self.logger.debug(f"å…³é”®é¡µ: ç¬¬{page_num + 1}é¡µ (å¾—åˆ†:{page_score}, å…³é”®è¯:{matched_keywords[:3]})")

                # é™åˆ¶æœ€å¤šæå–çš„é¡µæ•°
                if len(key_pages) >= max_pages:
                    self.logger.info(f"å·²è¾¾åˆ°æœ€å¤§å…³é”®é¡µæ•° {max_pages}ï¼Œåœæ­¢æ‰«æ")
                    break

            doc.close()

            # æ’åºå¹¶è¿”å›
            result = sorted(list(key_pages))
            self.logger.info(f"âœ… è¯†åˆ«å‡º {len(result)} ä¸ªå…³é”®é¡µ: {result}")

            return result

        except Exception as e:
            self.logger.error(f"è¯†åˆ«å®¡è®¡æŠ¥å‘Šå…³é”®é¡µå¤±è´¥: {e}")
            # é™çº§ç­–ç•¥ï¼šè¿”å›å‰10é¡µ
            return list(range(1, 11))