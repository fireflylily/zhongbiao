# å‘é‡æœç´¢è®¾è®¡æ–¹æ¡ˆ

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†AIæ ‡ä¹¦ç³»ç»Ÿçš„å‘é‡æœç´¢å¼•æ“è®¾è®¡ï¼ŒåŸºäºFAISSï¼ˆFacebook AI Similarity Searchï¼‰å®ç°é«˜æ€§èƒ½çš„è¯­ä¹‰æœç´¢åŠŸèƒ½ã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“è®¾è®¡
```
å‘é‡æœç´¢å¼•æ“
â”œâ”€â”€ æ–‡æ¡£å¤„ç†å±‚
â”‚   â”œâ”€â”€ æ–‡æ¡£åˆ†å— (Chunking)
â”‚   â”œâ”€â”€ æ–‡æœ¬æå– (Text Extraction)
â”‚   â””â”€â”€ å†…å®¹æ¸…æ´— (Content Cleaning)
â”œâ”€â”€ å‘é‡åŒ–å±‚
â”‚   â”œâ”€â”€ æ¨¡å‹åŠ è½½ (Model Loading)
â”‚   â”œâ”€â”€ æ–‡æœ¬ç¼–ç  (Text Encoding)
â”‚   â””â”€â”€ å‘é‡æ ‡å‡†åŒ– (Vector Normalization)
â”œâ”€â”€ ç´¢å¼•ç®¡ç†å±‚
â”‚   â”œâ”€â”€ FAISSç´¢å¼• (Index Management)
â”‚   â”œâ”€â”€ å‘é‡å­˜å‚¨ (Vector Storage)
â”‚   â””â”€â”€ å…ƒæ•°æ®æ˜ å°„ (Metadata Mapping)
â””â”€â”€ æœç´¢æœåŠ¡å±‚
    â”œâ”€â”€ æŸ¥è¯¢å¤„ç† (Query Processing)
    â”œâ”€â”€ ç›¸ä¼¼åº¦è®¡ç®— (Similarity Calculation)
    â”œâ”€â”€ ç»“æœæ’åº (Result Ranking)
    â””â”€â”€ æ··åˆæœç´¢ (Hybrid Search)
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 1. æ–‡æ¡£å¤„ç†æ¨¡å—

#### æ–‡æ¡£åˆ†å—ç­–ç•¥
```python
class DocumentChunker:
    def __init__(self):
        self.chunk_size = 512          # æ¯å—æœ€å¤§å­—ç¬¦æ•°
        self.chunk_overlap = 50        # å—é—´é‡å å­—ç¬¦æ•°
        self.min_chunk_size = 100      # æœ€å°å—å¤§å°

    def chunk_document(self, content: str, doc_type: str) -> List[Chunk]:
        """
        æ™ºèƒ½æ–‡æ¡£åˆ†å—ï¼Œæ ¹æ®æ–‡æ¡£ç±»å‹ä½¿ç”¨ä¸åŒç­–ç•¥
        """
        if doc_type == 'structured':
            return self._chunk_by_structure(content)
        elif doc_type == 'technical':
            return self._chunk_by_semantics(content)
        else:
            return self._chunk_by_sliding_window(content)

    def _chunk_by_structure(self, content: str) -> List[Chunk]:
        """åŸºäºæ–‡æ¡£ç»“æ„åˆ†å—ï¼ˆæ ‡é¢˜ã€æ®µè½ç­‰ï¼‰"""
        pass

    def _chunk_by_semantics(self, content: str) -> List[Chunk]:
        """åŸºäºè¯­ä¹‰è¾¹ç•Œåˆ†å—"""
        pass

    def _chunk_by_sliding_window(self, content: str) -> List[Chunk]:
        """æ»‘åŠ¨çª—å£åˆ†å—"""
        pass
```

#### å†…å®¹é¢„å¤„ç†
```python
class ContentPreprocessor:
    def __init__(self):
        self.stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ'])

    def preprocess(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†æµæ°´çº¿
        """
        # 1. æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        text = self._clean_special_chars(text)

        # 2. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = self._normalize_whitespace(text)

        # 3. å¤„ç†æ•°å­—å’Œæ—¥æœŸ
        text = self._normalize_numbers_dates(text)

        # 4. ç§»é™¤è¿‡çŸ­æ–‡æœ¬
        if len(text.strip()) < 10:
            return ""

        return text
```

### 2. å‘é‡åŒ–æ¨¡å—

#### æ¨¡å‹é…ç½®
```python
EMBEDDING_CONFIG = {
    'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    'model_path': 'models/embedding_model/',
    'dimension': 384,
    'max_seq_length': 512,
    'device': 'cpu',  # æˆ– 'cuda' å¦‚æœæœ‰GPU
    'batch_size': 32,
    'normalize_embeddings': True
}
```

#### å‘é‡ç¼–ç å™¨
```python
class VectorEncoder:
    def __init__(self, config: dict):
        self.model = SentenceTransformer(config['model_name'])
        self.dimension = config['dimension']
        self.batch_size = config['batch_size']
        self.device = config['device']

    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """
        æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
        """
        # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º
        all_embeddings = []
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_embeddings = self.model.encode(
                batch_texts,
                device=self.device,
                normalize_embeddings=True,
                show_progress_bar=True
            )
            all_embeddings.append(batch_embeddings)

        return np.vstack(all_embeddings)

    def encode_single(self, text: str) -> np.ndarray:
        """
        å•æ–‡æœ¬å‘é‡åŒ–ï¼ˆç”¨äºæœç´¢æŸ¥è¯¢ï¼‰
        """
        return self.model.encode([text], normalize_embeddings=True)[0]
```

### 3. FAISSç´¢å¼•ç®¡ç†

#### ç´¢å¼•ç±»å‹é€‰æ‹©
```python
class IndexManager:
    def __init__(self, dimension: int):
        self.dimension = dimension
        self.index_configs = {
            'flat': self._create_flat_index,      # ç²¾ç¡®æœç´¢ï¼Œå°æ•°æ®é›†
            'ivf': self._create_ivf_index,        # è¿‘ä¼¼æœç´¢ï¼Œä¸­ç­‰æ•°æ®é›†
            'hnsw': self._create_hnsw_index,      # é«˜æ€§èƒ½æœç´¢ï¼Œå¤§æ•°æ®é›†
            'pq': self._create_pq_index           # å‹ç¼©å­˜å‚¨ï¼Œå†…å­˜é™åˆ¶
        }

    def create_index(self, index_type: str, data_size: int) -> faiss.Index:
        """
        æ ¹æ®æ•°æ®è§„æ¨¡é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹
        """
        if data_size < 10000:
            return self._create_flat_index()
        elif data_size < 100000:
            return self._create_ivf_index()
        else:
            return self._create_hnsw_index()

    def _create_flat_index(self) -> faiss.Index:
        """åˆ›å»ºç²¾ç¡®æœç´¢ç´¢å¼•"""
        return faiss.IndexFlatIP(self.dimension)

    def _create_ivf_index(self, nlist: int = 100) -> faiss.Index:
        """åˆ›å»ºIVFç´¢å¼•ï¼ˆå€’æ’æ–‡ä»¶ç´¢å¼•ï¼‰"""
        quantizer = faiss.IndexFlatIP(self.dimension)
        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        return index

    def _create_hnsw_index(self, M: int = 32) -> faiss.Index:
        """åˆ›å»ºHNSWç´¢å¼•ï¼ˆå±‚æ¬¡å¯¼èˆªå°ä¸–ç•Œï¼‰"""
        index = faiss.IndexHNSWFlat(self.dimension, M)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 100
        return index
```

#### ç´¢å¼•æ“ä½œ
```python
class VectorIndex:
    def __init__(self, index_path: str, dimension: int):
        self.index_path = index_path
        self.dimension = dimension
        self.index = None
        self.id_mapping = {}  # FAISS ID -> æ•°æ®åº“IDæ˜ å°„

    def build_index(self, vectors: np.ndarray, doc_ids: List[int]):
        """
        æ„å»ºå‘é‡ç´¢å¼•
        """
        # åˆ›å»ºç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dimension)

        # æ·»åŠ å‘é‡
        self.index.add(vectors.astype('float32'))

        # å»ºç«‹IDæ˜ å°„
        for i, doc_id in enumerate(doc_ids):
            self.id_mapping[i] = doc_id

        # ä¿å­˜ç´¢å¼•
        self.save_index()

    def add_vectors(self, vectors: np.ndarray, doc_ids: List[int]):
        """
        å¢é‡æ·»åŠ å‘é‡
        """
        if self.index is None:
            self.load_index()

        start_idx = self.index.ntotal
        self.index.add(vectors.astype('float32'))

        # æ›´æ–°IDæ˜ å°„
        for i, doc_id in enumerate(doc_ids):
            self.id_mapping[start_idx + i] = doc_id

        self.save_index()

    def search(self, query_vector: np.ndarray, k: int = 10) -> Tuple[List[float], List[int]]:
        """
        å‘é‡æœç´¢
        """
        if self.index is None:
            self.load_index()

        # æ‰§è¡Œæœç´¢
        scores, faiss_ids = self.index.search(
            query_vector.reshape(1, -1).astype('float32'), k
        )

        # è½¬æ¢ä¸ºæ•°æ®åº“ID
        doc_ids = [self.id_mapping.get(fid, -1) for fid in faiss_ids[0]]

        return scores[0].tolist(), doc_ids

    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        faiss.write_index(self.index, self.index_path)

        # ä¿å­˜IDæ˜ å°„
        mapping_path = self.index_path.replace('.index', '_mapping.json')
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(self.id_mapping, f, ensure_ascii=False, indent=2)

    def load_index(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•"""
        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)

            # åŠ è½½IDæ˜ å°„
            mapping_path = self.index_path.replace('.index', '_mapping.json')
            if os.path.exists(mapping_path):
                with open(mapping_path, 'r', encoding='utf-8') as f:
                    self.id_mapping = {int(k): v for k, v in json.load(f).items()}
```

### 4. æœç´¢æœåŠ¡æ¨¡å—

#### æ··åˆæœç´¢å¼•æ“
```python
class HybridSearchEngine:
    def __init__(self, vector_index: VectorIndex, db_connection):
        self.vector_index = vector_index
        self.db = db_connection
        self.encoder = VectorEncoder(EMBEDDING_CONFIG)

    def search(self, query: str, filters: dict = None, k: int = 10) -> List[SearchResult]:
        """
        æ··åˆæœç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢
        """
        # 1. å‘é‡æœç´¢
        vector_results = self._vector_search(query, k * 2)

        # 2. å…³é”®è¯æœç´¢
        keyword_results = self._keyword_search(query, filters, k * 2)

        # 3. ç»“æœèåˆ
        merged_results = self._merge_results(vector_results, keyword_results)

        # 4. é‡æ–°æ’åº
        final_results = self._rerank_results(merged_results, query)

        return final_results[:k]

    def _vector_search(self, query: str, k: int) -> List[VectorSearchResult]:
        """å‘é‡è¯­ä¹‰æœç´¢"""
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vector = self.encoder.encode_single(query)

        # æœç´¢æœ€ç›¸ä¼¼å‘é‡
        scores, doc_ids = self.vector_index.search(query_vector, k)

        # æ„å»ºç»“æœ
        results = []
        for score, doc_id in zip(scores, doc_ids):
            if doc_id != -1:  # æœ‰æ•ˆID
                results.append(VectorSearchResult(
                    doc_id=doc_id,
                    similarity_score=float(score),
                    search_type='vector'
                ))

        return results

    def _keyword_search(self, query: str, filters: dict, k: int) -> List[KeywordSearchResult]:
        """å…³é”®è¯æœç´¢"""
        # æ„å»ºSQLæŸ¥è¯¢
        sql = """
        SELECT d.id, d.title, d.category, d.description,
               GROUP_CONCAT(c.content, ' ') as content
        FROM product_documents d
        LEFT JOIN document_chunks c ON d.id = c.document_id
        WHERE d.status = 'active'
        """

        params = []

        # æ·»åŠ å…³é”®è¯æ¡ä»¶
        if query.strip():
            keywords = query.split()
            keyword_conditions = []
            for keyword in keywords:
                keyword_conditions.append(
                    "(d.title LIKE ? OR d.description LIKE ? OR c.content LIKE ?)"
                )
                params.extend([f'%{keyword}%'] * 3)

            if keyword_conditions:
                sql += " AND (" + " OR ".join(keyword_conditions) + ")"

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            if 'category' in filters:
                sql += " AND d.category = ?"
                params.append(filters['category'])
            if 'security_level' in filters:
                sql += " AND d.security_level = ?"
                params.append(filters['security_level'])

        sql += " GROUP BY d.id ORDER BY d.upload_time DESC LIMIT ?"
        params.append(k)

        # æ‰§è¡ŒæŸ¥è¯¢
        cursor = self.db.execute(sql, params)
        results = []
        for row in cursor.fetchall():
            results.append(KeywordSearchResult(
                doc_id=row[0],
                title=row[1],
                category=row[2],
                description=row[3],
                content=row[4],
                search_type='keyword'
            ))

        return results

    def _merge_results(self, vector_results: List, keyword_results: List) -> List[SearchResult]:
        """ç»“æœèåˆç®—æ³•"""
        # ä½¿ç”¨åŠ æƒèåˆ
        vector_weight = 0.7
        keyword_weight = 0.3

        merged = {}

        # å¤„ç†å‘é‡æœç´¢ç»“æœ
        for i, result in enumerate(vector_results):
            doc_id = result.doc_id
            # è®¡ç®—ä½ç½®æƒé‡ï¼ˆæ’åè¶Šå‰æƒé‡è¶Šé«˜ï¼‰
            position_weight = 1.0 / (i + 1)
            score = result.similarity_score * vector_weight * position_weight

            merged[doc_id] = SearchResult(
                doc_id=doc_id,
                vector_score=result.similarity_score,
                keyword_score=0.0,
                final_score=score,
                search_types=['vector']
            )

        # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
        for i, result in enumerate(keyword_results):
            doc_id = result.doc_id
            position_weight = 1.0 / (i + 1)
            keyword_score = position_weight  # å…³é”®è¯åŒ¹é…å¾—åˆ†
            score = keyword_score * keyword_weight

            if doc_id in merged:
                # æ›´æ–°ç°æœ‰ç»“æœ
                merged[doc_id].keyword_score = keyword_score
                merged[doc_id].final_score += score
                merged[doc_id].search_types.append('keyword')
            else:
                # æ–°å¢ç»“æœ
                merged[doc_id] = SearchResult(
                    doc_id=doc_id,
                    vector_score=0.0,
                    keyword_score=keyword_score,
                    final_score=score,
                    search_types=['keyword']
                )

        return list(merged.values())

    def _rerank_results(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ç»“æœé‡æ’åº"""
        # æ ¹æ®æœ€ç»ˆå¾—åˆ†æ’åº
        results.sort(key=lambda x: x.final_score, reverse=True)

        # å¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é‡æ’åºé€»è¾‘ï¼Œæ¯”å¦‚ï¼š
        # 1. è€ƒè™‘æ–‡æ¡£æ–°é²œåº¦
        # 2. è€ƒè™‘æ–‡æ¡£è´¨é‡è¯„åˆ†
        # 3. è€ƒè™‘ç”¨æˆ·å†å²åå¥½

        return results
```

### 5. æŸ¥è¯¢ä¼˜åŒ–æ¨¡å—

#### æŸ¥è¯¢æ‰©å±•
```python
class QueryExpansion:
    def __init__(self):
        self.synonyms = self._load_synonyms()

    def expand_query(self, query: str) -> str:
        """
        æŸ¥è¯¢æ‰©å±•ï¼šæ·»åŠ åŒä¹‰è¯ã€ç›¸å…³è¯
        """
        words = query.split()
        expanded_words = set(words)

        for word in words:
            # æ·»åŠ åŒä¹‰è¯
            if word in self.synonyms:
                expanded_words.update(self.synonyms[word])

        return ' '.join(expanded_words)

    def _load_synonyms(self) -> dict:
        """åŠ è½½åŒä¹‰è¯è¯å…¸"""
        # å¯ä»¥ä»æ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½
        return {
            'è½¯ä»¶': ['ç³»ç»Ÿ', 'ç¨‹åº', 'åº”ç”¨'],
            'ç¡¬ä»¶': ['è®¾å¤‡', 'æœºå™¨', 'è£…ç½®'],
            'æœåŠ¡': ['æ”¯æŒ', 'ç»´æŠ¤', 'ä¿éšœ']
        }
```

#### æŸ¥è¯¢æ„å›¾è¯†åˆ«
```python
class QueryIntentClassifier:
    def __init__(self):
        self.intent_patterns = {
            'technical': ['æŠ€æœ¯', 'è§„æ ¼', 'å‚æ•°', 'é…ç½®'],
            'implementation': ['å®æ–½', 'éƒ¨ç½²', 'å®‰è£…', 'é…ç½®'],
            'service': ['æœåŠ¡', 'æ”¯æŒ', 'ç»´æŠ¤', 'åŸ¹è®­'],
            'case': ['æ¡ˆä¾‹', 'ç»éªŒ', 'å®¢æˆ·', 'é¡¹ç›®']
        }

    def classify_intent(self, query: str) -> str:
        """
        è¯†åˆ«æŸ¥è¯¢æ„å›¾ï¼Œä¼˜åŒ–æœç´¢ç­–ç•¥
        """
        for intent, keywords in self.intent_patterns.items():
            if any(keyword in query for keyword in keywords):
                return intent
        return 'general'
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. ç´¢å¼•ä¼˜åŒ–
```python
# ç´¢å¼•é…ç½®ä¼˜åŒ–
OPTIMIZATION_CONFIG = {
    'index_training_size': 50000,      # è®­ç»ƒæ ·æœ¬æ•°é‡
    'nlist': 1024,                     # èšç±»ä¸­å¿ƒæ•°é‡ï¼ˆâˆšnï¼‰
    'nprobe': 32,                      # æœç´¢æ—¶æ¢ç´¢çš„èšç±»æ•°
    'efConstruction': 200,             # HNSWæ„å»ºå‚æ•°
    'efSearch': 100,                   # HNSWæœç´¢å‚æ•°
    'use_gpu': False,                  # æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
    'omp_num_threads': 4              # OpenMPçº¿ç¨‹æ•°
}
```

### 2. ç¼“å­˜ç­–ç•¥
```python
class SearchCache:
    def __init__(self, cache_size: int = 1000):
        self.cache = LRUCache(cache_size)
        self.hit_count = 0
        self.miss_count = 0

    def get(self, query_hash: str) -> Optional[List[SearchResult]]:
        """è·å–ç¼“å­˜ç»“æœ"""
        result = self.cache.get(query_hash)
        if result:
            self.hit_count += 1
        else:
            self.miss_count += 1
        return result

    def put(self, query_hash: str, results: List[SearchResult]):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        self.cache.put(query_hash, results)

    def get_hit_ratio(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hit_count + self.miss_count
        return self.hit_count / total if total > 0 else 0.0
```

### 3. æ‰¹é‡å¤„ç†
```python
class BatchProcessor:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size

    def process_documents_batch(self, documents: List[Document]):
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        for i in range(0, len(documents), self.batch_size):
            batch = documents[i:i + self.batch_size]
            self._process_batch(batch)

    def _process_batch(self, batch: List[Document]):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        # 1. æ‰¹é‡åˆ†å—
        all_chunks = []
        for doc in batch:
            chunks = self.chunker.chunk_document(doc.content, doc.type)
            all_chunks.extend(chunks)

        # 2. æ‰¹é‡å‘é‡åŒ–
        texts = [chunk.content for chunk in all_chunks]
        vectors = self.encoder.encode_texts(texts)

        # 3. æ‰¹é‡ç´¢å¼•
        self.index.add_vectors(vectors, [chunk.doc_id for chunk in all_chunks])
```

## ğŸ” æœç´¢APIè®¾è®¡

### RESTful APIæ¥å£
```python
from flask import Flask, request, jsonify

app = Flask(__name__)
search_engine = HybridSearchEngine()

@app.route('/api/search', methods=['POST'])
def search_documents():
    """
    æ–‡æ¡£æœç´¢API
    """
    data = request.get_json()

    # å‚æ•°éªŒè¯
    query = data.get('query', '').strip()
    if not query:
        return jsonify({'error': 'æŸ¥è¯¢ä¸èƒ½ä¸ºç©º'}), 400

    k = min(data.get('k', 10), 100)  # é™åˆ¶æœ€å¤§è¿”å›æ•°é‡
    filters = data.get('filters', {})
    search_type = data.get('search_type', 'hybrid')  # vector, keyword, hybrid

    try:
        # æ‰§è¡Œæœç´¢
        results = search_engine.search(
            query=query,
            filters=filters,
            k=k,
            search_type=search_type
        )

        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        response = {
            'query': query,
            'total': len(results),
            'results': [result.to_dict() for result in results],
            'search_time_ms': results.search_time_ms if hasattr(results, 'search_time_ms') else 0
        }

        return jsonify(response)

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/search/suggest', methods=['GET'])
def search_suggestions():
    """
    æœç´¢å»ºè®®API
    """
    query = request.args.get('q', '').strip()
    if len(query) < 2:
        return jsonify({'suggestions': []})

    # åŸºäºå†å²æœç´¢å’Œæ–‡æ¡£æ ‡é¢˜ç”Ÿæˆå»ºè®®
    suggestions = search_engine.get_suggestions(query, limit=10)

    return jsonify({'suggestions': suggestions})
```

## ğŸ“ˆ ç›‘æ§å’Œåˆ†æ

### æ€§èƒ½ç›‘æ§
```python
class SearchMetrics:
    def __init__(self):
        self.query_count = 0
        self.total_response_time = 0.0
        self.slow_queries = []

    def record_search(self, query: str, response_time_ms: float, result_count: int):
        """è®°å½•æœç´¢æŒ‡æ ‡"""
        self.query_count += 1
        self.total_response_time += response_time_ms

        # è®°å½•æ…¢æŸ¥è¯¢
        if response_time_ms > 1000:  # è¶…è¿‡1ç§’
            self.slow_queries.append({
                'query': query,
                'response_time_ms': response_time_ms,
                'result_count': result_count,
                'timestamp': datetime.now()
            })

    def get_average_response_time(self) -> float:
        """è·å–å¹³å‡å“åº”æ—¶é—´"""
        return self.total_response_time / self.query_count if self.query_count > 0 else 0.0
```

### æœç´¢åˆ†æ
```python
class SearchAnalyzer:
    def __init__(self, db_connection):
        self.db = db_connection

    def analyze_search_patterns(self) -> dict:
        """åˆ†ææœç´¢æ¨¡å¼"""
        # çƒ­é—¨æŸ¥è¯¢
        popular_queries = self.db.execute("""
            SELECT query_text, COUNT(*) as freq
            FROM search_logs
            WHERE search_time >= datetime('now', '-7 days')
            GROUP BY query_text
            ORDER BY freq DESC
            LIMIT 10
        """).fetchall()

        # æ— ç»“æœæŸ¥è¯¢
        zero_result_queries = self.db.execute("""
            SELECT query_text, COUNT(*) as freq
            FROM search_logs
            WHERE result_count = 0
            AND search_time >= datetime('now', '-7 days')
            GROUP BY query_text
            ORDER BY freq DESC
            LIMIT 10
        """).fetchall()

        return {
            'popular_queries': popular_queries,
            'zero_result_queries': zero_result_queries
        }
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´

### ç´¢å¼•æ„å»ºè„šæœ¬
```bash
#!/bin/bash
# build_indexes.sh

echo "å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•..."

# 1. å¤‡ä»½ç°æœ‰ç´¢å¼•
if [ -f "data/vector_indexes/documents.index" ]; then
    cp data/vector_indexes/documents.index data/vector_indexes/backup/documents_$(date +%Y%m%d).index
fi

# 2. æ„å»ºæ–°ç´¢å¼•
python scripts/build_vector_index.py --input data/knowledge_base.db --output data/vector_indexes/

# 3. éªŒè¯ç´¢å¼•
python scripts/verify_index.py --index_path data/vector_indexes/documents.index

echo "å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ!"
```

### å¥åº·æ£€æŸ¥
```python
class HealthChecker:
    def __init__(self, search_engine):
        self.search_engine = search_engine

    def check_health(self) -> dict:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        health_status = {
            'overall': 'healthy',
            'components': {}
        }

        # æ£€æŸ¥å‘é‡ç´¢å¼•
        try:
            test_vector = np.random.random(384).astype('float32')
            self.search_engine.vector_index.search(test_vector, 1)
            health_status['components']['vector_index'] = 'healthy'
        except Exception as e:
            health_status['components']['vector_index'] = f'unhealthy: {str(e)}'
            health_status['overall'] = 'unhealthy'

        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        try:
            self.search_engine.db.execute("SELECT 1").fetchone()
            health_status['components']['database'] = 'healthy'
        except Exception as e:
            health_status['components']['database'] = f'unhealthy: {str(e)}'
            health_status['overall'] = 'unhealthy'

        return health_status
```

---

**ç»´æŠ¤çŠ¶æ€**: ğŸŸ¢ ç§¯æç»´æŠ¤ä¸­
**æœ€åæ›´æ–°**: 2025å¹´9æœˆ27æ—¥
**ç‰ˆæœ¬**: v1.0